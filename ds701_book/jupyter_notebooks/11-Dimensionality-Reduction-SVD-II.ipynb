{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Dimensionality Reduction - PCA + t-SNE\n",
        "jupyter: python3\n",
        "bibliography: references.bib\n",
        "nocite: |\n",
        "  @novembre2008genes, @strang2022introduction\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/11-Dimensionality-Reduction-SVD-II.ipynb)\n",
        "\n",
        "We previously learned how to use the SVD as a tool for constructing low-rank matrices.\n",
        "\n",
        "We now consider it as a tool for transforming (i.e., reducing the dimension of) our data. \n",
        "\n",
        "## Overview\n",
        "\n",
        "Collected data is often high-dimensional. The high-dimensionality of, or the large number of features in a dataset is challenging to work with. \n",
        "\n",
        "![](figs/elephant_perspective.png){fig-align=\"center\" width=50%}\n",
        "\n",
        "## High-Dimensional Challenges\n",
        "\n",
        "We have seen some of these challenges already, in particular:\n",
        "\n",
        ":::: {.incremental}\n",
        "- the curse of dimensionality, where data points become sparse in higher dimensions and distance metrics have less meaning,\n",
        "- overfitting, where high-dimensional data can lead models becoming overly complex and fitting to noise in the data as opposed to the actual signal,\n",
        "- computational complexity, high-dimensional data requires more computing power and memory,\n",
        "- visualization, where high-dimensional data makes understanding and interpreting the data difficult.\n",
        "::::\n",
        "--- \n",
        "\n",
        "How can we reduce the dimension of our data but still preserve the most important information in our dataset?\n",
        "\n",
        ":::: {.fragment}\n",
        "We consider two techniques:\n",
        "\n",
        ":::: {.incremental}\n",
        "- Principle Component Analysis (PCA)\n",
        "- t-distributed stochastic neighbor embedding (t-SNE)\n",
        "::::\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "We will demonstrate the relationship between PCA and the SVD.\n",
        "\n",
        "t-SNE is an alternative nonlinear method for dimensionality reduction.\n",
        "::::\n",
        "\n",
        "# PCA \n",
        "\n",
        "## Dimensionality Reduction \n",
        "\n",
        "Input: $\\mathbf{x}_1,\\ldots, \\mathbf{x}_m$ with  $\\mathbf{x}_i \\in \\mathbb{R}^n \\: \\: \\forall \\: i \\in \\{1, \\ldots, n\\}.$ \n",
        "\n",
        "Output: $\\mathbf{y}_1,\\dots, \\mathbf{y}_m$  with  $\\mathbf{y}_i \\in \\mathbb{R}^d \\: \\: \\forall \\: i \\in \\{1, \\dots, n\\}$. \n",
        "\n",
        "The goal is to compute the new data points $\\mathbf{y}_i$ such that <font color=\"red\"> $d << n$ </font> while still preserving the most information contained in the data points $\\mathbf{x}_i$.\n",
        "\n",
        "Be aware that row i of the data matrix $X_0$ is the $n$ dimensional vector $\\mathbf{x}_i$. This keeps our matrix in the structure $m$ data rows and $n$ features (columns). The same is true for $Y$ (i.e., there are $m$ rows with $d$ features).\n",
        "\n",
        "$$\n",
        "X_0 = \n",
        "\\begin{bmatrix} \n",
        "x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{m1} & x_{m2} & \\dots & x_{mn} \n",
        "\\end{bmatrix} \\:\n",
        "\\xrightarrow[\\text{PCA}]{} \\:\n",
        "Y = \\begin{bmatrix} \n",
        "y_{11} & y_{12} & \\dots & y_{1d} \\\\\n",
        "y_{21} & y_{22} & \\dots & y_{2d} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "y_{m1} & y_{m2} & \\dots & y_{md} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "## PCA Application: Genes Mirror Geography\n",
        "\n",
        "We consider a dataset from @novembre2008genes. The authors collected DNA data called SNPs (single nucleotide polymorphism) from 3000 individuals in Europe. \n",
        "\n",
        "SNPs describe the changes in DNA from a common base pair (A,T,C,G). A value of 0 means no changes to the base pair, a value of 1 means 1 change to the base pair, and a value of 2 means both base pairs change.\n",
        "\n",
        "The data for each individual consisted of approximately 500k SNPs. This means the data matrix we are working with is 3000 x 500k.\n",
        "\n",
        "---\n",
        "\n",
        "The authors performed PCA and plotted 1387 of the individuals in the reduced dimensions. \n",
        "\n",
        "![Image Credit @novembre2008genes](figs/PCA_genes_Europe.png)\n",
        "\n",
        "For comparison, a color coded map of western Europe is added and the same color coding was applied to the data samples by country of origin.\n",
        "\n",
        "---\n",
        "\n",
        "Key observations:\n",
        "\n",
        ":::: {.incremental}\n",
        "- the first principal components of the data almost reproduce the map of Europe, i.e., they appear to correspond to latitude and longitude.\n",
        "- SNPs are similar geographically\n",
        "- DNA of an individual reveals their birthplace within a few hundred kilometers\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Would a similar study in the USA be effective?\n",
        "::::\n",
        "\n",
        "## PCA Overview\n",
        "\n",
        "The following describes the process to perform PCA and obtain your reduced data set. The goal is given\n",
        "\n",
        "Input: $X_0\\in\\mathbb{R}^{m\\times n}$, produce\n",
        "\n",
        "Output: $Y\\in\\mathbb{R}^{m\\times d}$ with $d << n$.\n",
        "\n",
        "The first step is to center the data (subtract the mean across rows): $X_0 \\rightarrow X$.\n",
        "\n",
        "Example:\n",
        "$$\n",
        "X_0 = \n",
        "\\begin{bmatrix}\n",
        "90 & 60 & 60\\\\\n",
        "80 & 60 & 70 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The mean across rows is:\n",
        "$$\n",
        "\\boldsymbol{\\mu} = \\begin{bmatrix} 85 & 60 & 65 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The mean-centered dataset is\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "5 & 0 & -5\\\\\n",
        "-5 & 0 & 5 \\end{bmatrix} \n",
        "$$ \n",
        "\n",
        "The next step is to determine the directions of the data that correspond to the largest variances. These are the principal components.\n",
        "\n",
        "---\n",
        "\n",
        "## Least Squares Interpretation\n",
        "\n",
        "Centered data often clusters along a line (or other low-dimensional subspace of $\\mathbb{R}^{n}$).\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/PCA_variance.png)\n",
        "The sum of **variances** (squared distances to the mean) of the projected points is a **maximum**.\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/PCA_residual.png)\n",
        "The sum of **residuals** (squared distances from the points to the line) is a **minimum**.\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "What is the statistical entity that measures the variability in the data?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Answer: the covariance matrix.\n",
        "::::\n",
        "\n",
        "## Covariance Matrix\n",
        "\n",
        "Let $X\\in\\mathbb{R}^{m\\times n}$ contained the centered data. Recall that the sample covariance matrix is defined by\n",
        "\n",
        "$$\n",
        "S = \\frac{1}{m-1}X^{T}X\n",
        "$$\n",
        "\n",
        "Example:   \n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "5 & 0 & -5\\\\\n",
        "-5 & 0 & 5 \\end{bmatrix} \n",
        "$$\n",
        "\n",
        "$$ \n",
        "S =  \n",
        "\\begin{bmatrix}\n",
        "5 & -5\\\\\n",
        "0 & 0\\\\\n",
        "-5 & 5 \\end{bmatrix} \n",
        "\\begin{bmatrix}\n",
        "5 & 0 & -5\\\\\n",
        "-5 & 0 & 5 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "50 & 0 & -50\\\\\n",
        "0 & 0 & 0\\\\\n",
        "-50 & 0 & 50 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The matrix $S$ is symmetric, i.e., $S = S^{T}.$ \n",
        "\n",
        "## Spectral Decomposition\n",
        "\n",
        "We use the fact that $S$ is symmetric to apply the Spectral Decomposition, which states:\n",
        "\n",
        "> Every real symmetric matrix $S$ has the factorization $V\\Lambda V^{T}$, where $\\Lambda$ is a diagonal matrix that contains the eigenvalues of S and the columns of $V$ are orthogonal eigenvectors of $S$.\n",
        "\n",
        "You can refresh your memory about this in the [Linear Algebra Refresher](04-Linear-Algebra-Refresher.qmd).\n",
        "\n",
        "---\n",
        "\n",
        "The covariance matrix $S \\in \\mathbb{R}^{n\\times n}$ has the spectral decomposition $S = V\\Lambda V^T$ where\n",
        "\n",
        "$$\n",
        "\\Lambda = \n",
        "\\begin{bmatrix}\n",
        "\\lambda_1                                   \\\\\n",
        "& \\lambda_2             &   & \\text{\\Large0}\\\\\n",
        "&               & \\ddots                \\\\\n",
        "& \\text{\\Large0} &   & \\lambda_{n-1}            \\\\\n",
        "&               &   &   & \\lambda_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "with $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_{n-1} \\geq \\lambda_n$, and\n",
        "$$\n",
        "V = \\begin{bmatrix} \n",
        "\\bigg| & \\bigg| &  & \\bigg| & \\bigg| \\\\\n",
        "\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots & \\mathbf{v}_{n-1}   & \\mathbf{v}_n  \\\\\n",
        "\\bigg| & \\bigg| & & \\bigg| & \\bigg|\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        " \n",
        "with $S\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$ and $\\mathbf{v}_i \\perp \\mathbf{v}_j$ for $i\\neq j$.\n",
        "\n",
        "---\n",
        "\n",
        "The previous decomposition was for all $n$ dimensions. To obtain our reduced data, we take the first $d$ columns of $V$, i.e.,\n",
        "\n",
        "$$\n",
        "V' = \\begin{bmatrix} \n",
        "\\bigg| & \\bigg| &  & \\bigg|   \\\\\n",
        "\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots & \\mathbf{v}_{d}  \\\\\n",
        "\\bigg| & \\bigg| & & \\bigg| \n",
        "\\end{bmatrix},\n",
        "$$ \n",
        "and the $d\\times d$ upper block of matrix $\\Lambda$\n",
        "$$\n",
        "\\Lambda' = \n",
        "\\begin{bmatrix}\n",
        "\\lambda_1  &  & \\\\\n",
        "  & \\lambda_2  &   \\\\\n",
        "&               & \\ddots                \\\\\n",
        "&               &   &   & \\lambda_d\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "----\n",
        "\n",
        "The direction $\\mathbf{v}_i$ is the $i$-th principal component and the corresponding $\\lambda_i$ accounts for the $i$-th largest variance in the dataset.\n",
        "\n",
        "In other words, $\\mathbf{v}_1$ is the first principal component and is the direction that accounts for the most variance in the dataset. \n",
        "\n",
        "The vector $\\mathbf{v}_d$ is the $d$-th principal component and is the direction that accounts for the $d$-th most variance in the dataset. \n",
        "\n",
        "The reduced data matrix is obtained by computing\n",
        "\n",
        "$$\n",
        "Y = XV'.\n",
        "$$\n",
        "\n",
        "----\n",
        "\n",
        "Returning to our example\n",
        "$$X \n",
        "= \\begin{bmatrix}\n",
        "  5 & 0 & -5\\\\\n",
        "-5 & 0 & 5 \\end{bmatrix}, \n",
        "\\quad\n",
        "S = \n",
        "\\begin{bmatrix}\n",
        "50 & 0 & -50\\\\\n",
        "0 & 0 & 0\\\\\n",
        "-50 & 0 & 50 \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Eigenvalues of $S$: $\\lambda_1 = 100, \\: \\lambda_2 = \\lambda_3 =0$\n",
        "\n",
        "Eigenvectors of $S$:\n",
        "$$\n",
        "\\mathbf{v}_1 = \n",
        "\\begin{bmatrix}\n",
        "0.7071 \\\\ 0 \\\\  -0.7071\n",
        "\\end{bmatrix}, \\:\n",
        "\\mathbf{v}_2 = \\begin{bmatrix}\n",
        "0.7071 \\\\ 0 \\\\  0.7071\n",
        "\\end{bmatrix}, \\:\n",
        "\\mathbf{v}_3 = \\begin{bmatrix}\n",
        "0 \\\\ 1 \\\\ 0\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "What is the value of the total variance in the data?\n",
        "\n",
        ":::: {.fragment}\n",
        "Total variance: $T = 100$.\n",
        "\n",
        "The first principal component accounts for all the variance in the dataset.\n",
        "::::\n",
        "\n",
        "## PCA Summary\n",
        "\n",
        "- The columns of $V$ are the principal directions (or components). \n",
        "- The principal components are the projection of the data into principal directions: $XV$.\n",
        "- The total variance $T$ in the data is the sum of all eigenvalues: $T = \\lambda_1 + \\lambda_2 + \\dots + \\lambda_n.$\n",
        "- The first eigenvector $\\mathbf{v}_1$ points in the most significant direction of the data. This direction explains the largest fraction $\\lambda_1/T$ of the total variance.\n",
        "- The second eigenvector $\\mathbf{v}_2$ accounts for a smaller fraction $\\lambda_2/T$.\n",
        "- The **explained variance** of component $i$ is the value $\\lambda_i/T$. As $i\\rightarrow n$ the explained variance gets smaller and approaches $\\lambda_n/T$.\n",
        "- The **cumulative (total) explained variance** is the sum of the explained variances. The total explained variance for all eigenvalues is 1.\n",
        "\n",
        "\n",
        "## Case Study: Random Data\n",
        "\n",
        "Let's consider some randomly generated data consisting of 2 features. We"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 42\n",
        "rng = np.random.default_rng(seed)\n",
        "\n",
        "n_samples = 500\n",
        "C = np.array([[0.1, 0.6], [2., .6]])\n",
        "X0 = rng.standard_normal((n_samples, 2)) @ C + np.array([-6, 3])\n",
        "X = X0 - X0.mean(axis=0)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's do PCA and plot the principle components over our dataset. As expected, the principal components are orthogonal and point in the directions of the maximum variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "origin = [0, 0]\n",
        "\n",
        "# Compute the covariance matrix\n",
        "cov_matrix = np.cov(X, rowvar=False)\n",
        "\n",
        "# Calculate the eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "# Sort the eigenvalues and eigenvectors in descending order\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "# Plot the original dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "\n",
        "# Plot the principal components\n",
        "for i in range(len(eigenvalues)):\n",
        "    plt.quiver(origin[0], origin[1], -eigenvectors[0, i], -eigenvectors[1, i],\n",
        "               angles='xy', scale_units='xy', scale=1, color=['r', 'g'][i])\n",
        "\n",
        "plt.title('Principal Components on Original Dataset')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case Study: Digits\n",
        "\n",
        "Let's consider another example using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "plt.figure(figsize=(8, 8),)\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    plt.imshow(digits.images[i], cmap='gray_r')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We first stack the columns of each digit on top of each other (this operation is called vectorizing) and perform PCA on the 64-D representation of the digits.\n",
        "\n",
        "We can plot the explained variance ratio and the total (cumulative) explained variance ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 1, figsize=(6, 5))\n",
        "\n",
        "# Scree plot (graph of eigenvalues corresponding to PC number)\n",
        "# This shows the explained variance ratio\n",
        "axs[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
        "axs[0].set_title('Scree Plot')\n",
        "axs[0].set_xlabel('Principal Component')\n",
        "axs[0].set_ylabel('Explained Variance Ratio')\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Cumulative explained variance plot\n",
        "axs[1].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
        "axs[1].set_title('Cumulative Explained Variance Plot')\n",
        "axs[1].set_xlabel('Principal Component')\n",
        "axs[1].set_ylabel('Cumulative Explained Variance')\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's plot the data in the first 2 principal component directions. We'll use the digit labels to color each digit in the reduced space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "plt.figure(figsize=(7, 7))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)\n",
        "plt.title('Digits in PC1 and PC2 Space')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "# Create a legend with discrete labels\n",
        "legend_labels = np.unique(y)\n",
        "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]\n",
        "plt.legend(handles, legend_labels, title=\"Digit Label\", loc=\"best\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We observe the following in our plot of the digits in the first two principal components:\n",
        "\n",
        "- There is a decent clustering of some of our digits, in particular 0, 2, 3, 4, and 6.\n",
        "- The numbers 0 and 6 seem to be relatively close to each other in this space.\n",
        "- There is not a very clear separation of the number 5 from some of the other points.\n",
        "\n",
        "\n",
        "## To scale or not to scale\n",
        "\n",
        "Consider a situation where we have age (years) and height (feet) data for 4 people.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "| Person | Age [years] | Height [feet] |\n",
        "|--------|-------------|---------------|\n",
        "| A      | 25          | 6.232         |\n",
        "| B      | 30          | 6.232         |\n",
        "| C      | 25          | 5.248         |\n",
        "| D      | 30          | 5.248         |\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/Scaling_feet.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "Notice that the dominant direction of the variance is aligned horizontally.\n",
        "\n",
        "---\n",
        "\n",
        "What if the height is in cm?\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "| Person | Age [years] | Height [cm] |\n",
        "|--------|-------------|-------------|\n",
        "| A      | 25          | 189.95      |\n",
        "| B      | 30          | 189.95      |\n",
        "| C      | 25          | 159.96      |\n",
        "| D      | 30          | 159.96      |\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/Scaling_cm.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "Notice that the dominant direction of the variance is aligned vertically.\n",
        "\n",
        "---\n",
        "\n",
        "Let's standardize our data.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "| Person | Age [years] | Height [cm] |\n",
        "|--------|-------------|-------------|\n",
        "| A      | -1          | 1           |\n",
        "| B      | 1           | 1           |\n",
        "| C      | -1          | -1          |\n",
        "| D      | 1           | -1          |\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/Scaling.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "When we normalize our data, we observe an equal distribution of the variance.\n",
        "\n",
        "--- \n",
        "\n",
        "What quantity is represented by $\\frac{Cov(X, Y)}{\\sigma_X\\sigma_Y}$, where $X$ and $Y$ represent the random variables associated to sampling a person with that Age and Height?\n",
        "\n",
        ":::: {.fragment}\n",
        "This is the correlation matrix. When you standardize your data you are no longer working with the covariance matrix but the correlation matrix.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "PCA on a standardized dataset and working with the correlation matrix is the best practice when your data has very different scales. \n",
        "\n",
        "PCA identifies the directions with the maximum variance and large scale data can disproportionately influence the results of your PCA.\n",
        "::::\n",
        "\n",
        "\n",
        "## Relationship to the SVD\n",
        "\n",
        "Recall that the SVD of a mean-centered data matrix $X\\in\\mathbb{R}^{m\\times n}$ ($m>n$) is\n",
        "\n",
        "$$\n",
        "X = U\\Sigma V^T.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Draw matrix A\n",
        "rect_A = patches.Rectangle((0, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_A)\n",
        "ax.text(1, 1.5, r'$A$', fontsize=20, ha='center', va='center')\n",
        "ax.text(1, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw equal sign\n",
        "ax.text(2.5, 1.5, r'$=$', fontsize=20, ha='center', va='center')\n",
        "\n",
        "# Draw matrix U\n",
        "rect_U = patches.Rectangle((3, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_U)\n",
        "ax.text(4, 1.5, r'$U$', fontsize=20, ha='center', va='center')\n",
        "ax.text(4, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw Sigma\n",
        "rect_Sigma = patches.Rectangle((5.5, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_Sigma)\n",
        "ax.text(6.5, 2, r'$\\Sigma$', fontsize=20, ha='center', va='center')\n",
        "ax.text(6.5, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw matrix V^T with the same dimensions as Sigma\n",
        "rect_VT = patches.Rectangle((8, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_VT)\n",
        "ax.text(9, 2, r'$V^T$', fontsize=20, ha='center', va='center')\n",
        "ax.text(9, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Set limits and remove axes\n",
        "ax.set_xlim(-1, 11)\n",
        "ax.set_ylim(-2, 4)\n",
        "ax.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "How can we relate this to what we learned in PCA?\n",
        "\n",
        "In PCA, we computed an eigen-decomposition of the covariance matrix, i.e., $S=V\\Lambda V^{T}$.\n",
        "\n",
        "Consider the following computation using the SVD of $X$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "X^{T}X\n",
        "= \n",
        "(U\\Sigma V^T)^{T}(U\\Sigma V^T)\n",
        "=\n",
        "V\\Sigma U^TU \\Sigma V^{T}\n",
        "= \n",
        "V\\Sigma^{2}V^T\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We see that eigenvalues $\\lambda_i$ of the matrix $S$ are the squares $\\sigma_{i}^{2}$ of the singular values $\\Sigma$ (scaled by $\\frac{1}{m-1}$).\n",
        "\n",
        "---\n",
        "\n",
        "In practice, PCA is done by computing the SVD of your data matrix as opposed to forming the covariance matrix and computing the eigenvalues. \n",
        "\n",
        "The principal components are the right singular vectors $V$. The projected data is computed as $XV$. The eigenvalues of $S$ are obtained from squaring the entries of $\\Sigma$ and scaling them by $\\frac{1}{m-1}$.\n",
        "\n",
        "Reasons to compute PCA this way are\n",
        "\n",
        ":::: {.incremental}\n",
        "- **Numerical Stability**: SVD is a numerically stable method, which means it can handle datasets with high precision and avoid issues related to floating-point arithmetic errors.\n",
        "\n",
        "- **Efficiency**: SVD is computationally efficient, especially for large datasets. Many optimized algorithms and libraries (like those in NumPy and scikit-learn) leverage SVD for fast computations.\n",
        "\n",
        "- **Handling Non-Square Matrices**: SVD can be applied directly to non-square matrices, which is useful since the data matrix in PCA is often not square. This flexibility makes SVD a versatile tool for dimensionality reduction.\n",
        "\n",
        "- **Direct Computation of Principal Components**: SVD directly provides the principal components (right singular vectors) and the singular values, which are related to the explained variance. This makes the process straightforward and avoids the need to compute the covariance matrix explicitly.\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "- **Robustness to Data Scaling**: SVD is less sensitive to the scaling of data compared to other methods. This robustness ensures that the principal components are accurately computed regardless of the data's scale.\n",
        "\n",
        "- **Memory Efficiency**: For large datasets, SVD can be more memory-efficient. Techniques like truncated SVD can be used to compute only the top principal components, reducing memory usage.\n",
        "\n",
        "- **Versatility**: SVD is a fundamental linear algebra technique used in various applications beyond PCA, such as signal processing, image compression, and solving linear systems. This versatility makes it a well-studied and widely implemented method.\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Pros and Cons\n",
        "\n",
        "Here are some advantages (left column) and disadvantages (right column) of PCA.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "**Advantages**\n",
        "\n",
        "+ Allows for visualizations\n",
        "+ Removes redundant variables\n",
        "+ Prevents overfitting\n",
        "+ Speeds up other ML algorithms\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "**Disadvantages**\n",
        "- reduces interpretability\n",
        "- can result in information loss\n",
        "- can be less effective than non-linear methods\n",
        ":::\n",
        "::::\n",
        "\n",
        "# t-SNE\n",
        "\n",
        "## What is t-SNE?\n",
        "\n",
        "- t-SNE stands for **t-Distributed Stochastic Neighbor Embedding**.\n",
        "- It is a **non-linear dimensionality reduction technique**.\n",
        "- Primarily used for **visualizing high-dimensional data** in 2 or 3 dimensions.\n",
        "- Developed by Laurens van der Maaten and Geoffrey Hinton in 2008.\n",
        "\n",
        "\n",
        "## How t-SNE Works\n",
        "\n",
        "- t-SNE converts high-dimensional Euclidean distances into **conditional probabilities**.\n",
        "- It aims to preserve the **local structure** of the data.\n",
        "- Uses a **heavy-tailed Student-t distribution** in the low-dimensional space to prevent crowding. This is where the $t$ in $t$-SNE comes from.\n",
        "\n",
        "\n",
        "We will describe how this process works.\n",
        "\n",
        "\n",
        "## Step 1: Compute Pairwise Similarities\n",
        "\n",
        "- Calculate pairwise similarities between points in the high-dimensional space.\n",
        "- Use a Gaussian distribution to convert distances into probabilities.\n",
        "- The similarity $p_{ij}$ between points $i$ and $j$ is given by:\n",
        "  $$\n",
        "  p_{j\\vert i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}.\n",
        "  $$\n",
        " - The $p_{j\\vert i}$ value represents the conditional probability that point $x_i$ would choose $x_j$ as its neighbor. \n",
        " - Note that $p_{i\\vert i} = 0$.\n",
        " - The t-SNE method uses the quantities $p_{ij} = \\frac{p_{j\\vert i} + p_{i\\vert j}}{2d}$ where $d$ is the dimension of the point. This is because the probabilities $p_{j\\vert i}$ and $p_{i\\vert j}$ are different.\n",
        "\n",
        "## Visualizing the Pairwise Similarities\n",
        "\n",
        "The Gaussian distribution is centered at point $x_i$ and the points $x_j$ that are further away have less probability of being chosen as neighbor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "y = (1/(std_dev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std_dev)**2)\n",
        "\n",
        "# Points on the x-axis\n",
        "points_x = [0, 1, 4, 4.5]\n",
        "points_y = [0, 0, 0, 0]\n",
        "colors = ['#009E73', '#009E73', '#CC79A7', '#CC79A7']\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(x, y, label='Gaussian Distribution')\n",
        "plt.scatter(points_x, points_y, color=colors, zorder=5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Gaussian Distribution with Individual Points')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing $\\sigma$\n",
        "\n",
        "How do we determine the variance for the Gaussian?\n",
        "\n",
        "- The variance is determined by a hyperparameter called the **perplexity**.\n",
        "- As a result the perplexity controls the effective number of neighbors. \n",
        "- A high perplexity means more neighbors for each point. A low perplexity means less neighbors are considered for each point.\n",
        "- The perplexity is defined as $\\operatorname{Perp}(P_i) = 2^{H(P_i)}$, where $H(P_i)$ is the entropy of $P_i$. $P_i$ is the probability distribution induced by $\\sigma_i$.\n",
        "- The entropy $H(P_i) = -\\sum_j p_{j\\vert i} \\log_2{(p_{j\\vert i})}$.\n",
        "- The value for $\\sigma_i$ is computed to produce a distribution $P_i$ which equals the chosen value of the perplexity. \n",
        "\n",
        "The perplexity commonly ranges  between 5 and 50.\n",
        "\n",
        "\n",
        "## Step 2: Define Low-Dimensional Map\n",
        "\n",
        "- Initialize points randomly in the low-dimensional space.\n",
        "- Define a similar probability distribution using a Student-t distribution:\n",
        "  $$\n",
        "  q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}.\n",
        "  $$\n",
        "- The heavy tails of the Student-t distribution help to spread out the points and prevent crowding of the points in the lower dimensional space.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "\n",
        "# Generate data for Gaussian distribution\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "gaussian_y = (1/(std_dev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std_dev)**2)\n",
        "\n",
        "# Generate data for Student's t-distribution with 10 degrees of freedom\n",
        "df = 1\n",
        "t_y = t.pdf(x, df)\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(x, gaussian_y, label='Gaussian Distribution')\n",
        "plt.plot(x, t_y, label=\"Student's t-Distribution\", linestyle='dashed')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Gaussian Distribution and Student\\'s t-Distribution')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Minimize Kullback-Leibler Divergence\n",
        "\n",
        "- Minimize the Kullback-Leibler (KL) divergence between the high-dimensional and low-dimensional distributions:\n",
        "  $$\n",
        "  KL(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n",
        "  $$\n",
        "- THe KL divergence measures how different the distribution $P$ is from the distribution $Q$. In other words, how different are each of the higher dimensional $P_i$ point distributions from the lower dimensional $Q_i$ distributions.\n",
        "- By minimizing this function, we ensure that the lower dimensional point clusters are similar to the higher dimensional clusters.\n",
        "- To minimize the KL divergence we use gradient descent to iteratively adjust the positions of points in the low-dimensional space.\n",
        "\n",
        "\n",
        "## Pros of t-SNE\n",
        "\n",
        "- **Excellent for visualizing complex datasets**: Reveals clusters and patterns that are not visible in high-dimensional space.\n",
        "- **Captures non-linear relationships**: Preserves local structure and relationships between points.\n",
        "- **Preserves local structure effectively**: Ensures that similar points in high-dimensional space remain close in the low-dimensional representation.\n",
        "\n",
        "## Cons of t-SNE\n",
        "\n",
        "- **Computationally intensive**: Requires significant computational resources, especially for large datasets.\n",
        "- **Results can vary depending on hyperparameters**: Parameters like perplexity and learning rate can significantly affect the outcome.\n",
        "- **Not suitable for all types of data analysis**: Primarily used for visualization, not for tasks like feature extraction or predictive modeling.\n",
        "\n",
        "## Case Study: Digits\n",
        "\n",
        "Let's consider how t-SNE performs clustering the MNIST digits datset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import datasets\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)\n",
        "plt.title('Digits in t-SNE Space')\n",
        "plt.xlabel('tSNE Component 1')\n",
        "plt.ylabel('tSNE Component 2')\n",
        "\n",
        "# Create a legend with discrete labels\n",
        "legend_labels = np.unique(y)\n",
        "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]\n",
        "plt.legend(handles, legend_labels, title=\"Digit Label\", loc=\"best\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## t-SNE vs PCA: Dimensionality Reduction\n",
        "\n",
        "- **PCA**: Linear method, reduces dimensions by maximizing variance along principal components.\n",
        "- **t-SNE**: Non-linear method, focuses on preserving local structure and relationships between points.\n",
        "\n",
        "\n",
        "## t-SNE vs PCA: Visualization\n",
        "\n",
        "- **PCA**: Good for understanding global structure and variance in the data.\n",
        "- **t-SNE**: Superior for visualizing clusters, local relationships, and non-linear structures.\n",
        "\n",
        "\n",
        "## t-SNE vs PCA: Computational Complexity\n",
        "\n",
        "- **PCA**: Faster and less computationally intensive, suitable for large datasets.\n",
        "- **t-SNE**: Slower, requires more computational resources, but provides more detailed visualizations.\n",
        "\n",
        "# Recap\n",
        "\n",
        "Today we discussed two dimensionality reduction techniques: PCA and t-SNE.\n",
        "\n",
        "We considered both on the MNIST digits dataset.\n",
        "\n",
        "In summary\n",
        "\n",
        "- PCA is a faster, linear dimensionality reduction technique.\n",
        "- PCA captures the maximum amount of variance in each principal direction.\n",
        "- No parameter adjustments are needed for the SVD.\n",
        "- t-SNE is a powerful tool for reducing and visualizing high-dimensional data.\n",
        "- Choose t-SNE for detailed visualizations of local structures and clusters.\n",
        "- t-SNE is sensitive to changes in it's parameters.\n",
        "- Both methods have their own strengths and are chosen based on the specific needs of the analysis.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/ds701/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}