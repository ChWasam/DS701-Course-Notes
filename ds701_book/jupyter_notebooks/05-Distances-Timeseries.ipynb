{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Distances and Time Series\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)\n",
        "\n",
        "We will start building some tools for making comparisons of data objects with particular attention to time series.\n",
        "\n",
        "Working with data, we can encounter a wide variety of different data objects\n",
        "\n",
        "::: {.incremental}\n",
        "* records of users,\n",
        "* images,\n",
        "* videos,\n",
        "* text (webpages, books),\n",
        "* strings (DNA sequences), and\n",
        "* time series.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "How can we compare them?\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Lecture Overview\n",
        "\n",
        "We cover the following topics in today's lecture\n",
        "\n",
        ":::: {.incremental}\n",
        "- feature space and matrix representations of data,\n",
        "- metrics, norms, similarity, and dissimilarity,\n",
        "- bit vectors, sets, and time series.\n",
        "::::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Time Series Data\n",
        "Some examples of time series data\n",
        "\n",
        ":::: {.incremental}\n",
        "- stock prices,\n",
        "- weather data,\n",
        "- electricity consumption,\n",
        "- website traffic,\n",
        "- retail sales, and\n",
        "- various economic indicators.\n",
        "::::\n",
        ":::\n",
        "\n",
        "## Feature space representation\n",
        "\n",
        "Usually a data object consists of a set of attributes.\n",
        "\n",
        "These are also commonly called __features.__\n",
        "\n",
        "* (\"J. Smith\", 25, \\$ 200,000)\n",
        "* (\"M. Jones\", 47, \\$ 45,000)\n",
        "\n",
        "If all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.\n",
        " \n",
        "* `(25, USD 200000)` $\\rightarrow \\begin{bmatrix}25\\\\200000\\end{bmatrix}$.\n",
        "\n",
        "Likewise If all features are binary then we can think of each data object as a binary vector in vector space.\n",
        "\n",
        "The space is called __feature space.__\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## One-hot encoding\n",
        ":::\n",
        "Vector spaces are such a useful tool that we often use them even for non-numeric data.\n",
        "\n",
        "For example, consider a categorical variable that can be only one of \"house\", \"tree\", or \"moon\". For such a variable, we can use a __one-hot__ encoding.  \n",
        "\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "We would encode as follows:\n",
        ":::\n",
        "\n",
        "::: {.incremental}\n",
        "* `house`: $[1, 0, 0]$\n",
        "* `tree`:  $[0, 1, 0]$\n",
        "* `moon`:  $[0, 0, 1]$\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "So an encoding of `(25, USD 200000, 'house')` could be: \n",
        "$$\\begin{bmatrix}25\\\\200000\\\\1\\\\0\\\\0\\end{bmatrix}.$$\n",
        "::: \n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Encodings\n",
        ":::\n",
        "\n",
        "We will see many other encodings that take non-numeric data and encode them into vectors or matrices.\n",
        "\n",
        "For example, there are vector or matrix encodings for\n",
        "\n",
        "::: {.incremental}\n",
        "* graphs,\n",
        "* images, and\n",
        "* text.\n",
        ":::\n",
        "\n",
        "## Matrix representation of data\n",
        "\n",
        "We generally store data in a matrix form as\n",
        "\n",
        "$$ \n",
        "\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n",
        "\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{i1} \\\\ \\vdots \\\\ x_{m1} \\end{array}&\n",
        "\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n",
        "\\begin{array}{c} x_{1j} \\\\ \\vdots \\\\ x_{ij} \\\\ \\vdots \\\\ x_{mj} \\end{array}&\n",
        "\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n",
        "\\begin{array}{c} x_{1n} \\\\ \\vdots \\\\ x_{in} \\\\ \\vdots \\\\ x_{mn} \\end{array}\n",
        "\\end{array}\\right]}^{\\mbox{$n$ features}} \n",
        "$$\n",
        "\n",
        "The number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "A metric is a function $d(x, y)$ that satisfies the following properties.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "::: {.incremental}\n",
        "* $d(x, x) = 0$\n",
        "* $d(x, y) > 0 \\hspace{1cm} \\forall x\\neq y$ (positivity)\n",
        "* $d(x, y) = d(y, x)$ (symmetry)\n",
        "* $d(x, y)\\leq d(x, z) + d(z, y)$ (triangle inequality)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        ":::: {.fragment}\n",
        "![](figs/TriangleInequality.png){fig-align=\"center\"}\n",
        "::::\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.fragment}\n",
        "We can use a metric to determine how __similar__ or __dissimilar__ two objects are.\n",
        "\n",
        "A metric is a measure of the dissimilarity between two objects. The larger the\n",
        "measure, the more dissimilar the objects are.\n",
        "\n",
        "If the objects are vectors, then the metric is also commonly called a __distance__.\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Sometimes we will use \"distance\" informally, i.e., to refer to a similarity or\n",
        "dissimilarity function even if we are not sure it is a metric.   \n",
        "\n",
        "We'll try to say \"dissimilarity\" in those cases though.\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Distance Matrices\n",
        ":::\n",
        "\n",
        "The distance matrix is defined as\n",
        "\n",
        "$$ \n",
        "\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n",
        "\\overbrace{\\left[\\begin{array}{ccccc}\n",
        "\\begin{array}{c} 0  \\\\  d(x_1, x_2) \\\\ d(x_1,x_3) \\\\ \\vdots \\\\ d(x_1,x_m)  \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\  0      \\\\ d(x_2,x_3) \\\\ \\vdots \\\\ d(x_2,x_m)  \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ 0      \\\\ \\vdots \\\\ \\cdots   \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\ddots \\\\ d(x_{m-1},x_m)   \\end{array}  &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\;     \\\\[6pt] 0 \\end{array} &\n",
        "\\end{array}\\right]}^{\\mbox{$m$ data objects}},\n",
        "$$\n",
        "\n",
        "where $x_i$ denotes the $i$-th column of the data matrix $X$.\n",
        "\n",
        "## Norms\n",
        "\n",
        "Let $\\mathbf{u}, \\mathbf{v}\\in\\mathbb{R}^{n}$ and $a\\in\\mathbb{R}$. The vector function $p(\\mathbf{v})$ is called a __norm__ if\n",
        "\n",
        ":::: {.fragment}\n",
        "\n",
        "::: {.incremental}\n",
        "* $p(a\\mathbf{v}) = |a|p(\\mathbf{v})$,\n",
        "* $p(\\mathbf{u} + \\mathbf{v}) \\leq p(\\mathbf{u}) + p(\\mathbf{v})$,\n",
        "* $p(\\mathbf{v}) = 0$ if and only if $\\mathbf{v} = 0$.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "__Every norm defines a corresponding metric.__ \n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "In particular If $p()$ is a norm, then $d(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}-\\mathbf{y})$ is a metric.\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_p$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_p$ norm\n",
        ":::\n",
        "\n",
        "A general class of norms are called __$\\ell_p$__ norms, where $p \\geq 1.$\n",
        "\n",
        "$$\\Vert \\mathbf{x} \\Vert_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}}.$$ \n",
        "\n",
        "The corresponding distance that an $\\ell_p$ norm defines is called the _Minkowski distance._\n",
        "\n",
        "$$\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_p = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}}.$$\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_2$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_2$ norm\n",
        ":::\n",
        "\n",
        "A special -- but very important -- case is the $\\ell_2$ norm.\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2}.\n",
        "$$\n",
        "\n",
        "We've already mentioned it: it is the __Euclidean__ norm.\n",
        "\n",
        "The distance defined by the $\\ell_2$ norm is the same as the Euclidean distance between two vectors $\\mathbf{x}, \\mathbf{y}$.\n",
        "\n",
        "$$ \n",
        "\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2  = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_1$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_1$ norm\n",
        ":::\n",
        "\n",
        "Another important special case is the $\\ell_1$ norm.\n",
        "\n",
        "$$ \\Vert \\mathbf{x} \\Vert_1 = \\sum_{i=1}^d |x_i|.$$\n",
        "\n",
        "This defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:\n",
        "\n",
        "$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 = \\sum_{i=1} |x_i - y_i|.$$\n",
        "\n",
        "![](figs/L05-manhattan-distance.png){fig-align=\"center\"}\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_\\infty$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_\\infty$ norm\n",
        ":::\n",
        "\n",
        "If we take the limit of the $\\ell_p$ norm as $p$ gets large we get the $\\ell_\\infty$ norm.  \n",
        "\n",
        "We have that\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} \\vert .\n",
        "$$\n",
        "\n",
        "::: {.fragment}\n",
        "What is the metric that this norm induces?\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## $\\ell_0$ norm\n",
        "\n",
        "Another related idea is the $\\ell_0$ \"norm,\" which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.\n",
        "\n",
        "Note that this is __not__ a norm, but it gets called that anyway.   \n",
        "\n",
        "This \"norm\" simply counts the number of __nonzero__ elements in a vector.\n",
        "\n",
        "This is called the vector's __sparsity.__\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Visualizing norms\n",
        ":::\n",
        "\n",
        "Here is the notion of a \"circle\" under each of three norms.\n",
        "\n",
        "That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L5-Vector-Norms.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "<br><br>\n",
        "$|x_1| + |x_2| = 1$\n",
        "\n",
        "<br><br>\n",
        "$\\sqrt{x_1^2 + x_2^2} = 1$\n",
        "\n",
        "<br><br>\n",
        "$\\max(|x_1|, |x_2|) = 1$\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "[Source](https://commons.wikimedia.org/w/index.php?curid=678101)\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## What norm should I use?\n",
        "\n",
        "The choice of norm depends on the characteristics of your data and the problem you're trying to solve.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_1$ norm__\n",
        "\n",
        "- Use when your data is sparse.\n",
        "- Robust to outliers.\n",
        ":::\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_2$ norm__\n",
        "\n",
        "- Use when measuring distances in Euclidean space.\n",
        "- Smooth and differentiable.\n",
        ":::\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_\\infty$ norm__\n",
        "\n",
        "- Use when you need uniform bounds.\n",
        "- Maximum deviation.\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Similarity and Dissimilarity Functions\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"44%\"}\n",
        "Similarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.\n",
        "\n",
        ":::: {.fragment}\n",
        "__Examples__\n",
        "\n",
        ":::: {.incremental}\n",
        "- cosine similarity,\n",
        "- Jaccard similarity _(intersection over union)_.\n",
        "::::\n",
        "::::\n",
        ":::\n",
        "::: {.column width=\"55%\"}\n",
        "Dissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.\n",
        "\n",
        ":::: {.fragment}\n",
        "__Examples__\n",
        "\n",
        ":::: {.incremental}\n",
        "- Manhattan distance,\n",
        "- Hamming distance.\n",
        "::::\n",
        "::::\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### Similarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Similarity\n",
        ":::\n",
        "\n",
        "We know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them\n",
        "\n",
        "$$ \\cos(\\theta) = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert} \\equiv \\cos(\\mathbf{x}, \\mathbf{y})  .$$\n",
        "\n",
        "This value is close to 1 when $\\mathbf{x} \\approx \\mathbf{y}$. We can use this formula to define a __similarity__ function called the __cosine similarity__ $\\cos(\\mathbf{x}, \\mathbf{y})$.\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### Dissimilarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Dissimilarity\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "Given a similarity function $s(\\mathbf{x}, \\mathbf{y})$, how could we convert it to a dissimilarity function $d(\\mathbf{x}, \\mathbf{y})$?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Two straightforward ways of doing that are:\n",
        "\n",
        "$$d(\\mathbf{x},\\mathbf{y}) = 1\\,/\\,s(\\mathbf{x},\\mathbf{y})$$\n",
        "\n",
        "or \n",
        "\n",
        "$$d(\\mathbf{x},\\mathbf{y}) = k - s(\\mathbf{x},\\mathbf{y})$$\n",
        "\n",
        "for some properly chosen $k$.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "For cosine similarity, one often uses:\n",
        "    \n",
        "$$ d(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos(\\mathbf{x}, \\mathbf{y})$$\n",
        "::::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Note however that this is __not a metric!__\n",
        ":::\n",
        "\n",
        "\n",
        "## Bit vectors and Sets\n",
        "\n",
        "When working with bit vectors, the $\\ell_1$ metric is commonly used and is called the __Hamming__ distance.\n",
        "\n",
        "![](figs/L5-hamming-1.png){fig-align=\"center\"}\n",
        "\n",
        "This has a natural interpretation: \"how well do the two vectors match?\"\n",
        "\n",
        "Or: \"What is the smallest number of bit flips that will convert one vector into the other?\"\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Hamming distance\n",
        ":::\n",
        "\n",
        "![](figs/L5-hamming-2.png){fig-align=\"center\"}\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "In other cases, the Hamming distance is not a very appropriate metric.\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Hamming distance with sets\n",
        ":::\n",
        "\n",
        "Consider the case in which the bit vector is being used to represent a set.\n",
        "\n",
        "In that case, Hamming distance measures the __size of the set difference.__\n",
        "\n",
        "For example, consider two documents. We will use bit vectors to represent the sets of words in each document.\n",
        "\n",
        ":::: {.incremental}\n",
        "* Case 1: both documents are large, almost identical, but differ in 10 words.\n",
        "* Case 2: both documents are small, disjoint, have 5 words each.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "What matters is not just the size of the set difference, but the size of the intersection.\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard similarity\n",
        ":::\n",
        "This leads to the _Jaccard_ similarity:\n",
        "\n",
        "$$\n",
        "J_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n",
        "$$\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "This takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$\n",
        "\n",
        "In fact, this is a __metric!__\n",
        "\n",
        "$$\n",
        "J_{Dist}(\\mathbf{x}, \\mathbf{y}) = 1- \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/L5-jaccard-1.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard Similarity Example 1\n",
        ":::\n",
        "    \n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Let's revisit the previously introduces cases comparing documents.\n",
        ":::\n",
        "\n",
        "Case 1: Very large almost identical documents.\n",
        "\n",
        "![](figs/L5-jaccard-2.png){fig-align=\"center\"}\n",
        "\n",
        "Here $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is almost 1.\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard Similarity Example 2\n",
        ":::\n",
        "\n",
        "Case 2: Very small disjoint documents.\n",
        "\n",
        "![](figs/L5-jaccard-3.png){fig-align=\"center\"}\n",
        "\n",
        "Here $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is 0.\n",
        "\n",
        "## Time Series\n",
        "\n",
        "A time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.\n",
        "\n",
        "Some examples are\n",
        "\n",
        "::: {.incremental}\n",
        "* stock prices,\n",
        "* the volume of sales over time, and\n",
        "* daily temperature readings.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "A time series database is a large collection of time series.\n",
        ":::\n",
        "\n",
        "## Similarity of Time Series\n",
        "\n",
        "Suppose we wish to compare the following time series.\n",
        "\n",
        "::: {.incremental}\n",
        "* Stock price movements for companies over a time interval.\n",
        "* The motion data of two people walking.\n",
        "* Credit usage patterns for bank clients.\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "How should we measure the \"similarity\" of these time series?\n",
        "::::\n",
        "\n",
        "::: {.fragment}\n",
        "There are two problems to address.\n",
        "\n",
        "::: {.incremental}\n",
        "1. Defining a meaningful similarity (or distance) function.\n",
        "2. Finding an efficient algorithm to compute it.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Norm-based Similarity Measures\n",
        "\n",
        "We could just view each sequence as a vector.\n",
        "\n",
        "Then we could use a $p$-norm, e.g., $\\ell_1, \\ell_2,$ or $\\ell_p$ to measure similarity.\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "__Advantages__\n",
        "\n",
        "::: {.incremental}    \n",
        "1. Easy to compute - linear in the length of the time series (O(n)).\n",
        "2. It is a metric.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Beware of Norm-based Similarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "__Disadvantage__\n",
        "\n",
        "1. May not be __meaningful!__\n",
        "\n",
        "We may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n",
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "__Disadvantage__\n",
        "1. May not be __meaningful!__\n",
        "\n",
        "![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n",
        "\n",
        "We may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "However, according to Euclidean distance: \n",
        "\n",
        "$$ \\Vert \\mathbf{ts1} - \\mathbf{ts2} \\Vert_2 = 26.9,$$\n",
        "\n",
        "while \n",
        "\n",
        "$$ \\Vert \\mathbf{ts1} - \\mathbf{ts3} \\Vert_2 = 23.2.$$\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## Feature Engineering\n",
        "\n",
        "In general, there may be different aspects of a time series that are important in different settings.\n",
        "\n",
        "::: {.fragment}\n",
        "The first step therefore is to ask yourself \"what is important about time series in my application?\"\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "This is an example of __feature engineering.__\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Feature Engineering\n",
        ":::\n",
        "\n",
        "Feature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.\n",
        "\n",
        "::: {.fragment}\n",
        "A reasonable approach is to\n",
        "\n",
        "::: {.incremental}    \n",
        "* extract the relevant features,\n",
        "* use a simple method (e.g., a norm) to define similarity over those features.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "In the case above, one might think of using \n",
        "\n",
        "::: {.incremental}\n",
        "* Fourier coefficients (to capture periodicity),\n",
        "* histograms,\n",
        "* or something else!\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## Dynamic Time Warping\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Bump Hunting\n",
        ":::\n",
        "\n",
        "One case that arises often is something like the following:  \"bump hunting\"\n",
        "\n",
        "![](figs/L5-DTW-1.png){fig-align=\"center\"} \n",
        "\n",
        "Both time series have the same key characteristics: four bumps.\n",
        "\n",
        "But a one-to-one match (ala Euclidean distance) will not detect the similarity.\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "(Be sure to think about why Euclidean distance will fail here.)\n",
        ":::\n",
        "\n",
        "A solution to this is called __dynamic time warping.__\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Dynamic Time Warping\n",
        ":::\n",
        "\n",
        "The basic idea is to allow acceleration or deceleration of signals along the time dimension.\n",
        "\n",
        "::: {.fragment}\n",
        "__Classic applications__\n",
        "\n",
        "::: {.incremental}\n",
        "* speech recognition\n",
        "* handwriting recognition\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "Specifically\n",
        "\n",
        "::: {.incremental}\n",
        "* Consider $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_m$.\n",
        "* We are allowed to modify each sequence by inserting, deleting, or matching elements to form $X'$ and $Y'$.\n",
        "* We then calculate, e.g., Euclidean distance between the extended sequences $X'$ and $Y'$.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Visualizing DTW\n",
        ":::\n",
        "\n",
        "There is a simple way to visualize this algorithm.\n",
        "\n",
        "Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure).\n",
        "\n",
        "![](figs/L5-DTW-2.png){fig-align=\"center\"}\n",
        "\n",
        "$M$ measures the amount of error we get if we match $x_i$ with $y_j$. \n",
        "\n",
        "So we seek a __path through $M$ that minimizes the total error.__\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## DTW restrictions\n",
        ":::\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "We need to start in the lower left and work our way up via a continuous path.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "The basic restrictions on path are:\n",
        "    \n",
        "::: {.incremental}\n",
        "* Monotonicity\n",
        "  * The path should not go down or to the left.\n",
        "* Continuity\n",
        "  * No elements may be skipped in a sequence.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "This can be solved via dynamic programming. However, the algorithm is still quadratic in $n$.\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Improving DTW \n",
        ":::\n",
        "\n",
        "To reduce the computational complexity, we can put a restriction on the amount that the path can deviate from the diagonal.\n",
        "\n",
        "The basic algorithm looks like this:\n",
        "\n",
        "```\n",
        "D[0, 0] = 0\n",
        "for i in range(n):\n",
        "  for j in range(m):\n",
        "    D[i,j] = M[i,j] + \n",
        "             min( D[i-1, j],    # insertion\n",
        "                  D[i, j-1],    # deletion\n",
        "                  D[i-1, j-1] ) # match\n",
        "```\n",
        "\n",
        "Unfortunately, the algorithm is still quadratic in $n$ -- it is $\\mathcal{O}(nm)$.\n",
        "\n",
        "Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.\n",
        "\n",
        "This is implemented by not allowing the path to pass through locations where $|i - j| > w$.\n",
        "\n",
        "Then the algorithm is $\\mathcal{O}(nw)$.\n",
        "\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "## From Time series to Strings\n",
        "\n",
        "A closely related idea concerns strings.\n",
        "\n",
        "The key point is that, like time series, strings are __sequences__.\n",
        "\n",
        "Given two strings, one way to define a 'distance' between them is:\n",
        "\n",
        "* the minimum number of __edit operations__ that are needed to transform one string into the other.\n",
        "\n",
        "Edit operations are insertion, deletion, and substitution of single characters.\n",
        "\n",
        "This is called __edit distance__ or __Levenshtein distance.__\n",
        "\n",
        "For example, given strings:\n",
        "\n",
        "``s = VIVALASVEGAS``\n",
        "    \n",
        "and\n",
        "\n",
        "``t = VIVADAVIS``\n",
        "\n",
        "\n",
        "we would like to \n",
        "\n",
        "* compute the edit distance, and\n",
        "* obtain the optimal __alignment__.\n",
        "\n",
        "\n",
        "![](figs/viva-las-vegas.png){fig-align=\"center\"}\n",
        "\n",
        "[Source](http://medicalbioinformaticsgroup.de/downloads/lectures/Algorithmen_und_Datenstrukturen/WS15-16/aldabi_ws15-16_woche6.pdf)\n",
        "\n",
        "A dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__\n",
        "\n",
        "In bioinformatics this algorithm is called __\"Smith-Waterman\" sequence alignment.__\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Recap\n",
        "\n",
        "We covered the following topics\n",
        "\n",
        ":::: {.incremental}\n",
        "- reviewed representations of data,\n",
        "- discussed metrics and norms,\n",
        "- discussed similarity and dissimilarity functions,\n",
        "- introduced time series, \n",
        "- feature engineering, and\n",
        "- dynamic time warping.\n",
        ":::: \n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}