{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: NN I -- Gradient Descent\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/23-NN-I-Gradient-Descent.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mp\n",
        "import sklearn\n",
        "import networkx as nx\n",
        "from IPython.display import Image, HTML\n",
        "\n",
        "import laUtilities as ut\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "The content builds upon \n",
        "\n",
        "* Andrej Karpathy's excellent [video](https://youtu.be/VMj-3S1tku0?si=9HKPIq36EnHektSm) on building _micrograd_ and \n",
        "* _Understanding Deep Learning_ [book preprint](https://udlbook.github.io/udlbook/) by Simone Prince \n",
        "* as well as many other sources cited below.\n",
        ":::\n",
        "\n",
        "\n",
        "## The \"Unreasonable\" Effectiveness of Deep Neural Networks\n",
        "\n",
        "Deep Neural Networks have been effective in many applications.\n",
        "\n",
        "![](figs/NN-figs/IntroModels.svg){width=\"75%\"}\n",
        "\n",
        "---\n",
        "\n",
        "![](figs/NN-figs/IntroModels2a.svg){width=\"75%\"}\n",
        "\n",
        "[Understanding Deep Learning, Simon J.D. Prince, MIT Press, 2023](http://udlbook.com)\n",
        "\n",
        "## Emergent Behavior in Pre-Trained Large Language Models\n",
        "\n",
        "![Emergence](./figs/NN-figs/EmergentAbilitiesFig2.png)\n",
        "\n",
        "[Emergent Abilities of Large Language Models.](https://arxiv.org/abs/2206.07682) J. Wei et al., Oct. 26, 2022.\n",
        "\n",
        "## Theory Sometimes Follows Invention\n",
        "\n",
        "| Invention | Theory |\n",
        "| --------- | ------ |\n",
        "| Telescope (1608) | Optics (1650-1700) |\n",
        "| Steam Engine (1695-1715) | Thermodynamics (1824...) |\n",
        "| Electromagnetism (1820) | Electrodynamics (1821) |\n",
        "| Sailboat (??) | Aerodynamics (1757), Hydrodynamics (1738) |\n",
        "| Airplane (1885-1905) | Wing Theory (1907-1918) |\n",
        "| Computer (1941-1945) | Computer Science (1950-1960) |\n",
        "| Teletype (1906) | Information Theory (1948) |\n",
        "\n",
        "* But then when theory is developed it can more quickly improve invention\n",
        "* The same can be said for Neural Networks. The theory to make them work is well understood. The theory of why they work is still developing.\n",
        "* We'll balance theory and application\n",
        "\n",
        "[The Power and Limits of Deep Learning](https://learning.acm.org/techtalks/powerandlimitsdl), Yann LeCun, March 2019.\n",
        "\n",
        "Underlying all these techniques is the idea of applying optimization techniques\n",
        "to minimize some kind of \"loss\" function.\n",
        "\n",
        "## Loss Functions for Model Fitting\n",
        "\n",
        "Most of the machine learning we have studied this semester is based on the idea\n",
        "that we have a model that is _parameterized_, and our goal is to find good\n",
        "settings for the parameters.\n",
        "\n",
        "We have seen example after example of this problem.\n",
        "\n",
        "* In $k$-means, our goal was to find $k$ cluster centroids, so that the $k$-means\n",
        "  objective was minimized.\n",
        "* In linear regression, our goal was to find a parameter vector $\\beta$ so that\n",
        "  sum of squared error $\\Vert \\mathbf{y} - \\hat{\\mathbf{y}}\\Vert_2$ was minimized.\n",
        "* In the support vector machine, our goal was to find a parameter vector $\\theta$\n",
        "  so that classification error was minimized.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Loss Functions for Model Fitting\n",
        ":::\n",
        "\n",
        "And similarly we'll want to find good parameter settings in neural networks.\n",
        "\n",
        "It's time now to talk about how, in general, one can find \"good settings\" for the\n",
        "parameters in problems like these.\n",
        "\n",
        "What allows us to unify our approach to many such problems is the following:\n",
        "\n",
        "First, we start by defining an error function, generally called a __loss__\n",
        "function, to describe how well our method is doing.\n",
        "\n",
        "And second, we choose loss functions that are __differentiable__ with respect to\n",
        "the parameters.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Loss Functions for Model Fitting\n",
        ":::\n",
        "\n",
        "These two requirements mean that we can think of the parameter tuning problem\n",
        "using surfaces like these:\n",
        "\n",
        "![](figs/L23-convex_cost_function.jpeg){width=\"75%\"}\n",
        "\n",
        "Imagine that the $x$ and $y$ axes in these pictures represent parameter settings.\n",
        "That is, we have two parameters to set, corresponding to the values of $x$ and $y$.\n",
        "\n",
        "---\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "![](figs/L23-convex_cost_function.jpeg){width=\"25%\"}\n",
        ":::\n",
        "\n",
        "For each $(x, y)$ setting, the $z$-axis shows the value of the loss function. \n",
        "\n",
        "What we want to do is find the minimum of a surface, corresponding to the\n",
        "parameter settings that minimize loss.\n",
        "\n",
        "Notice the difference between the two kinds of surfaces.    \n",
        "\n",
        "The surface on the left corresponds to a __strictly convex__ loss function.   \n",
        "If we find a local minimum of this function, it is a global minimum.\n",
        "\n",
        "The surface on the right corresponds to a __non-convex__ loss function.\n",
        "There are local minima that are not globally minimal.\n",
        "\n",
        "---\n",
        "\n",
        "Both kinds of loss functions arise in machine learning.\n",
        "\n",
        "For example, convex loss functions arise in\n",
        "\n",
        "* Linear regression\n",
        "* Logistic regression\n",
        "\n",
        "While non-convex loss functions arise in\n",
        "\n",
        "* $k$-means\n",
        "* Gaussian Mixture Modeling\n",
        "* and of course neural networks\n",
        "\n",
        "## Gradient Descent Intuitively\n",
        "\n",
        "The intuition of gradient descent is the following.   \n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/L23-fog-in-the-mountains.jpeg){width=\"100%\"}\n",
        "<!-- Image credit http://nederlandliving.com/?p=1931 -->\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "Imagine you are lost in the mountains, and it is foggy out.  \n",
        "\n",
        "You want to find a valley.  But since it is foggy, you can only see the local\n",
        "area around you.\n",
        ":::\n",
        "::::\n",
        "\n",
        "The natural thing to do is:\n",
        "\n",
        "1. Look around you 360 degrees.  \n",
        "2. Observe in which direction the ground is sloping downward most steeply.  \n",
        "3. Take a few steps in that direction.  \n",
        "4. Repeat the process ... until the ground seems to be level.\n",
        "\n",
        "---\n",
        "\n",
        "The key to this intuitive idea is formalizing the idea of \"direction of steepest\n",
        "descent.\"\n",
        "\n",
        "This is where the differentiability of the loss function comes into play.\n",
        "\n",
        "As long as the loss function is _locally_ differentiable, we can define the \n",
        "direction of steepest descent (really, ascent).\n",
        "\n",
        "That direction is called the __gradient.__\n",
        "\n",
        "## Derivatives on Single Variable Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll build up to concept of gradient by starting with derivatives on single\n",
        "variable functions.\n",
        "\n",
        "Let's start with a simple quadratic function.\n",
        "\n",
        "$$\n",
        "f(x) = 3x^2 - 4x +5\n",
        "$$\n",
        "\n",
        "Which we can write in python as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def f(x):\n",
        "  return 3*x**2 - 4*x + 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "And we can plot it.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "plt.plot(xs, ys);\n",
        "plt.title('$f(x) = 3x^2 - 4x + 5$');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "Let's assume for a minute that this is our loss function that we are minimizing.\n",
        "\n",
        "__Question__\n",
        "\n",
        "What do we know about where the minimum is in terms of the slope of the curve?\n",
        "\n",
        "__Answer__\n",
        "\n",
        "It is necessary but _not sufficient_ that the slope be zero.\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "__Question__ \n",
        "\n",
        "How do we calculate the slope?\n",
        "\n",
        "__Answer__\n",
        "\n",
        "---\n",
        "\n",
        "We take the derivative, denoted\n",
        "\n",
        "$$\n",
        "\\frac{d f(x)}{dx} \\hspace{10pt} \\textrm{Leibniz' notation} \n",
        "$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "f'(x) \\hspace{10pt} \\textrm{Lagrange's notation} \n",
        "$$\n",
        "\n",
        "\n",
        "You may see both notations. The nice thing about Leibniz' notation is that it is\n",
        "easy to express _partial derivatives_ when we get to multivariate differentiation,\n",
        "which we'll get to shortly.\n",
        "\n",
        "---\n",
        "\n",
        "We can take the derivate of the $f(x)$\n",
        "\n",
        "$$\n",
        "f(x) = 3x^2 - 4x +5\n",
        "$$\n",
        "\n",
        "By definition of the [derivative](https://en.wikipedia.org/wiki/Derivative), the\n",
        "function $f(x)$ is differentiable at $x$ if\n",
        "\n",
        "$$\n",
        "\\lim_{h\\to 0} \\frac{f(a+h)-f(a)}{h} \n",
        "$$\n",
        "\n",
        "exists at $x$. And in fact, that limit approaches the value of the derivative in the limit.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the function f(x)\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x + 5\n",
        "\n",
        "# Define the derivative f'(x)\n",
        "def df(x):\n",
        "    return 6 * x - 4\n",
        "\n",
        "# Function to plot f(x) and its tangent line at x = x_value\n",
        "def plot_with_tangents(x_value, h_value):\n",
        "    # Generate x values for the function\n",
        "    x = np.linspace(-5, 5, 400)\n",
        "    y = f(x)\n",
        "\n",
        "    #h_value = 10**(-e_value)\n",
        "    \n",
        "    # Compute the slope and function value at x = x_value\n",
        "    slope_at_x_value = df(x_value)\n",
        "    limit_at_x_value = (f(x_value + h_value) - f(x_value)) / h_value\n",
        "    f_at_x_value = f(x_value)\n",
        "    f_at_x_plus_h_value = f(x_value + h_value)\n",
        "    \n",
        "    # Generate x and y values for the tangent line near x = x_value\n",
        "    x_tangent = np.linspace(x_value - 2, x_value + 2, 400)\n",
        "    y_tangent = f_at_x_value + slope_at_x_value * (x_tangent - x_value)\n",
        "    y_limit_tangent = f_at_x_value + limit_at_x_value * (x_tangent - x_value)\n",
        "    \n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y, label='f(x) = 3x^2 - 4x + 5')\n",
        "    plt.plot(x_tangent, y_tangent, linestyle='--', label=f'Asymptotic slope of {df(x_value):.2f} at x = {x_value:.2f}')\n",
        "    plt.plot(x_tangent, y_limit_tangent, linestyle='-.', label=f'Asymptotic limit of {limit_at_x_value:.2f} at x = {x_value:.2f}, h = {h_value:.3f}')\n",
        "    plt.scatter([x_value], [f_at_x_value], color='red')  # point of tangency\n",
        "    plt.scatter([x_value+h_value], [f_at_x_plus_h_value], color='red')  # point of tangency\n",
        "    plt.title('Plot of the function f(x) = 3x^2 - 4x + 5')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('f(x)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Create an interactive widget\n",
        "widgets.interact(plot_with_tangents, x_value=widgets.FloatSlider(value=-2, min=-5, max=5, step=0.1), h_value=widgets.FloatSlider(value=1, min=.001, max=2, step=.001));\n",
        "#widgets.interact(plot_with_tangents, h_value=widgets.FloatSlider(value=1, min=1, max=10, step=1));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We use the rules of derivatives. See for example the derivative\n",
        "[rules for basic functions](https://en.wikipedia.org/wiki/Derivative#Rules_for_basic_functions), \n",
        "e.g.\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx} x^a = ax^{a-1}, \n",
        "  \\quad \\textrm{e.g.} \\quad \\frac{d}{dx} 3x^2 = 6x \n",
        "  \\quad \\textrm{,} \\quad \\frac{d}{dx} 6x = 6\n",
        "  \\quad \\textrm{,} \\quad \\frac{d}{dx} 6 = 0 \n",
        "$$\n",
        "\n",
        "so\n",
        "\n",
        "$$\n",
        "\\frac{d f(x)}{dx} = 6x - 4 \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# define the derivate of f as df\n",
        "def df(x):\n",
        "    return 6*x - 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can solve for where $\\frac{d}{dx} f(x) = 0$\n",
        "\n",
        "$$\n",
        "6x - 4 = 0 \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Evaluate df and f for x where df = 0 \n",
        "x_zero = 2/3\n",
        "\n",
        "# Evaluate df\n",
        "df(x_zero)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# And f at that value is\n",
        "f(x_zero)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Which we can add to the plot of $f(x)$ to see if it indeed is at the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "plt.plot(xs, ys)\n",
        "\n",
        "# Add a circle point at (2, 5)\n",
        "plt.plot([x_zero], [f(x_zero)], 'o')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now as Wikipedia [states](https://en.wikipedia.org/wiki/Derivative),\n",
        "\n",
        "> The derivative of a function of a single variable at a chosen input value, when\n",
        "it exists, is the slope of the tangent line to the graph of the function at that point.\n",
        "\n",
        "## Slope of a Function\n",
        "\n",
        "We can explore the tangent at different x-values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the function f(x)\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x + 5\n",
        "\n",
        "# Define the derivative f'(x)\n",
        "def df(x):\n",
        "    return 6 * x - 4\n",
        "\n",
        "# Function to plot f(x) and its tangent line at x = x_value\n",
        "def plot_with_tangent(x_value):\n",
        "    # Generate x values for the function\n",
        "    x = np.linspace(-5, 5, 400)\n",
        "    y = f(x)\n",
        "\n",
        "    # Compute the slope and function value at x = x_value\n",
        "    slope_at_x_value = df(x_value)\n",
        "    f_at_x_value = f(x_value)\n",
        "\n",
        "    # Generate x and y values for the tangent line near x = x_value\n",
        "    x_tangent = np.linspace(x_value - 2, x_value + 2, 400)\n",
        "    y_tangent = f_at_x_value + slope_at_x_value * (x_tangent - x_value)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, y, label='$f(x) = 3x^2 - 4x + 5$')\n",
        "    plt.plot(x_tangent, y_tangent, linestyle='--', label=f'Asymptotic slope of {df(x_value):.2f} at x = {x_value:.2f}')\n",
        "    plt.scatter([x_value], [f_at_x_value], color='red')  # point of tangency\n",
        "    plt.title('Plot of the function $f(x) = 3x^2 - 4x + 5$')\n",
        "    plt.xlabel('$x$')\n",
        "    plt.ylabel('$f(x)$')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Create an interactive widget\n",
        "widgets.interact(plot_with_tangent, x_value=widgets.FloatSlider(value=-2, min=-5, max=5, step=0.1));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slope Shows Influence of $x$ on $f$\n",
        "\n",
        "__Important Note:__\n",
        "\n",
        "* if the slope is negative, then by increasing $x$, we will decrease $f(x)$.\n",
        "* And if the slope is positive, then decreasing $x$ will decrease $f(x)$.\n",
        "\n",
        "## Interpretation of Slope\n",
        "\n",
        "Let's illustrate with this function $f(x)$ a useful way to interpret the slope.\n",
        "\n",
        "In the graph above, with $x=-2$, we see the slope, call it $m$, is -16. What that\n",
        "means is that when we change the value of $x$, the impact on the ouptut will\n",
        "roughly be _amplified_ by $m$, or -16 when $x=2$.\n",
        "\n",
        "Put another way, the slope (equivalently the derivative) of a function $f(x)$ at\n",
        "an input $x$ indicates how sensitive the output is to changes in the input.\n",
        "\n",
        "> This will be key to understanding how we have to tweak the weights of our model\n",
        "> to minimize our loss function.\n",
        "\n",
        "## Gradient Descent on a Linear Regression Model\n",
        "\n",
        "Now, in 2 or higher dimensions we can there many directions that will descend,\n",
        "but we want to pick the direction of steepest descent. We'll formalize that idea.\n",
        "\n",
        "As long as the loss function is _locally_ differentiable, we can define the\n",
        "direction of steepest descent.\n",
        "\n",
        "That direction is given by the _negative_ of the __gradient.__\n",
        "\n",
        "The gradient is a generalization of the slope of a line.\n",
        "\n",
        "Let's say we have a loss function $\\mathcal{L}(\\mathbf{w})$.   \n",
        "\n",
        "The components of $\\mathbf{w}\\in\\mathbb{R}^n$ are the parameters we want to optimize.\n",
        "\n",
        "Just a reminder that $\\mathbf{w} \\in \\mathbb{R}^n$ denotes an $n$-dimensional vector.\n",
        "\n",
        "---\n",
        "\n",
        "For linear regression, the loss function could be squared loss:\n",
        "    \n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w}) = \\Vert\\mathbf{y} - \\hat{\\mathbf{y}}\\Vert^2 \n",
        "$$\n",
        "\n",
        "where $\\hat{\\mathbf{y}}$ is our estimate, ie, $\\hat{\\mathbf{y}} = X\\mathbf{w}$ so that \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w}) = \\Vert\\mathbf{y} - X\\mathbf{w}\\Vert^2 \n",
        "$$\n",
        "\n",
        "To find the gradient, we take the partial derivative of our loss function with respect to each parameter:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w_i} \n",
        "$$\n",
        "\n",
        "and collect all the partial derivatives into a vector of the same shape as $\\mathbf{w}$:\n",
        "\n",
        "$$\n",
        "\\nabla_\\mathbf{w}\\mathcal{L} = \\begin{bmatrix}\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_1}\\\\\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_2}\\\\\n",
        "    \\vdots \\\\\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_n}\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "When you see the notation  $\\nabla_\\mathbf{w}\\mathcal{L},$ think of it as the\n",
        "derivative with respect to the vector $\\mathbf{w}$.\n",
        "\n",
        "The _nabla_ symbol, $\\nabla$, denotes the _vector differentiator operator_ called _del_.\n",
        "\n",
        "---\n",
        "\n",
        "It turns out that if we are going to take a small step of unit length, then the\n",
        "gradient is the direction that maximizes the change in the loss function.\n",
        "\n",
        "<!-- Image credit https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/ -->\n",
        "\n",
        "![](figs/L23-gradient-of-convex.png){width=\"60%\"}\n",
        "\n",
        "As you can see from the above figure, in general the gradient varies depending on\n",
        "where you are in the parameter space.\n",
        "\n",
        "---\n",
        "\n",
        "So we write:\n",
        "\n",
        "$$\n",
        "\\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}) = \\begin{bmatrix}\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_1}(\\mathbf{w})\\\\\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_2}(\\mathbf{w})\\\\\n",
        "    \\vdots \\\\\n",
        "    \\frac{\\partial \\mathcal{L}}{\\partial w_n}(\\mathbf{w})\n",
        "   \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Each time we seek to improve our parameter estimates $\\mathbf{w}$, we will take\n",
        "a step in the negative direction of the gradient.\n",
        "\n",
        "... \"negative direction\" because the gradient specifies the direction of maximum\n",
        "increase -- and we want to decrease the loss function.\n",
        "\n",
        "---\n",
        "\n",
        "How big a step should we take? \n",
        "\n",
        "For step size, will use a scalar value, here denoted by the greek letter \"eta\",\n",
        "$\\eta$, which we call the __learning rate.__\n",
        "\n",
        "The learning rate is a hyperparameter that needs to be tuned for a given problem,\n",
        "or even can be modified adaptively as the algorithm progresses as we will see later.\n",
        "\n",
        "Now we can write the __gradient descent__ algorithm formally:\n",
        "\n",
        "1. Start with an initial parameter estimate $\\mathbf{w}^0$.\n",
        "2. Update: $\\mathbf{w}^{n+1} = \\mathbf{w}^n - \\eta \\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^n)$\n",
        "3. If not converged, go to step 2.\n",
        "\n",
        "How do we know if we are \"converged\"?  \n",
        "\n",
        "Typically we stop\n",
        "\n",
        "* after a certain number of iterations, or\n",
        "* the loss has not improved by a fixed amount -- _early stopping_\n",
        "\n",
        "## Example: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mp\n",
        "import sklearn\n",
        "import networkx as nx\n",
        "from IPython.display import Image, HTML\n",
        "\n",
        "import laUtilities as ut\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say we have this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "def centerAxes(ax):\n",
        "    ax.spines['left'].set_position('zero')\n",
        "    ax.spines['right'].set_color('none')\n",
        "    ax.spines['bottom'].set_position('zero')\n",
        "    ax.spines['top'].set_color('none')\n",
        "    ax.xaxis.set_ticks_position('bottom')\n",
        "    ax.yaxis.set_ticks_position('left')\n",
        "    bounds = np.array([ax.axes.get_xlim(), ax.axes.get_ylim()])\n",
        "    ax.plot(bounds[0][0],bounds[1][0],'')\n",
        "    ax.plot(bounds[0][1],bounds[1][1],'')\n",
        "\n",
        "n = 10\n",
        "beta = np.array([1., 0.5])\n",
        "ax = plt.figure(figsize = (7, 7)).add_subplot()\n",
        "centerAxes(ax)\n",
        "np.random.seed(1)\n",
        "xlin = -10.0 + 20.0 * np.random.random(n)\n",
        "y = beta[0] + (beta[1] * xlin) + np.random.randn(n)\n",
        "ax.plot(xlin, y, 'ro', markersize = 10);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's fit a least-squares line to this data. \n",
        "\n",
        "The loss function for this problem is the least-squares error:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{\\beta}) = \\Vert\\mathbf{y} - X\\mathbf{\\beta}\\Vert^2\n",
        "$$\n",
        "\n",
        "Of course, we know how to solve this problem using the normal equations, but let's do it using gradient descent instead.\n",
        "\n",
        "---\n",
        "\n",
        "Here is the line we'd like to find:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "ax = plt.figure(figsize = (7, 7)).add_subplot()\n",
        "centerAxes(ax)\n",
        "ax.plot(xlin, y, 'ro', markersize = 10)\n",
        "ax.plot(xlin, beta[0] + beta[1] * xlin, 'b-')\n",
        "plt.text(-9, 3, r'$y = \\beta_0 + \\beta_1x$', size=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "There are $n = 10$ data points, whose $x$ and $y$ values are stored in `xlin` and `y`.\n",
        "\n",
        "First, let's create our $X$ (design) matrix, and include a column of ones to model the intercept:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.column_stack([np.ones((n, 1)), xlin])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's visualize the loss function $\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y}-X\\mathbf{\\beta}\\Vert^2.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "fig = ut.three_d_figure((23, 1), '',\n",
        "                        -12, 12, -4, 4, -1, 2000, \n",
        "                        figsize = (7, 7))\n",
        "qf = np.array(X.T @ X)\n",
        "fig.ax.view_init(azim = 60, elev = 22)\n",
        "fig.plotGeneralQF(X.T @ X, -2 * (y.T @ X), y.T @ y, alpha = 0.5)\n",
        "fig.ax.set_zlabel('$\\mathcal{L}$')\n",
        "fig.ax.set_xlabel(r'$\\beta_0$')\n",
        "fig.ax.set_ylabel(r'$\\beta_1$')\n",
        "fig.set_title(r'$\\Vert \\mathbf{y}-X\\mathbf{\\beta}\\Vert^2$', '', \n",
        "              number_fig = False, size = 18)\n",
        "# fig.save();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We won't take you through computing the gradient for this problem (you can find it in the online text).  \n",
        "\n",
        "We'll will just tell you that the gradient for a least squares problem is:\n",
        "    \n",
        "$$\n",
        "\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = X^T X \\beta - X^T\\mathbf{y} \n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "\n",
        "::: {.callout-note}\n",
        "For those interested in a little more insight into what these plots are showing,\n",
        "here is the derivation.\n",
        "\n",
        "We start from the rule that $\\Vert \\mathbf{v}\\Vert = \\sqrt{\\mathbf{v}^T\\mathbf{v}}$.   \n",
        "\n",
        "Applying this rule to our loss function:\n",
        "\n",
        "$$ \n",
        "\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2 = \\beta^T X^T X \\beta - 2\\mathbf{\\beta}^TX^T\\mathbf{y}  + \\mathbf{y}^T\\mathbf{y} \n",
        "$$\n",
        "\n",
        "The first term, $\\beta^T X^T X \\beta$, is a quadratic form, and it is what makes\n",
        "this surface curved.  As long as $X$ has independent columns, $X^TX$ is positive\n",
        "definite, so the overall shape is a paraboloid opening upward, and the surface\n",
        "has a unique minimum point.\n",
        "\n",
        "To find the gradient, we can use standard calculus rules for derivates involving\n",
        "vectors.  The rules are not complicated, but the bottom line is that in this case,\n",
        "you can almost use the same rules you would if $\\beta$ were a scalar:\n",
        "\n",
        "$$\n",
        "\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = 2X^T X \\beta - 2X^T\\mathbf{y} \n",
        "$$\n",
        "\n",
        "And by the way -- since we've computed the derivative as a function of $\\beta$, instead of using gradient descent, we could simply solve for the point where the gradient is zero.  This is the optimal point which we know must exist:\n",
        "\n",
        "$$\n",
        "\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = 0 \n",
        "$$\n",
        "\n",
        "$$\n",
        "2X^T X \\beta - 2X^T\\mathbf{y} = 0 \n",
        "$$\n",
        "\n",
        "$$\n",
        "X^T X \\beta = X^T\\mathbf{y} \n",
        "$$\n",
        "\n",
        "Which of course, are the normal equations for this linear system.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "So here is our code for gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def loss(X, y, beta):\n",
        "    return np.linalg.norm(y - X @ beta) ** 2\n",
        "\n",
        "def gradient(X, y, beta):\n",
        "    return X.T @ X @ beta - X.T @ y\n",
        "\n",
        "def gradient_descent(X, y, beta_hat, eta, nsteps = 1000):\n",
        "    losses = [loss(X, y, beta_hat)]\n",
        "    betas = [beta_hat]\n",
        "    #\n",
        "    for step in range(nsteps):\n",
        "        #\n",
        "        # the gradient step\n",
        "        new_beta_hat = beta_hat - eta * gradient(X, y, beta_hat)\n",
        "        beta_hat = new_beta_hat\n",
        "        #\n",
        "        # accumulate statistics\n",
        "        losses.append(loss(X, y, new_beta_hat))\n",
        "        betas.append(new_beta_hat)\n",
        "        \n",
        "    return np.array(betas), np.array(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We'll start at an arbitrary point, say, $(-8, -3.2)$.\n",
        "\n",
        "That is, $\\beta_0 = -8$, and $\\beta_1 = -3.2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta_start = np.array([-8, -3.2])\n",
        "eta = 0.002\n",
        "betas, losses = gradient_descent(X, y, beta_start, eta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What happens to our loss function per GD iteration?   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "plt.plot(np.log(losses), '.-')\n",
        "plt.ylabel(r'$\\log\\mathcal{L}$', size = 14)\n",
        "plt.xlabel('Iteration', size = 14)\n",
        "plt.title('Improvement in Loss Per Iteration of GD', size = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And how do the parameter values $\\beta$ evolve?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "plt.plot(betas[:, 0], betas[:, 1], '.-')\n",
        "plt.xlabel(r'$\\beta_0$', size = 14)\n",
        "plt.ylabel(r'$\\beta_1$', size = 14)\n",
        "plt.title(r'Evolution of $\\beta$', size = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Notice that the improvement in loss decreases over time.  Initially the gradient\n",
        "is steep and loss improves fast, while later on the gradient is shallow and loss\n",
        "doesn't improve much per step.\n",
        "\n",
        "Now remember that in reality we are like the person who is trying to find their\n",
        "way down the mountain, in the fog.\n",
        "\n",
        "In general we cannot \"see\" the entire loss function surface.\n",
        "\n",
        "Nonetheless, since we know what the loss surface looks like in this case, we can\n",
        "visualize the algorithm \"moving\" on that surface.\n",
        "\n",
        "---\n",
        "\n",
        "This visualization combines the last two plots into a single view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "# set up view\n",
        "import matplotlib.animation as animation\n",
        "mp.rcParams['animation.html'] = 'jshtml'\n",
        "\n",
        "anim_frames = np.array(list(range(10)) + [2 * x for x in range(5, 25)] + [5 * x for x in range(10, 100)])\n",
        "\n",
        "fig = ut.three_d_figure((23, 1), 'z = 3 x1^2 + 7 x2 ^2',\n",
        "                        -12, 12, -4, 4, -1, 2000, \n",
        "                        figsize = (7, 7))\n",
        "plt.close()\n",
        "fig.ax.view_init(azim = 60, elev = 22)\n",
        "qf = np.array(X.T @ X)\n",
        "fig.plotGeneralQF(X.T @ X, -2 * (y.T @ X), y.T @ y, alpha = 0.5)\n",
        "fig.ax.set_zlabel('$\\mathcal{L}$')\n",
        "fig.ax.set_xlabel(r'$\\beta_0$')\n",
        "fig.ax.set_ylabel(r'$\\beta_1$')\n",
        "fig.set_title(r'$\\Vert \\mathbf{y}-X\\mathbf{\\beta}\\Vert^2$', '', \n",
        "              number_fig = False, size = 18)\n",
        "#\n",
        "def anim(frame):\n",
        "    fig.ax.plot(betas[:frame, 0], betas[:frame, 1], 'o-', zs = losses[:frame],  c = 'k', markersize = 5)\n",
        "    # fig.canvas.draw()\n",
        "#\n",
        "# create the animation \n",
        "animation.FuncAnimation(fig.fig, anim,\n",
        "                       frames = anim_frames,\n",
        "                       fargs = None,\n",
        "                       interval = 1,\n",
        "                       repeat = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We can also see how evolution of the parameters translate to the line fitting to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "fig, ax = plt.subplots(figsize = (7, 7))\n",
        "plt.close()\n",
        "centerAxes(ax)\n",
        "ax.plot(xlin, y, 'ro', markersize = 10)\n",
        "fit_line = ax.plot([], [])\n",
        "\n",
        "#\n",
        "#to get additional args to animate:\n",
        "#def animate(angle, *fargs):\n",
        "#    fargs[0].view_init(azim=angle)\n",
        "def animate(frame):\n",
        "    fit_line[0].set_data(xlin, betas[frame, 0] + betas[frame, 1] * xlin)\n",
        "    fig.canvas.draw()\n",
        "#\n",
        "# create the animation \n",
        "animation.FuncAnimation(fig, animate,\n",
        "                       frames = anim_frames,\n",
        "                       fargs=None,\n",
        "                       interval=100,\n",
        "                       repeat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenges in Gradient Descent\n",
        "\n",
        "Gradient Descent is a very general algorithm, one that can be applied to a huge\n",
        "array of problem types.\n",
        "\n",
        "However, there are a variety of issues that arise in using gradient descent in\n",
        "practice.\n",
        "\n",
        "## Learning Rate\n",
        "\n",
        "Setting the learning rate can be a challenge.\n",
        "\n",
        "Previously we had set the learning rate $\\eta = 0.002$.   \n",
        "\n",
        "Let set it a little higher and see what happens:  $\\eta = 0.0065.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "beta_start = np.array([-8, -2])\n",
        "eta = 0.0065\n",
        "betas, losses = gradient_descent(X, y, beta_start, eta, nsteps = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "plt.plot(np.log(losses), '.-')\n",
        "plt.ylabel(r'$\\log\\mathcal{L}$', size = 14)\n",
        "plt.xlabel('Iteration', size = 14)\n",
        "plt.title('Improvement in Loss Per Iteration of GD', size = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "plt.plot(betas[:, 0], betas[:, 1], '.-')\n",
        "plt.xlabel(r'$\\beta_0$', size = 14)\n",
        "plt.ylabel(r'$\\beta_1$', size = 14)\n",
        "plt.title(r'Evolution of $\\beta$', size = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "This is a total disaster.  What is going on?  \n",
        "\n",
        "It is helpful to look at the progress of the algorithm using the loss surface:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "%matplotlib inline\n",
        "fig = ut.three_d_figure((23, 1), '',\n",
        "                        -12, 2, -4, 4, -1, 2000, \n",
        "                        figsize = (7, 7))\n",
        "qf = np.array(X.T @ X)\n",
        "fig.ax.view_init(azim = 142, elev = 58)\n",
        "fig.plotGeneralQF(X.T @ X, -2 * (y.T @ X), y.T @ y, alpha = 0.5)\n",
        "fig.ax.set_zlabel('$\\mathcal{L}$')\n",
        "fig.ax.set_xlabel(r'$\\beta_0$')\n",
        "fig.ax.set_ylabel(r'$\\beta_1$')\n",
        "fig.set_title(r'$\\Vert \\mathbf{y}-X\\mathbf{\\beta}\\Vert^2$', '', \n",
        "              number_fig = False, size = 18)\n",
        "nplot = 18\n",
        "fig.ax.plot(betas[:nplot, 0], betas[:nplot, 1], 'o-', zs = losses[:nplot], markersize = 5);\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "We can see what is going on more clearly here.  \n",
        "\n",
        "What is happening is that because the steps are __too large,__ each step\n",
        "overshoots the local minimum.  \n",
        "\n",
        "The next step then lands on a portion of the surface that steeper ... and in the\n",
        "opposite direction.\n",
        "\n",
        "And so the process diverges.\n",
        "\n",
        "> For an interesting comparison, try setting $\\eta = 0.0055$ and observe the\n",
        "> evolution of $\\beta$.\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "Hence it is important to decrease the step size when divergence appears.  \n",
        "\n",
        "Unfortunately, on a complicated loss surface, a given step size may diverge in\n",
        "one location or starting point, but not in another.\n",
        "\n",
        "## Complex Loss Surfaces\n",
        "\n",
        "The loss surface for linear regression is the best possible kind:  it is strictly\n",
        "convex, so it has a single global minimum.\n",
        "\n",
        "For neural networks, the loss surface is more complex. \n",
        "\n",
        "In general, the larger the neural network, the more complex the loss surface.\n",
        "\n",
        "And deep neural networks, especially transformers have billions of parameters.\n",
        "\n",
        "Here's a visualization of the loss surface for the 56 layer neural network \n",
        "[VGG-56](http://arxiv.org/abs/1409.1556), from\n",
        "[Visualizing the Loss Landscape of Neural Networks](https://www.cs.umd.edu/~tomg/projects/landscapes/). \n",
        "\n",
        "<!-- https://www.cs.umd.edu/~tomg/projects/landscapes/ -->\n",
        "\n",
        "![](figs/L23-complex-landscape.png){width=\"40%\"}\n",
        "\n",
        "For a fun exploration, see\n",
        "[https://losslandscape.com/explorer](https://losslandscape.com/explorer).\n",
        "\n",
        "## Recap\n",
        "\n",
        "So far we applied gradient descent on a simple linear regression model.\n",
        "\n",
        "As we'll soon see, deep neural networks are much more complicated multi-stage\n",
        "models, with millions or billions of parameters to differentiate.\n",
        "\n",
        "Fortunately, the _Chain Rule_ from calculus gives us a relatively simple and\n",
        "scalable algorithm, called _Back Propagation_, that solves this problem.\n",
        "\n",
        "## Neuron and Neural Networks\n",
        "\n",
        "Now let's switch gears a bit to define an _artificial neuron_. For better or worse\n",
        "it is named after and loosely modeled on a biological neuron.\n",
        "\n",
        "<!-- Image Credit \"https://cs231n.github.io/neural-networks-1/\"-->\n",
        "\n",
        "![](figs/NN-figs/neuron.png){width=\"75%\"}\n",
        "\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "* The dendrites carry impulses from other neurons of different distances.\n",
        "* Once the collective firing rate of the impulses exceed a certain threshold,\n",
        "  the neuron fires its own pulse through the axon to other neurons\n",
        "\n",
        "## Artificial Neuron\n",
        "\n",
        "<!-- Image Credit \"https://cs231n.github.io/neural-networks-1/\"-->\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"%\"}\n",
        "\n",
        "![](figs/NN-figs/neuron_model.jpeg){width=\"75%\"}\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "The more common artifical neuron\n",
        "\n",
        "* collects one or more inputs, \n",
        "* each multiplied by a unique weight\n",
        "* sums the weighted inputs\n",
        "* adds a bias\n",
        "* then finally usually applies a nonlinear activation function\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)\n",
        "\n",
        "![](figs/NN-figs/neural_net2.jpeg){width=\"75%\"}\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "Multiple artificial neurons can be acting on the same inputs, in what we call\n",
        "a _layer_, and we can have more than one _layer_ until we produce one or more\n",
        "outputs.\n",
        "\n",
        "The example above shows a network with _3 inputs_, two layers of neurons, each\n",
        "with 4 neurons, followed by one layer that produces a single value output.\n",
        "\n",
        "E.g. a binary classifier.\n",
        "\n",
        "## Next Lecture\n",
        "\n",
        "* Introduce and visualize our compute graph\n",
        "* Implement Backpropagation\n",
        "* Build out our neural network\n",
        "* Train and evaluate it\n",
        "\n",
        "## Recap\n",
        "\n",
        "* We introduced gradient descent for linear regression\n",
        "* We defined an artificial neuron and a multi-layer perceptron\n",
        "* We visualized the loss surface for linear regression\n",
        "* We saw how divergence can occur if the step size is too large"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tomg/Source/courses/tools4ds/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}