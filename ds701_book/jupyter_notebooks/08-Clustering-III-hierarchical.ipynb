{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Hierarchical Clustering\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/08-Clustering-III-hierarchical.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn.datasets as sk_data\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Today we will look at a fairly different approach to clustering.\n",
        "\n",
        "So far, we have been thinking of clustering as finding a __partition__ of our dataset.\n",
        "\n",
        "That is, a set of nonoverlapping clusters, in which each data item is in one cluster.\n",
        "\n",
        "However, in many cases, the notion of a strict partition is not as useful.\n",
        "\n",
        "## How Many Clusters?\n",
        "\n",
        "How many clusters would you say there are here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_rand, y_rand = sk_data.make_blobs(n_samples=[100, 100, 250, 70, 75, 80], centers = [[1, 2], [1.5, 1], [3, 2], [1.75, 3.25], [2, 4], [2.25, 3.25]], n_features = 2,\n",
        "                          center_box = (-10.0, 10.0), cluster_std = [.2, .2, .3, .1, .15, .15], random_state = 0)\n",
        "df_rand = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\n",
        "df_rand = df_rand.astype({'label': 'int'})\n",
        "df_rand['label2'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3}[x] for x in df_rand['label']]\n",
        "df_rand['label3'] = [{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 2}[x] for x in df_rand['label']]\n",
        "# kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\n",
        "# df_rand['label'] = kmeans.fit_predict(df_rand[['X', 'Y']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "df_rand.plot('X', 'Y', kind = 'scatter',  \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Three clusters?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label3', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Four clusters?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label2', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Six clusters?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dataset shows clustering on __multiple scales.__\n",
        "\n",
        "To fully capture the structure in this dataset, two things are needed:\n",
        "1. Capturing the differing clusters depending on the scale\n",
        "2. Capturing the containment relations -- which clusters lie within other clusters\n",
        "\n",
        "These observations motivate the notion of __hierarchical__ clustering.\n",
        "\n",
        "In hierarchical clustering, we move away from the __partition__ notion of $k$-means, \n",
        "\n",
        "and instead capture a more complex arrangement that includes containment of one cluster within another.\n",
        "\n",
        "## Hierarchical Clustering\n",
        "\n",
        "A hierarchical clustering produces a set of __nested__ clusters organized into a tree.\n",
        "\n",
        "A hierarchical clustering is visualized using a __dendrogram__ \n",
        "\n",
        "* A tree-like diagram that records the containment relations among clusters.\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"./figs/L08-dendrogram.png\" width=\"600px\">\n",
        "\n",
        "</center>\n",
        "\n",
        "### Strengths of Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering has a number of advantages:\n",
        "\n",
        "First, a hierarchical clustering encodes many __different__ clusterings.  That is, it does not itself decide on the correct number of clusters.  \n",
        "\n",
        "A clustering is obtained by \"cutting\" the dendrogram at some level.\n",
        "\n",
        "This means that you can make this crucial decision yourself, by inspecting the dendrogram.  \n",
        "\n",
        "Put another way, you can obtain any desired number of clusters.\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"./figs/L08-dendrogram-cut.png\" width=\"600px\">\n",
        "\n",
        "</center>\n",
        "\n",
        "The second advantage is that the dendrogram may itself correspond to a meaningful structure, for example, a taxonomy.\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"figs/L08-animal-taxonomy.jpg\" width=\"600px\">\n",
        "\n",
        "</center>\n",
        "\n",
        "The third advantage is that many hierarchical clustering methods can be performed using either similarity (proximity) or dissimilarity (distance) metrics.\n",
        "\n",
        "This can be very helpful! \n",
        "\n",
        "(Note that techniques like $k$-means cannot be used with unmodified similarity metrics.)\n",
        "\n",
        "### Compared to $k$-means\n",
        "\n",
        "Another aspect of hierachical clustering is that it can handle certain cases better than $k$-means.\n",
        "\n",
        "Because of the nature of the $k$-means algorithm, $k$-means tends to produce:\n",
        "* Roughly spherical clusters\n",
        "* Clusters of approximately equal size\n",
        "* Non-overlapping clusters\n",
        "\n",
        "In many real-world situations, clusters may not be round, they may be of unequal size, and they may overlap.\n",
        "\n",
        "Hence we would like clustering algorithms that can work in those cases also.\n",
        "\n",
        "## Hierarchical Clustering Algorithms\n",
        "\n",
        "There are two main approaches to hierarchical clustering: \"bottom-up\" and \"top-down.\"\n",
        "\n",
        "__Agglomerative__ Clustering (\"bottom-up\"):\n",
        "\n",
        "* Start by defining each point as its own cluster\n",
        "* At each successive step, merge the two clusters that are closest to each other\n",
        "* Repeat until only one cluster is left.\n",
        "\n",
        "__Divisive__ Clustering (\"top-down\"):\n",
        "    \n",
        "* Start with one, all-inclusive cluster\n",
        "* At each step, find the cluster split that creates the largest distance between resulting clusters\n",
        "* Repeat until each point is in its own cluster.\n",
        "\n",
        "Agglomerative techniques are by far the more common.\n",
        "\n",
        "The key to both of these methods is defining __the distance between two clusters.__\n",
        "\n",
        "Different definitions for the inter-cluster distance yield different clusterings.\n",
        "\n",
        "To illustrate the impact of the choice of cluster distances, we'll focus on agglomerative clustering.\n",
        "\n",
        "### Defining Cluster Proximity\n",
        "\n",
        "Given two clusters, how do we define the _distance_ between them?\n",
        "\n",
        "Here are three natural ways to do it:\n",
        "   * __Single-Linkage:__ the distance between two clusters is the distance between the closest two points that are in different clusters.\n",
        "   \n",
        "$$ D_\\text{single}(i,j) = \\min_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}$$\n",
        "\n",
        "* __Complete-Linkage:__ the distance between two clusters is the distance between the farthest two points that are in different clusters.\n",
        "\n",
        "$$ D_\\text{complete}(i,j) = \\max_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}$$\n",
        "\n",
        "* __Average-Linkage:__ the distance between two clusters is the average distance between all pairs of points from different clusters.\n",
        "\n",
        "$$ D_\\text{average}(i,j) = \\frac{1}{|C_i|\\cdot|C_j|}\\sum_{x \\in C_i,\\, y \\in C_j}d(x, y)$$\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"./figs/L08-hierarchical-criteria.png\" width=\"600px\">\n",
        "\n",
        "</center>\n",
        "\n",
        "<div style = \"float: left; width: 41%; text-align: center;\">\n",
        "    Single-Linkage\n",
        "</div>\n",
        "<div style = \"float: left; width: 18%; text-align: center;\">\n",
        "    Complete-Linkage\n",
        "</div>\n",
        "<div style = \"float: left; width: 41%; text-align: center;\">\n",
        "    Average-Linkage\n",
        "</div>\n",
        "\n",
        "Notice that it is easy to express the definitions above in terms of similarity instead of distance.\n",
        "\n",
        "Here is a set of 6 points that we will cluster to show differences between distance metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "slideshow": {
          "slide_type": "-"
        },
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "pt_x = [0.4, 0.22, 0.35, 0.26, 0.08, 0.45]\n",
        "pt_y = [0.53, 0.38, 0.32, 0.19, 0.41, 0.30]\n",
        "plt.plot(pt_x, pt_y, 'o', markersize = 10, color = 'k')\n",
        "plt.ylim([.15, .60])\n",
        "plt.xlim([0.05, 0.70])\n",
        "for i in range(6):\n",
        "    plt.annotate(f'{i}', (pt_x[i]+0.02, pt_y[i]-0.01), fontsize = 12)\n",
        "plt.axis('off')\n",
        "plt.savefig('figs/L08-basic-pointset.png');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X = np.array([pt_x, pt_y]).T\n",
        "from scipy.spatial import distance_matrix\n",
        "labels = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5']\n",
        "D = pd.DataFrame(distance_matrix(X, X), index = labels, columns = labels)\n",
        "D.style.format('{:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single-Linkage Clustering\n",
        "\n",
        "<img src=\"figs/L08-singlelink-pointset.png\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "scrolled": false,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "import scipy.cluster\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "Z = hierarchy.linkage(X, method='single')\n",
        "hierarchy.dendrogram(Z);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Advantages__:\n",
        "\n",
        "* Single-linkage clustering can handle non-elliptical shapes.\n",
        "\n",
        "In fact it can produce long, elongated clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n",
        "Z = hierarchy.linkage(X_moon_05, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Single-Linkage Can Find Irregularly Shaped Clusters')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_rand_lo, y_rand_lo = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n",
        "                          center_box = (-10.0, 10.0), cluster_std = [.1, .5], random_state = 0)\n",
        "Z = hierarchy.linkage(X_rand_lo, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_rand_lo[:,0], X_rand_lo[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Single-Linkage Can Find Different-Sized Clusters')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Disadvantages:__ \n",
        "\n",
        "* Single-linkage clustering can be sensitive to noise and outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_moon_10, y_moon_10 = sk_data.make_moons(random_state = 0, noise = 0.1)\n",
        "Z = hierarchy.linkage(X_moon_10, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_rand_hi, y_rand_hi = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n",
        "                          center_box = (-10.0, 10.0), cluster_std = [.15, .6], random_state = 0)\n",
        "Z = hierarchy.linkage(X_rand_hi, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\n",
        "plt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete-Linkage Clustering\n",
        "\n",
        "<img src=\"figs/L08-completelink-pointset.png\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X, method='complete')\n",
        "hierarchy.dendrogram(Z);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Advantages__:\n",
        "\n",
        "* Produces more-balanced clusters -- more-equal diameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "X_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n",
        "Z = hierarchy.linkage(X_moon_05, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X_rand_hi, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Less susceptible to noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X_moon_10, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Complete-Linkage Clustering of Noisy Data similar to Less Noisy')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Average-Linkage Clustering\n",
        "\n",
        "<img src=\"figs/L08-averagelink-pointset.png\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "split",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X, method='average')\n",
        "hierarchy.dendrogram(Z);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Average-Linkage clustering is in some sense a compromise between Single-linkage and Complete-linkage clustering.\n",
        "\n",
        "__Strengths:__\n",
        "    \n",
        "* Less susceptible to noise and outliers\n",
        "\n",
        "__Limitations:__\n",
        "    \n",
        "* Biased toward elliptical clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "center",
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X_moon_10, method='average')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Average-Linkage Similar to Complete - Globular Clusters')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "Z = hierarchy.linkage(X_rand_hi, method='average')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.title('Average-Linkage More resistant to noise than Single-Linkage')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All Three Compared\n",
        "\n",
        "<div style = \"float: left; width: 33%; text-align: center;\">\n",
        "    <img src=\"figs/L08-singlelink-pointset.png\" style=\"width:100%\">\n",
        "    Single-Linkage\n",
        "</div>\n",
        "\n",
        "<div style = \"float: left; width: 33%; text-align: center;\">\n",
        "    <img src=\"figs/L08-completelink-pointset.png\"  style=\"width:100%\">\n",
        "    Complete-Linkage\n",
        "</div>\n",
        "\n",
        "<div style = \"float: left; width: 33%; text-align: center;\">\n",
        "    <img src=\"figs/L08-averagelink-pointset.png\" style=\"width:100%\">\n",
        "    Average-Linkage\n",
        "</div>\n",
        "\n",
        "## Ward's Distance\n",
        "\n",
        "Finally, we consider one more cluster distance.\n",
        "\n",
        "Ward's distance asks \"what if\".\n",
        "\n",
        "That is, \"What if we combined these two clusters -- how would clustering improve?\"\n",
        "\n",
        "To define \"how would clustering improve?\" we appeal to the $k$-means criterion.\n",
        "\n",
        "So:\n",
        "\n",
        "__Ward's Distance__ between clusters $C_i$ and $C_j$ is the difference between the total within cluster sum of squares for the two clusters separately, __compared to__ the within cluster sum of squares resulting from merging the two clusters into a new cluster $C_{i+j}$:\n",
        "\n",
        "$$D_\\text{Ward}(i, j) = \\sum_{x \\in C_i} (x - r_i)^2 + \\sum_{x \\in C_j} (x - r_j)^2  - \\sum_{x \\in C_{i+j}} (x - r_{i+j})^2 $$\n",
        "\n",
        "where $r_i, r_j, r_{i+j}$ are the corresponding cluster centroids.\n",
        "\n",
        "In a sense, this cluster distance results in a hierarchical analog of $k$-means.\n",
        "\n",
        "As a result, it has properties similar to $k$-means:\n",
        "    \n",
        "* Less susceptible to noise and outliers\n",
        "* Biased toward elliptical clusters\n",
        "\n",
        "Hence it tends to behave more like group-average hierarchical clustering.\n",
        "\n",
        "## Hierarchical Clustering In Practice\n",
        "\n",
        "Now we'll look at doing hierarchical clustering in practice, using python.\n",
        "\n",
        "We'll use the same synthetic data as we did in the k-means case -- ie.,\n",
        "three \"blobs\" living in 30 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n",
        "                          center_box=(-10.0, 10.0),random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a reminder of the raw data here is the visualization: first the raw data, then an embedding into 2-D (using MDS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": false,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "import sklearn.manifold\n",
        "import sklearn.metrics as metrics\n",
        "euclidean_dists = metrics.euclidean_distances(X)\n",
        "mds = sklearn.manifold.MDS(n_components = 2, max_iter = 3000, eps = 1e-9, random_state = 0,\n",
        "                   dissimilarity = \"precomputed\", n_jobs = 1)\n",
        "fit = mds.fit(euclidean_dists)\n",
        "pos = fit.embedding_\n",
        "plt.axis('equal')\n",
        "plt.scatter(pos[:, 0], pos[:, 1], s = 8);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hierarchical clustering is available in __`sklearn`__, but there is a much more fully developed set of tools in the __`scipy`__ package and that is the one to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "import scipy.cluster\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "import scipy.spatial.distance\n",
        "\n",
        "# linkages = ['single','complete','average','weighted','ward']\n",
        "Z = hierarchy.linkage(X, method = 'single')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "R = hierarchy.dendrogram(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical Clustering Real Data\n",
        "\n",
        "Once again we'll use the \"20 Newsgroup\" data provided as example data in sklearn.\n",
        "\n",
        "(http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
        "news_data = fetch_20newsgroups(subset = 'train', categories = categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', min_df = 4, max_df = 0.8)\n",
        "data = vectorizer.fit_transform(news_data.data).todense()\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "# metrics can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, \n",
        "# ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, \n",
        "# ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, \n",
        "# ‘sqeuclidean’, ‘yule’.\n",
        "Z_20ng = hierarchy.linkage(data, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(14,4))\n",
        "R_20ng = hierarchy.dendrogram(Z_20ng, p=4, truncate_mode = 'level', show_leaf_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selecting the Number of Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "clusters = hierarchy.fcluster(Z_20ng, 3, criterion = 'maxclust')\n",
        "print(clusters.shape)\n",
        "clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "max_clusters = 20\n",
        "s = np.zeros(max_clusters+1)\n",
        "for k in range(2, max_clusters+1):\n",
        "    clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n",
        "    s[k] = metrics.silhouette_score(np.asarray(data), clusters, metric = 'euclidean')\n",
        "plt.plot(range(2, len(s)), s[2:], '.-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "print('Top Terms Per Cluster:')\n",
        "k = 5\n",
        "clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n",
        "for i in range(1,k+1):\n",
        "    items = np.array([item for item,clust in zip(data, clusters) if clust == i])\n",
        "    centroids = np.squeeze(items).mean(axis = 0)\n",
        "    asc_order_centroids = centroids.argsort()#[:, ::-1]\n",
        "    order_centroids = asc_order_centroids[::-1]\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    print(f'Cluster {i}:')\n",
        "    for ind in order_centroids[:10]:\n",
        "        print(f' {terms[ind]}')\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tomg/Source/courses/tools4ds/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}