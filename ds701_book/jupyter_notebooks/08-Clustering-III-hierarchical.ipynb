{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Hierarchical Clustering\n",
        "jupyter: python3\n",
        "fig-align: center\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/08-Clustering-III-hierarchical.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn.datasets as sk_data\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Today we will look at a fairly different approach to clustering.\n",
        "\n",
        "So far, we have been thinking of clustering as finding a __partition__ of our dataset.\n",
        "\n",
        "That is, a set of nonoverlapping clusters, in which each data item is in one cluster.\n",
        "\n",
        "However, in many cases, the notion of a strict partition is not as useful.\n",
        "\n",
        "# Example\n",
        "\n",
        "## How Many Clusters?\n",
        "\n",
        "How many clusters would you say there are here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "X_rand, y_rand = sk_data.make_blobs(\n",
        "    n_samples=[100, 100, 250, 70, 75, 80], \n",
        "    centers = [[1, 2], [1.5, 1], [3, 2], [1.75, 3.25], [2, 4], [2.25, 3.25]], \n",
        "    n_features = 2,\n",
        "    center_box = (-10.0, 10.0), \n",
        "    cluster_std = [.2, .2, .3, .1, .15, .15], \n",
        "    random_state = 0\n",
        ")\n",
        "df_rand = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\n",
        "df_rand = df_rand.astype({'label': 'int'})\n",
        "df_rand['label2'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3}[x] for x in df_rand['label']]\n",
        "df_rand['label3'] = [{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 2}[x] for x in df_rand['label']]\n",
        "# kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\n",
        "# df_rand['label'] = kmeans.fit_predict(df_rand[['X', 'Y']])\n",
        "\n",
        "df_rand.plot('X', 'Y', kind = 'scatter',  \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## __Three clusters?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label3', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## __Four clusters?__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label2', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## __Six clusters?__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n",
        "                   colorbar = False, figsize = (6, 6))\n",
        "plt.axis('square')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "400px",
        "fig-height": "400px"
      },
      "source": [
        "#| fig-align: left\n",
        "\n",
        "df_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n",
        "                   colorbar = False, figsize = (4, 4))\n",
        "plt.axis('square')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "\n",
        "This dataset shows clustering on __multiple scales.__\n",
        "\n",
        ":::: {.fragment}\n",
        "To fully capture the structure in this dataset, two things are needed:\n",
        "::::\n",
        "\n",
        "::: {.incremental}\n",
        "1. Capturing the differing clusters depending on the scale\n",
        "2. Capturing the containment relations -- which clusters lie within other clusters\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "These observations motivate the notion of __hierarchical__ clustering.\n",
        "\n",
        "In hierarchical clustering, we move away from the __partition__ notion of $k$-means, \n",
        "\n",
        "and instead capture a more complex arrangement that includes containment of one cluster within another.\n",
        "::::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "# Hierarchical Clustering\n",
        "\n",
        "## Hierarchical Clustering\n",
        "\n",
        "A hierarchical clustering produces a set of __nested__ clusters organized into a tree.\n",
        "\n",
        "A hierarchical clustering is visualized using a...\n",
        "\n",
        " __dendrogram__ \n",
        "\n",
        "* A tree-like diagram that records the containment relations among clusters.\n",
        "\n",
        "![](./figs/L08-dendrogram.png){width=\"600px\"}\n",
        "\n",
        "\n",
        "## Strengths of Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering has a number of advantages:\n",
        "\n",
        ":::: {.incremental}\n",
        "* Encodes many __different__ clusterings.\n",
        "    * It does not itself decide on the correct number of clusters.\n",
        "* A clustering is obtained by \"cutting\" the dendrogram at some level.\n",
        "* You can make this crucial decision yourself, by inspecting the dendrogram.  \n",
        "* You can obtain a (somewhat) arbitrary number of clusters.\n",
        "::::\n",
        "\n",
        "![](./figs/L08-dendrogram-cut.png){width=\"600px\"}\n",
        "\n",
        "## Another advantage\n",
        "\n",
        "Another advantage is that the dendrogram may itself correspond to a meaningful\n",
        "structure, for example, a taxonomy.\n",
        "\n",
        "![](./figs/L08-animal-taxonomy.jpg){width=\"100%\"}\n",
        "\n",
        "## Yet another advantage\n",
        "\n",
        "* Many hierarchical clustering methods can be performed using either similarity (proximity) or dissimilarity (distance) metrics.\n",
        "* This can be very helpful! \n",
        "* Techniques like $k$-means rely on a specific way of measuring similarity or distance between data points. \n",
        "\n",
        "## Compared to $k$-means\n",
        "\n",
        "Another aspect of hierachical clustering is that it can handle certain cases better than $k$-means.\n",
        "\n",
        "Because of the nature of the $k$-means algorithm, $k$-means tends to produce:\n",
        "\n",
        "* Roughly spherical clusters\n",
        "* Clusters of approximately equal size\n",
        "* Non-overlapping clusters\n",
        "\n",
        "In many real-world situations, clusters may not be round, they may be of\n",
        "unequal size, and they may overlap.\n",
        "\n",
        "Hence we would like clustering algorithms that can work in those cases also.\n",
        "\n",
        "\n",
        "# Hierarchical Clustering Algorithms\n",
        "\n",
        "## Hierarchical Clustering Algorithms\n",
        "\n",
        "There are two main approaches to hierarchical clustering: \"bottom-up\" and \"top-down.\"\n",
        "\n",
        "::: {.fragment}\n",
        "__Agglomerative__ Clustering (\"bottom-up\"):\n",
        "\n",
        "* Start by defining each point as its own cluster\n",
        "* At each successive step, merge the two clusters that are closest to each other\n",
        "* Repeat until only one cluster is left.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "__Divisive__ Clustering (\"top-down\"):\n",
        "    \n",
        "* Start with one, all-inclusive cluster\n",
        "* At each step, find the cluster split that creates the largest distance between resulting clusters\n",
        "* Repeat until each point is in its own cluster.\n",
        ":::\n",
        "\n",
        "## Some key points\n",
        " \n",
        "* Agglomerative techniques are by far the more common.\n",
        "* The key to both of these methods is defining __the distance between two clusters.__\n",
        "* Different definitions for the inter-cluster distance yield different clusterings.\n",
        "\n",
        "::: {.fragment}\n",
        "To illustrate the impact of the choice of cluster distances, we'll focus on agglomerative clustering.\n",
        ":::\n",
        "\n",
        "## Hierarchical Clustering Algorithm Inputs\n",
        "\n",
        "1. __Input data__ as either\n",
        "    * __2-D sample/feature matrix__\n",
        "    * __1D condensed distance matrix__ -- upper triangular of pairwise distance matrix flattened\n",
        "2. The cluster __linkage__ method (__single__, complete, average, ward, ...)\n",
        "3. The __distance metric__ (_Euclidean_, manhattan, Jaccard, ...)\n",
        "\n",
        "## Hierarchical Clustering Algorithm Output\n",
        "\n",
        "An $(n-1,4)$ shaped __linkage matrix__.\n",
        "\n",
        "Where __each row__ is `[idx1, idx2, dist, sample_count]` is a single step in the clustering\n",
        "process and\n",
        "\n",
        "`idx1` and `idx2` are indices of the cluster being merged, where \n",
        "\n",
        "* indices $\\{ 0...n-1 \\}$ refer to the original data points and \n",
        "* indices $\\{n, n+1, ...\\}$ refer to each new cluster created\n",
        "\n",
        "And `sample_count` is the number of original data samples in the new cluster.\n",
        "\n",
        "\n",
        "## Hierarchical Clustering Algorithm Explained\n",
        "\n",
        "1. __Initialization__\n",
        "    * Start with each data point as its own cluster.\n",
        "    * Calculate the distance matrix if not provided\n",
        "2. __Iterative Clustering__\n",
        "    * At each step, the two closest clusters (according to linkage method and distance metric)\n",
        "      are merged into a new cluster.\n",
        "    * The distance between new cluster and the remaining clusters is added\n",
        "    * Repeat previous two steps until only one cluster remains.\n",
        "\n",
        "\n",
        "## Defining Cluster Proximity\n",
        "\n",
        "Given two clusters, how do we define the _distance_ between them?\n",
        "\n",
        "Here are three natural ways to do it.\n",
        "\n",
        "## Cluster Proximity -- Single-Linkage\n",
        "\n",
        "![](./figs/L08-hierarchical-criteria-a.jpeg){height=\"300px\" fig-align=\"center\"}\n",
        "\n",
        "__Single-Linkage:__ the distance between two clusters is the distance between the\n",
        "closest two points that are in different clusters.\n",
        "   \n",
        "$$\n",
        "D_\\text{single}(i,j) = \\min_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n",
        "$$\n",
        "\n",
        "## Cluster Proximity -- Complete-Linkage\n",
        "\n",
        "![](./figs/L08-hierarchical-criteria-b.jpeg){height=\"300px\" fig-align=\"center\"}\n",
        "\n",
        "__Complete-Linkage:__ the distance between two clusters is the distance between\n",
        "the farthest two points that are in different clusters.\n",
        "\n",
        "$$\n",
        "D_\\text{complete}(i,j) = \\max_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n",
        "$$\n",
        "\n",
        "## Cluster Proximity -- Average-Linkage\n",
        "\n",
        "![](./figs/L08-hierarchical-criteria-c.jpeg){height=\"300px\" fig-align=\"center\"}\n",
        "\n",
        "__Average-Linkage:__ the distance between two clusters is the average distance between all pairs of points from different clusters.\n",
        "\n",
        "$$\n",
        "D_\\text{average}(i,j) = \\frac{1}{|C_i|\\cdot|C_j|}\\sum_{x \\in C_i,\\, y \\in C_j}d(x, y)\n",
        "$$\n",
        "\n",
        "## Cluster Proximity Example\n",
        "\n",
        "Notice that it is easy to express the definitions above in terms of similarity instead of distance.\n",
        "\n",
        "Here is a set of 6 points that we will cluster to show differences between distance metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": "600px"
      },
      "source": [
        "#| fig-align: center\n",
        "\n",
        "pt_x = [0.4, 0.22, 0.35, 0.26, 0.08, 0.45]\n",
        "pt_y = [0.53, 0.38, 0.32, 0.19, 0.41, 0.30]\n",
        "plt.plot(pt_x, pt_y, 'o', markersize = 10, color = 'k')\n",
        "plt.ylim([.15, .60])\n",
        "plt.xlim([0.05, 0.70])\n",
        "for i in range(6):\n",
        "    plt.annotate(f'{i}', (pt_x[i]+0.02, pt_y[i]-0.01), fontsize = 12)\n",
        "plt.axis('on')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Cluster Proximity Example, cont.\n",
        ":::\n",
        "\n",
        "We can calculate the distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.array([pt_x, pt_y]).T\n",
        "from scipy.spatial import distance_matrix\n",
        "labels = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5']\n",
        "D = pd.DataFrame(distance_matrix(X, X), index = labels, columns = labels)\n",
        "D.style.format('{:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single-Linkage Clustering\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "![](./figs/L08-singlelink-pointset.png){width=\"600px\"}\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.cluster\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "Z = hierarchy.linkage(X, method='single', metric='euclidean')\n",
        "hierarchy.dendrogram(Z)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "## Single Linkage Clustering Advantages\n",
        "\n",
        "Single-linkage clustering can handle non-elliptical shapes.\n",
        "\n",
        "Here we use SciPy's [fcluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html)\n",
        "to form flat custers from hierarchical clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "X_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n",
        "\n",
        "Z = hierarchy.linkage(X_moon_05, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_moon_05[:, 0], X_moon_05[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides \"}\n",
        "## Single Linkage Advantages, cont.\n",
        ":::\n",
        "\n",
        "Single-Linkage can find different sized clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_rand_lo, y_rand_lo = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n",
        "                          center_box = (-10.0, 10.0), cluster_std = [.1, .5], random_state = 0)\n",
        "\n",
        "Z = hierarchy.linkage(X_rand_lo, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_rand_lo[:, 0], X_rand_lo[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "# plt.title('Single-Linkage Can Find Different-Sized Clusters')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Linkage Disadvantages\n",
        "\n",
        "Single-linkage clustering can be sensitive to noise and outliers.\n",
        "\n",
        "The results can change drastically on even slightly more noisy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_moon_10, y_moon_10 = sk_data.make_moons(random_state = 0, noise = 0.1)\n",
        "\n",
        "Z = hierarchy.linkage(X_moon_10, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "# plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Single Linkage Disadvantages, cont.\n",
        ":::\n",
        "\n",
        "And here's another example where we bump the standard deviation on the clusters slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_rand_hi, y_rand_hi = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n",
        "                          center_box = (-10.0, 10.0), cluster_std = [.15, .6], random_state = 0)\n",
        "\n",
        "Z = hierarchy.linkage(X_rand_hi, method='single')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete-Linkage Clustering\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "![](./figs/L08-completelink-pointset.png){width=\"100%\"}\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X, method='complete')\n",
        "hierarchy.dendrogram(Z)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "## Complete-Linkage Clustering Advantages\n",
        "\n",
        "Produces more-balanced clusters -- more-equal diameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n",
        "\n",
        "Z = hierarchy.linkage(X_moon_05, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_moon_05[:, 0], X_moon_05[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X_rand_hi, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "\n",
        "plt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Less susceptible to noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X_moon_10, method='complete')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete-Linkage Clustering Disadvantages\n",
        "\n",
        "Some disadvantages for complete-linkage clustering are:\n",
        "\n",
        "- Sensitivity to outliers\n",
        "- Tendency to compute more compact, spherical clusters\n",
        "- Computationally intensive for large datasets.\n",
        "\n",
        "## Average-Linkage Clustering\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "![](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X, method='average')\n",
        "hierarchy.dendrogram(Z)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "## Average-Linkage Clustering Strengths and Limitations\n",
        "\n",
        "Average-Linkage clustering is in some sense a compromise between Single-linkage and Complete-linkage clustering.\n",
        "\n",
        "__Strengths:__\n",
        "    \n",
        "* Less susceptible to noise and outliers\n",
        "\n",
        "__Limitations:__\n",
        "    \n",
        "* Biased toward elliptical clusters\n",
        "\n",
        "Produces more isotropic clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X_moon_10, method='average')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More resistant to noise than Single-Linkage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z = hierarchy.linkage(X_rand_hi, method='average')\n",
        "labels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n",
        "plt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All Three Compared\n",
        "\n",
        "::: {layout-ncol=\"3\"}\n",
        "\n",
        "![Single-Linkage](./figs/L08-singlelink-pointset.png){width=\"100%\"}\n",
        "\n",
        "![Complete-Linkage](./figs/L08-completelink-pointset.png){width=\"100%\"}\n",
        "\n",
        "![Average-Linkage](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n",
        "\n",
        ":::\n",
        "\n",
        "## Ward's Distance\n",
        "\n",
        "Finally, we consider one more cluster distance.\n",
        "\n",
        "Ward's distance asks \"What if we combined these two clusters -- how would clustering improve?\"\n",
        "\n",
        "To define \"how would clustering improve?\" we appeal to the $k$-means criterion.\n",
        "\n",
        "So:\n",
        "\n",
        "__Ward's Distance__ between clusters $C_i$ and $C_j$ is the difference between\n",
        "the total within cluster sum of squares for the two clusters separately, \n",
        "__compared to__ the _within cluster sum of squares_ resulting from merging the two\n",
        "clusters into a new cluster $C_{i+j}$:\n",
        "\n",
        "$$\n",
        "D_\\text{Ward}(i, j) = \\sum_{x \\in C_i} (x - c_i)^2 + \\sum_{x \\in C_j} (x - c_j)^2  - \\sum_{x \\in C_{i+j}} (x - c_{i+j})^2 \n",
        "$$\n",
        "\n",
        "where $c_i, c_j, c_{i+j}$ are the corresponding cluster centroids.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Ward's Distance continued\n",
        ":::\n",
        "\n",
        "In a sense, this cluster distance results in a hierarchical analog of $k$-means.\n",
        "\n",
        "As a result, it has properties similar to $k$-means:\n",
        "    \n",
        "* Less susceptible to noise and outliers\n",
        "* Biased toward elliptical clusters\n",
        "\n",
        "Hence it tends to behave more like average-linkage hierarchical clustering.\n",
        "\n",
        "\n",
        "# Hierarchical Clustering in Practice\n",
        "\n",
        "## Hierarchical Clustering In Practice\n",
        "\n",
        "Now we'll look at doing hierarchical clustering in practice.\n",
        "\n",
        "We'll use the same synthetic data as we did in the k-means case -- i.e., three \"blobs\" living in 30 dimensions.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Hierarchical Clustering in Practice, cont.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n",
        "                          center_box=(-10.0, 10.0),random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The raw data is shown in the following visualization: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Hierarchical Clustering in Practice, cont.\n",
        ":::\n",
        "\n",
        "\n",
        "then an embedding into 2-D (using MDS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.manifold\n",
        "import sklearn.metrics as metrics\n",
        "euclidean_dists = metrics.euclidean_distances(X)\n",
        "mds = sklearn.manifold.MDS(n_components = 2, max_iter = 3000, eps = 1e-9, random_state = 0,\n",
        "                   dissimilarity = \"precomputed\", n_jobs = 1)\n",
        "fit = mds.fit(euclidean_dists)\n",
        "pos = fit.embedding_\n",
        "plt.axis('equal')\n",
        "plt.scatter(pos[:, 0], pos[:, 1], s = 8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Hierarchical Clustering in Practice, cont.\n",
        ":::\n",
        "\n",
        "Hierarchical clustering is available in __`sklearn`__, but there is a much more\n",
        "fully developed set of \n",
        "[tools](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) \n",
        "in the [scipy](https://docs.scipy.org/doc/scipy/index.html) package and that is the one to use.\n",
        "\n",
        "Let's run hierarchical clustering on our synthetic dataset.\n",
        "\n",
        "::: {.callout-tip}\n",
        "Try the other linkage methods and see how the clustering and dendrogram changes.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.cluster\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "import scipy.spatial.distance\n",
        "\n",
        "# linkages = ['single','complete','average','weighted','ward']\n",
        "Z = hierarchy.linkage(X, method = 'single')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And draw the dendrogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "R = hierarchy.dendrogram(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical Clustering Real Data\n",
        "\n",
        "Once again we'll use the\n",
        "[\"20 Newsgroup\"](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
        "data provided as example data in sklearn.\n",
        "\n",
        "Load three of the newsgroups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
        "news_data = fetch_20newsgroups(subset = 'train', categories = categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectorize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', min_df = 4, max_df = 0.8)\n",
        "data = vectorizer.fit_transform(news_data.data).todense()\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cluster hierarchically and display dendrogram. Feel free to experiment with different metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# linkages are one of 'single','complete','average','weighted','ward'\n",
        "#\n",
        "# metrics can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, \n",
        "# ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, \n",
        "# ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, \n",
        "# ‘sqeuclidean’, ‘yule’.\n",
        "\n",
        "Z_20ng = hierarchy.linkage(data, method = 'ward', metric = 'euclidean')\n",
        "plt.figure(figsize=(14,4))\n",
        "R_20ng = hierarchy.dendrogram(Z_20ng, p=4, truncate_mode = 'level', show_leaf_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the Number of Clusters\n",
        "\n",
        "Let's flatten the hierarchy to different numbers clusters and calculate the \n",
        "_Silhouette Score_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_clusters = 20\n",
        "s = np.zeros(max_clusters+1)\n",
        "\n",
        "for k in range(2, max_clusters+1):\n",
        "    clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n",
        "    s[k] = metrics.silhouette_score(np.asarray(data), clusters, metric = 'euclidean')\n",
        "\n",
        "plt.plot(range(2, len(s)), s[2:], '.-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a first peak at 5.\n",
        "\n",
        "Top terms per cluster when we flatten to a depth of 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "k = 5\n",
        "clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n",
        "for i in range(1,k+1):\n",
        "    items = np.array([item for item,clust in zip(data, clusters) if clust == i])\n",
        "    centroids = np.squeeze(items).mean(axis = 0)\n",
        "    asc_order_centroids = centroids.argsort()#[:, ::-1]\n",
        "    order_centroids = asc_order_centroids[::-1]\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    print(f'Cluster {i}:')\n",
        "    for ind in order_centroids[:10]:\n",
        "        print(f' {terms[ind]}')\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of Linkages\n",
        "\n",
        "Scikit-Learn has a very nice [notebook](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html)\n",
        "and plot, copied here, that shows the different clusters resulting from different\n",
        "linkage methods.\n",
        "\n",
        "![](./figs/L08-sphx_glr_plot_linkage_comparison_001.png){width=\"100%\" fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Recap\n",
        "\n",
        "## Clustering Recap\n",
        "\n",
        "This wraps up our _partitional_ Cluster topics. We covered:\n",
        "\n",
        "::: {.incremental}\n",
        "* What the clusering problem is\n",
        "* An overview of the $k$-means clustering algorithm including initialization with $k$-means++\n",
        "* Visualization techniques such as Multi-Dimensional Scaling\n",
        "* Cluster evaluation with (Adjusted) Rand Index and Silhouette Coefficient\n",
        "* Using evaluation to determine number of clusters\n",
        "* Hierarchical Clustering with different methods and metrics\n",
        "* Looked at applications of clustering on various types of synthetic data, image\n",
        "  color quantization, newsgroup clustering\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/ds701/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}