{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'NN II -- Compute Graph, Backprop and Training'\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/24-NN-II-Backprop.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mp\n",
        "from IPython.display import Image, HTML\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lecture we'll gradually build out a light weight neural network training framework reminiscent of PyTorch.\n",
        "\n",
        "We build:\n",
        "* A simple neural network engine we call `Value` class that wraps numbers and math operators and includes useful attributes and methods for implementing _forward pass_ and _back propagation_. _(~63 lines of code)_\n",
        "\n",
        "We'll provide (and explain)\n",
        "* A simple _compute graph_ visualization function. _(34 lines of code)_\n",
        "* A small set of helper functions that easily define a neuron, layer and multi-layer perceptron (MLP). _(84 lines of code)_\n",
        "\n",
        "With that we can implement a neural network training loop, and see how similar it is to a PyTorch implementation.\n",
        "\n",
        "\n",
        "The code is based on Andrej Karpathy's [micrograd](https://github.com/karpathy/micrograd).\n",
        "\n",
        "## Neuron and Neural Networks (Recap)\n",
        "\n",
        "Now let's switch gears a bit to define an _artificial neuron_. For better or worse\n",
        "it is named after and loosely modeled on a biological neuron.\n",
        "\n",
        "<!-- Image Credit \"https://cs231n.github.io/neural-networks-1/\"-->\n",
        "\n",
        "<center>\n",
        "    \n",
        "<img src=\"figs/NN-figs/neuron.png\" width=\"75%\">\n",
        "    \n",
        "</center> \n",
        "\n",
        "From [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "* The dendrites carry impulses from other neurons of different distances.\n",
        "* Once the collective firing rate of the impulses exceed a certain threshold, the neuron fires its own pulse through the axon to other neurons\n",
        "\n",
        "There are companies trying to mimic this impulse (i.e. spiking) based neuron in silicon -- so called _neuromorphic computing_.\n",
        "\n",
        "See for example [Neuromorphic Computing](https://en.wikipedia.org/wiki/Neuromorphic_engineering) or [Spiking Neural Network](https://en.wikipedia.org/wiki/Spiking_neural_network)\n",
        "\n",
        "Some examples of companies and projects are Intel's [Loihi](https://www.intel.com/content/www/us/en/research/neuromorphic-computing-loihi-2-technology-brief.html) and startups such as GrAI Matter Labs [VIP processor](https://www.graimatterlabs.ai/product).\n",
        "\n",
        "### Artificial Neuron\n",
        "\n",
        "<!-- Image Credit \"https://cs231n.github.io/neural-networks-1/\"-->\n",
        "\n",
        "<center>\n",
        "    \n",
        "<img src=\"figs/NN-figs/neuron_model.jpeg\" width=\"75%\">\n",
        "    \n",
        "</center> \n",
        "\n",
        "From [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "The more common artifical neuron \n",
        "* collects one or more inputs, \n",
        "* each multiplied by a unique weight\n",
        "* sums the weighted inputs\n",
        "* adds a bias\n",
        "* then finally usually applies a nonlinear activation function\n",
        "\n",
        "### Relating Back to Earlier Lectures\n",
        "\n",
        "__Question__\n",
        "\n",
        "What does \n",
        "\n",
        "$$\\sum_i w_i x_i + b$$\n",
        "\n",
        "remind you of?\n",
        "\n",
        "__Answer__\n",
        "\n",
        "How about multiple regression in the Linear Regression lecture?\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 u + \\beta_2 v$$\n",
        "\n",
        "We just renamed things: $\\beta_0 \\gets b$, $\\beta_1 \\gets w_0$, $\\beta_2 \\gets w_1$, $u \\gets x_0$ and $v \\gets x_1$ for $i \\in [0,1]$.\n",
        "\n",
        "> So multiple linear regression can be viewed as one linear neuron.\n",
        "\n",
        "Activation function is typically some nonlinear function that compresses the input in some way. Historically, it's been the sigmoid and $\\tanh()$ functions. See for example [Hyperbolic Functions](https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('Sigmoid function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('sigmoid(x)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hyperbolic tangent, $\\tanh$, is basically the sigmoid function shifted and scaled to a range of [-1,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "plt.plot(np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2)))\n",
        "plt.title('tanh(x)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more common activation function these days and that is more efficient to implement is the _Rectified Linear Unit_ or _ReLU_.\n",
        "\n",
        "$$ \\textrm{ReLU}(x) = \\mathrm{max}(0, x) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "plt.plot(np.arange(-5,5,0.2), np.maximum(0,np.arange(-5,5,0.2)))\n",
        "plt.title('ReLU(x)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many other variations. See for example [PyTorch Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "\n",
        "### Relating Back to _Another_ Earlier Lecture\n",
        "\n",
        "__Question__\n",
        "\n",
        "What does \n",
        "\n",
        "$$ \\mathrm{sigmoid}(\\sum_i w_i x_i + b) \\hspace{10pt} \\textrm{where} \\hspace{10pt} \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "remind you of?\n",
        "\n",
        "__Answer__\n",
        "\n",
        "How about the Logistic Regression model?\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "P(y=1\\mid x) & = \\frac{e^{\\alpha+\\beta x}}{1+e^{\\alpha+\\beta x}}\\\\\n",
        "             & = \\frac{e^{\\alpha+\\beta x}}{1+e^{\\alpha+\\beta x}} \\left( \\frac{e^{-(\\alpha+\\beta x)}}{e^{-(\\alpha+\\beta x)}} \\right) \\\\\n",
        "             & = \\frac{e^{(\\alpha+\\beta x)-(\\alpha+\\beta x)}}{e^{-(\\alpha+\\beta x)}+e^{(\\alpha+\\beta x)-(\\alpha+\\beta x)}} \\\\\n",
        "             & = \\frac{e^0}{e^{-(\\alpha+\\beta x)} + e^0} \\\\\n",
        "             & = \\frac{1}{1 + e^{-(\\alpha+\\beta x)}}\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Which is the Sigmoid with $\\alpha = 0$ and $\\beta = 1$.\n",
        "\n",
        "In fact, just like in Logistic Regression, we use the sigmoid function on the last layer of a neural network that is\n",
        "doing binary classification to output the probabilities.\n",
        "\n",
        "> So Logistic Regression is similar to one neuron with a sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": true,
        "slideshow": {
          "slide_type": "subslide"
        },
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "alphas = [-4, -8,-12,-20, 0]\n",
        "betas = [0.4,0.4,0.6,1, 1]\n",
        "x = np.arange(-10,35)\n",
        "fig = plt.figure(figsize=(8, 6)) \n",
        "ax = plt.subplot(111)\n",
        "\n",
        "for i in range(len(alphas)):\n",
        "    a = alphas[i]\n",
        "    b = betas[i]\n",
        "    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n",
        "    ax.plot(x,y,label=r\"$\\alpha=%d,$    $\\beta=%3.1f$\" % (a,b))\n",
        "\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('x', fontsize = 14)\n",
        "ax.set_ylabel('$p(x)$', fontsize = 14)\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\n",
        "ax.set_title('Logistic Functions', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)\n",
        "\n",
        "<center>\n",
        "    \n",
        "<img src=\"figs/NN-figs/neural_net2.jpeg\" width=\"75%\">\n",
        "    \n",
        "</center>\n",
        "\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "Multiple artificial neurons can be acting on the same inputs, in what we call\n",
        "a _layer_, and we can have more than one _layer_ until we produce one or more\n",
        "outputs.\n",
        "\n",
        "The example above shows a network with _3 inputs_, two layers of neurons, each\n",
        "with 4 neurons, followed by one layer that produces a single value output.\n",
        "\n",
        "E.g. a binary classifier.\n",
        "\n",
        "## Computation Graph\n",
        "\n",
        "The way we are going to differentiate more complex functions is to first build a \"computation graph\" to apply our operations on. We'll see that it breaks down the process into simple steps that easily scale to large networks.\n",
        "\n",
        "It's the concept employed by TensorFlow and PyTorch, and in fact we'll follow PyTorch interface definition.\n",
        "\n",
        "### Building the `Value` Class\n",
        "\n",
        "To do that we will build a data wrapper as a `class` called `Value` and gradually build in on all the functionality we need to define a Multi-Layer Neural Network (a.k.a. Multi-Layer Perceptron) and train it.\n",
        "\n",
        "First, the class has only a simple initialization method and a representation method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "# Value version 1\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(4.0)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are not familiar with [python classes](https://docs.python.org/3/tutorial/classes.html), there are a few things to note here.\n",
        "1. The property `self` is just a pointer to the object itself.\n",
        "2. The `__init__` method is called when you initialize a class object\n",
        "3. The `__repr__` method is how you represent the class object\n",
        "\n",
        "### Implementing Addition\n",
        "\n",
        "So the Value object doesn't do much yet except for taking a value and printing it. We'd also like to do things like addition and other operations with them, but..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(4.0)\n",
        "b = Value(-3.0)\n",
        "\n",
        "try:\n",
        "    a+b \n",
        "except Exception as e:\n",
        "    print(\"Uh oh!\", e)\n",
        "else:\n",
        "    print(\"It worked!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When python tries to add two objects `a` and `b`, internally it will call\n",
        "`a.__add__(b)`. So we have to add the `__add__()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Value version 2\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "a = Value(4.0)\n",
        "b = Value(-3.0)\n",
        "\n",
        "try:\n",
        "    a+b\n",
        "except Exception as e:\n",
        "    print(\"Uh oh!\", e)\n",
        "else:\n",
        "    print(\"It worked!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which, as mentioned is equivalent to calling the `__add__` method on `a`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a.__add__(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing More Operations\n",
        "\n",
        "Similarly we can support multiplication and implement a ReLU function as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Value version 3\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other): # self + other\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data)\n",
        "        return out\n",
        "    \n",
        "    def __mul__(self, other): # self * other\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data)\n",
        "        return out\n",
        "    \n",
        "    def relu(self):\n",
        "        out = Value(np.maximum(0, self.data))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(4.0)\n",
        "b = Value(-3.0)\n",
        "c = Value(8.0)\n",
        "\n",
        "d = a*b+c\n",
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d.relu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By the way, internally, python will call `__mul__` on `a`, then `__add__` on the temporary product object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(a.__mul__(b)).__add__(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Child Nodes\n",
        "\n",
        "In order to calculate the gradients, we will need to capture the computation graphs. To do that, we'll need to store pointers to the operands of each operation.\n",
        "\n",
        "To start with, we'll accept a tuple of child nodes in the initializer and store that as a set in the object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Value version 4\n",
        "class Value:\n",
        "                        #    vvvvvvvvvvvv\n",
        "    def __init__(self, data, _children=()):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other)) # store tuple of children\n",
        "        return out                        # ^^^^^^^^^^^^^\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other)) # store tuple of children\n",
        "        return out                        # ^^^^^^^^^^^^^\n",
        "    \n",
        "    def relu(self):\n",
        "        out = Value(np.maximum(0, self.data), (self,))\n",
        "        return out                         #  ^^^^^^^"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "a = Value(4.0)\n",
        "b = Value(-3.0)\n",
        "c = Value(8.0)\n",
        "\n",
        "d = a*b\n",
        "e = d + c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see the children of the operands that produced the output value by printing the `_prev` value. The name `_prev` might not be intuitive yet, but it will make more sense when we view these operations as a graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d._prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e._prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Child Operations\n",
        "\n",
        "Now we've recorded pointers to the child nodes. It would be helpful to also record the operator used.\n",
        "\n",
        "We'll also add labels for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Value version 5\n",
        "class Value:\n",
        "                                    #     vvvvvvv  vvvvvvvv\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # store the operation that created this node\n",
        "        self.label = label # label for the node\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)   \n",
        "        out = Value(self.data + other.data, (self, other), '+') # store tuple of children\n",
        "        return out                                      #  ^^^\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*') # store tuple of children\n",
        "        return out                                      #  ^^^\n",
        "    \n",
        "    def relu(self):                                 #  vvvvvv\n",
        "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
        "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = Value(4.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(8.0, label='c')\n",
        "\n",
        "d = a*b ; d.label = 'd'\n",
        "e = d + c ; e.label = 'e'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d._prev, d._op, d.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e._prev, e._op, e.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Compute Graph\n",
        "\n",
        "We now have enough information stored about the compute graph to visualize it.\n",
        "\n",
        "These are two functions to walk the graph and build sets of all nodes and edges (`trace`) and then draw them as a\n",
        "directed graph (`draw_dot`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "hide-input"
        ]
      },
      "source": [
        "# draw_dot version 1\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and set of all edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f }\" % (n.label, n.data), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this no de to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "a = Value(4.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(8.0, label='c')\n",
        "\n",
        "d = a*b ; d.label = 'd'\n",
        "e = d + c ; e.label = 'e'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "tags": []
      },
      "source": [
        "draw_dot(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that every value object becomes a node in the graph. The operators are also represented as a kind of fake node so they can be visualized too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "nodes, edges = trace(e)\n",
        "print(\"Nodes: \", nodes)\n",
        "print(\"Edges: \", edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets add one more operation, or stage in the compute graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "a = Value(4.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(8.0, label='c')\n",
        "\n",
        "d = a*b; d.label = 'd'\n",
        "e = d + c; e.label = 'e'\n",
        "f = Value(2.0, label='f')\n",
        "\n",
        "L = e*f; L.label = 'L'\n",
        "\n",
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recap\n",
        "\n",
        "So far we've built a Value class and associated data structures to capture a computational graph and calculate the output based on the inputs and operations. We'll call this the __forward pass__.\n",
        "\n",
        "But now, we're interested in calculating the gradients with respect to some of the parameters with respect to $L$. \n",
        "\n",
        "So next we'll update our Value class to capture the partial derivative at each node relative to L.\n",
        "\n",
        "## Calculating Gradients\n",
        "\n",
        "Add a gradient member variable, `grad`, to our class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Value version 6\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0 # default to 0  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # store the operation that created this node\n",
        "        self.label = label # label for the node\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)   \n",
        "        out = Value(self.data + other.data, (self, other), '+') # store tuple of children\n",
        "        return out\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)   \n",
        "        out = Value(self.data * other.data, (self, other), '*') # store tuple of children\n",
        "        return out\n",
        "    \n",
        "    def relu(self):         \n",
        "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
        "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And update `draw_dot()` to show `grad` in the node info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# draw_dot version 2\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and set of all edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this node to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And reinitialize and redraw..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        },
        "tags": []
      },
      "source": [
        "a = Value(4.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(8.0, label='c')\n",
        "\n",
        "d = a*b; d.label = 'd'\n",
        "e = d + c; e.label = 'e'\n",
        "f = Value(2.0, label='f')\n",
        "\n",
        "L = e*f; L.label = 'L'\n",
        "\n",
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Gradient Calculation\n",
        "\n",
        "Before we start implementing backpropagation, it is helpful to manually calculate some gradients to better understand the procedure.\n",
        "\n",
        "For the node $L$, we trivially calculate $\\frac{dL}{dL} = 1$. \n",
        "\n",
        "From limit ratio perspective, \n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial L} = \\lim_{h \\rightarrow 0} \\frac{ (L+h) - L }{h} = \\frac{h}{h} = 1$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "L.grad = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we go backwards a step in the graph, we see that $L=e*f$, so we calculate\n",
        "\n",
        "$$\\frac{\\partial{L}}{\\partial{e}} = \\frac{\\partial}{\\partial{e}} (e\\times f) = f$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\frac{\\partial{L}}{\\partial{f}} = \\frac{\\partial}{\\partial{f}} (e\\times f) = e.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "e.grad = f.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "f.grad = e.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> To summarize, the partial derivative w.r.t. to one operand of a simple product is simply the other operand.\n",
        "\n",
        "And we can redraw the graph above again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How do the parameters $e$ and $f$ influence $L$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "# Try uncommenting `e += h` or `f += h` and calling `wiggle()` then `wiggle(1.0)`\n",
        "# to see the influence of e or f on L\n",
        "def wiggle(h = 0.0):\n",
        "    a = Value(4.0, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(8.0, label='c')\n",
        "\n",
        "    d = a*b; d.label = 'd'\n",
        "    e = d + c; e.label = 'e'\n",
        "    # e += h\n",
        "    f = Value(2.0, label='f')\n",
        "    f += h\n",
        "\n",
        "    L = e*f; L.label = 'L'\n",
        "    print(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle(1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Propagating Back\n",
        "\n",
        "Now we want to calculate\n",
        "\n",
        "$$\\frac{\\partial{L}}{\\partial{c}}$$\n",
        "\n",
        "or put another way, we want to know how much $L$ wiggles if we wiggle $c$, or how $c$ influences $L$.\n",
        "\n",
        "Looking at the graph again we see that $c$ influences $e$ and $e$ influences $L$, so we should be able see the ripple effect of $c$ on $L$.\n",
        "\n",
        "$$ c \\rightarrow e \\rightarrow L $$\n",
        "\n",
        "So $e = c + d$, and so we calculate\n",
        "\n",
        "$$ \\frac{\\partial{e}}{\\partial{c}} = \\frac{\\partial{}}{\\partial{c}} (d + c) = 1$$\n",
        "\n",
        "### Question\n",
        "\n",
        "$$ c \\rightarrow e \\rightarrow L $$\n",
        "\n",
        "So now we know $\\partial{L}/\\partial{e}$ and we also know $\\partial{e}/\\partial{c}$,\n",
        "\n",
        "How do we get $\\partial{L}/\\partial{c}$?\n",
        "\n",
        "### The Chain Rule\n",
        "\n",
        "To paraphrase from the Wikipedia page on [Chain rule](https://en.wikipedia.org/wiki/Chain_rule), if a variable $L$ depends on the variable $e$, which itself depends on the variable $c$ (that is, $e$ and $L$ are dependent variables), then $L$ depends on $c$ as well, via the intermediate variable $e$. In this case, the chain rule is expressed as\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial c} = \\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial c},$$\n",
        "\n",
        "and\n",
        "\n",
        "$$ \\left.\\frac{\\partial L}{\\partial c}\\right|_{c} = \\left.\\frac{\\partial L}{\\partial e}\\right|_{e(c)}\\cdot \\left. \\frac{\\partial e}{\\partial c}\\right|_{c} ,$$\n",
        "\n",
        "for indicating at which points the derivatives have to be evaluated.\n",
        "\n",
        "Now since we've established that\n",
        "\n",
        "$$ \\frac{\\partial{e}}{\\partial{c}} = 1$$\n",
        "\n",
        "then\n",
        "\n",
        "$$\\frac{dL}{dc} = \\frac{dL}{de} \\cdot 1.$$\n",
        "\n",
        "So in the case of an operand in an addition operation, we just copy the gradient of the parent node.\n",
        "\n",
        "Or put another way, \n",
        "\n",
        "> in the addition operator, we just route the parent gradient to the child."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "d.grad = e.grad\n",
        "c.grad = e.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How does $c$ and $d$ influence $L$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Try uncommenting `c += h` or `d += h` and calling `wiggle()` then `wiggle(1.0)`\n",
        "# to see the influence of c or d on L\n",
        "def wiggle(h = 0.0):\n",
        "    a = Value(4.0, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(8.0, label='c')\n",
        "    # c += h\n",
        "\n",
        "    d = a*b; d.label = 'd'\n",
        "    # d += h\n",
        "\n",
        "    e = d + c; e.label = 'e'\n",
        "    f = Value(2.0, label='f')\n",
        "\n",
        "    L = e*f; L.label = 'L'\n",
        "    print(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle(1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Propagating Back Again\n",
        "\n",
        "Now we want to calculate\n",
        "\n",
        "$$ \\frac{\\partial{L}}{\\partial{b}} \\hspace{10pt} \\textrm{and} \\hspace{10pt} \\frac{\\partial{L}}{\\partial{a}}$$\n",
        "\n",
        "But we have\n",
        "\n",
        "$$ \\frac{\\partial{L}}{\\partial{d}}$$\n",
        "\n",
        "and we know that\n",
        "\n",
        "$$ \\frac{\\partial{d}}{\\partial{b}} = \\frac{\\partial{}}{\\partial{b}}(a\\cdot b) = a$$\n",
        "\n",
        "so again from the chain rule\n",
        "\n",
        "$$\\frac{\\partial{L}}{\\partial{b}} \n",
        "  = \\frac{\\partial{L}}{\\partial{d}} \\cdot \\frac{\\partial{d}}{\\partial{b}}\n",
        "  = \\frac{\\partial{L}}{\\partial{d}} \\cdot a$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "b.grad = a.data * d.grad\n",
        "a.grad = b.data * d.grad\n",
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Try uncommenting `a += h` or `b += h` and calling `wiggle()` then `wiggle(1.0)`\n",
        "# to see the influence of a or b on L\n",
        "def wiggle(h = 0.0):\n",
        "    a = Value(4.0, label='a')\n",
        "    # a += h\n",
        "    b = Value(-3.0, label='b')\n",
        "    b += h\n",
        "    c = Value(8.0, label='c')\n",
        "\n",
        "    d = a*b; d.label = 'd'\n",
        "\n",
        "    e = d + c; e.label = 'e'\n",
        "    f = Value(2.0, label='f')\n",
        "\n",
        "    L = e*f; L.label = 'L'\n",
        "    print(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wiggle(1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recap\n",
        "\n",
        "As you saw, we recursively went backwards through the computation graph and applied the local gradients to the gradients calculated so far to get the partial gradients. Put another we propagated this calculations backwards through the graph.\n",
        "\n",
        "Of course, in practice, we will only need the gradients on the parameters, not the inputs, so we won't bother calculating them on inputs.\n",
        "\n",
        "_That is the essence of Back Propagation._\n",
        "\n",
        "## A Step in Optimization\n",
        "\n",
        "Let's take a look at the graph again. Assume we want the value of L to _decrease_. We are free to change the values of the leaf nodes -- all the other nodes are derived from children and leaf nodes.\n",
        "\n",
        "The leaf nodes are $a, b, c$ and $f$.\n",
        "\n",
        "> Again, in practice we would only update the parameter leaf nodes, not the input leaf node, but we'll ignore that distinction temporarily for this exmaple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "draw_dot(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the current value of L."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# remind ourselves what L is\n",
        "print(L.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we showed before, we want to nudge each of those leaf nodes by the negative\n",
        "of the gradient, multiplied by a step size, $\\eta$.\n",
        "\n",
        "$$ w_{n+1} = w_n - \\eta * \\frac{\\partial{L}}{\\partial{w_n}} $$\n",
        "\n",
        "where $n$ is the iteration number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# nudge all the leaf nodes along the negative direction of the gradient\n",
        "step_size = 0.01    # also called eta above\n",
        "\n",
        "a.data -= step_size * a.grad\n",
        "b.data -= step_size * b.grad\n",
        "c.data -= step_size * c.grad\n",
        "f.data -= step_size * f.grad\n",
        "\n",
        "# recompute the forward pass\n",
        "d = a*b\n",
        "e = d + c\n",
        "L = e*f\n",
        "\n",
        "print(L.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Single Neuron\n",
        "\n",
        "Let's now programmatically define a single neuron with\n",
        "* two inputs\n",
        "* two weights (1 for each input)\n",
        "* a bias\n",
        "* the ReLU activation function\n",
        "\n",
        "Recall the neuron figure above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "tags": []
      },
      "source": [
        "# inputs x0, x1\n",
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "\n",
        "# bias of the neuron\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.relu(); o.label = 'o'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "draw_dot(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only new operation we've added is the ReLU, so let's take a quick look at how we \n",
        "differentiate the ReLU. \n",
        "\n",
        "Like before, for the output node, o:\n",
        "\n",
        "$$ \\frac{\\partial o}{\\partial o} = 1 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "o.grad = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Credit: _Understanding Deep Learning_, Figure 7.6 -->\n",
        "ReLU is technically not differentiable at 0, but practically we implement the\n",
        "derivative as 0 when $ \\le 0$ and 1 when $ 1 > 0 $\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"figs/NN-figs/Train2ReLUDeriv.svg\" width=\"50%\">\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "n.grad = (o.data > 0) * o.grad  # = 0 when o.data <= 0; = o.grad when o.data > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coding Backpropagation\n",
        "\n",
        "Now we'll update our `Value` class once more to support the backward pass.\n",
        "\n",
        "There's a\n",
        "* private `_backward()` function _in each operator_ that implements the local\n",
        "step of the chain rule, and\n",
        "* a `backward()` function in the class that topologically sorts the graph and calls the operator `_backward()` function starting at the end of the graph and going _backward_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# version 9\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0 # default to 0, no impact on the output\n",
        "        self._backward = lambda: None  # by default backward doesn't do anything\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
        "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We redefined the class so we have to reinitialize the objects and run the operations again.\n",
        "\n",
        "This constitutes the _forward pass_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# inputs x0, x1\n",
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "\n",
        "# bias of the neuron\n",
        "#b = Value(6.7, label='b')\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "o = n.relu(); o.label = 'o'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we've filled the data values for all the nodes, but haven't calculated the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "draw_dot(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, all we have to do is call the `backward()` method of the last node..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "o.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And voila! We have all the gradients!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "draw_dot(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accumulating the Gradients\n",
        "\n",
        "The observant viewer will notice that we are accumulating the gradients.\n",
        "\n",
        "That is to handle cases like where a `Value` object is on both sides of the operand like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "a = Value(3.0, label='a')\n",
        "b = a + a ; b.label = 'b'\n",
        "b.backward()\n",
        "\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we didn't have the accumulation, then `a.grad = 1` instead.\n",
        "\n",
        "Or the other case where a node goes to different operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "a = Value(-2.0, label='a')\n",
        "b = Value(3.0, label='b')\n",
        "d = a * b  ; d.label = 'd'\n",
        "e = a + b   ; e.label = 'e'\n",
        "  \n",
        "draw_dot( f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see analytical verification of the above result in the note in the online book.\n",
        "\n",
        "The risk now is that if you don't zero the gradients for the next update iteration, you will have incorrect gradients. \n",
        "\n",
        "> Always remember to zero the gradient in each iteration of the training loop!\n",
        "\n",
        "```{note}\n",
        "\n",
        "We can verify that the gradients are correct analytically.\n",
        "\n",
        "To find the partial derivative $\\frac{\\partial f}{\\partial a}$, we first need to define $f$ in terms of $a$ and $b$.\n",
        "\n",
        "Given:\n",
        "$$\\begin{aligned}\n",
        "d &= a \\times b \\\\\n",
        "e &= a + b      \\\\\n",
        "f &= d \\times e\n",
        "\\end{aligned}$$\n",
        "\n",
        "Then $f$ can be expanded as:\n",
        "$$\\begin{aligned}\n",
        "f &= (a \\times b) \\times (a + b) \\\\\n",
        "f &= a^2 \\times b + a \\times b^2\n",
        "\\end{aligned}$$\n",
        "\n",
        "Next, we find the partial derivative of $f$ with respect to $a$:\n",
        "$$ \\frac{\\partial f}{\\partial a} = 2a \\times b + b^2 $$\n",
        "\n",
        "Finally, we plug in the given values $a = -2.0$ and $b = 3.0$:\n",
        "$$\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial a} &= 2(-2.0) \\times 3.0 + 3.0^2 \\\\\n",
        "\\frac{\\partial f}{\\partial a} &= -12.0 + 9.0                 \\\\\n",
        "\\frac{\\partial f}{\\partial a} &= -3.0\n",
        "\\end{aligned}$$\n",
        "\n",
        "So the partial derivative $\\frac{\\partial f}{\\partial a}$ for the value $a = -2.0$ is $-3.0$.\n",
        "\n",
        "```\n",
        "\n",
        "## Enhancements to `Value` Class\n",
        "\n",
        "There are still some useful operations that `Value` doesn't support, so to be more\n",
        "complete we have the final version of the `Value` class below.\n",
        "\n",
        "We added:\n",
        "* `__radd__` for when the `Value` object is the right operand of an add\n",
        "* `__rmul__` for when the `Value` object is the right operand of a product \n",
        "* `__pow__` to support the ** operator\n",
        "* plus some others you can see below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# version 9\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0 # default to 0, no impact on the output\n",
        "        self._backward = lambda: None  # by default backward doesn't do anything\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        \"\"\"Adding support for ** operator, which we'll need for the \n",
        "        squared loss function\"\"\"\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n",
        "        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "    \n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Return a string representation of the object for display\"\"\"\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing to PyTorch\n",
        "\n",
        "We're using a class implementation that resembles the PyTorch implementation, and in fact we can compare our implementation with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
        "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
        "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
        "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
        "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.relu(n)\n",
        "\n",
        "print(o.data.item())\n",
        "o.backward()\n",
        "\n",
        "print('---')\n",
        "print('x2.grad', x2.grad.item())\n",
        "print('w2.grad', w2.grad.item())\n",
        "print('x1.grad', x1.grad.item())\n",
        "print('w1.grad', w1.grad.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, tensors don't store gradients and so won't support backprop, so we explicitly set `requires_grad = True`.\n",
        "\n",
        "## Neural Network Modules\n",
        "\n",
        "Now we'll define some classes which help us build out a small neural network.\n",
        "\n",
        "__Module__ -- A base class\n",
        "\n",
        "__Neuron__ -- Implement a single linear or nonlinear neuron with `nin` inputs.\n",
        "\n",
        "__Layer__ -- Implement a layer of network consisting of `nout` neurons, each taking `nin` inputs\n",
        "\n",
        "__MLP__ -- A _Multi-Layer Perceptron_ that implements `len(nouts)` layers of neurons.\n",
        "\n",
        "Each class can calculate a forward pass and enumerate all its parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "tags": []
      },
      "source": [
        "import random\n",
        "# we assume that Value class is already defined\n",
        "\n",
        "class Module:\n",
        "    \"\"\"Define a Neural Network Module base class \"\"\"\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"When we run in a training loop, we'll need to zero out all the gradients\n",
        "        since they are defined to accumulate in the backwards passes.\"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "    \"\"\"Define a Neuron as a subclass of Module\"\"\"\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        \"\"\"Randomly initialize a set of weights, one for each input, and initialize the bias to zero.\"\"\"\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0.0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Implement the forward pass of the neuron\"\"\"\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "    \"\"\"Define a Layer of Network as a subclass of Module\"\"\"\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        \"\"\"Initialize nout Neurons, each with nin inputs\"\"\"\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass each neuron in the layer\"\"\"\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "    \"\"\"Define a Multi-Layer Perceptron\"\"\"\n",
        "\n",
        "    def __init__(self, nin: int, nouts: list):\n",
        "        \"\"\"\n",
        "        Initialize the Multi-Layer Perceptron, by initializing each layer\n",
        "        then initializing each neuron of each layer.\n",
        "\n",
        "        Parameters:\n",
        "            nin: Number of inputs (int)\n",
        "            nouts: A list of the number of neurons in each layer\n",
        "        \"\"\"\n",
        "        sz = [nin] + nouts\n",
        "        # self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "        # Create a list of layer objects for this MLP. All but the last layer\n",
        "        # have ReLU activations. The last layer is linear.\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass through the MLP\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Recursively retrieve the parameters of the MLP\"\"\"\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# help(Module)\n",
        "# Module.__doc__\n",
        "# help(Neuron)\n",
        "# help(Layer)\n",
        "# help(MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize and Evaluate a Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# 2 inputs\n",
        "x = [2.0, 3.0]\n",
        "\n",
        "# initialize neuron with 2 inputs\n",
        "n = Neuron(2, nonlin=False)\n",
        "\n",
        "# evaluate our neuron with our 2 inputs\n",
        "n(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# list the 2 weights and the bias\n",
        "n.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize and Evaluate a Layer of Neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# same 2 inputs again\n",
        "x = [2.0, 3.0]\n",
        "\n",
        "# Now initialize a layer of 3 neurons, each with 2 inputs\n",
        "l = Layer(2, 3, nonlin=False)\n",
        "\n",
        "# Evaluate our layer of neurons with the 2 inputs\n",
        "l(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "l.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize and Evaluate an MLP\n",
        "\n",
        "We'll instantiate an MLP like the picture below.\n",
        "\n",
        "<center>\n",
        "<img src=\"figs/NN-figs/neural_net2.jpeg\" width=\"75%\">\n",
        "</center>\n",
        "\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/convolutional-networks/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "x = [2.0, 3.0, -1.0]\n",
        "m = MLP(3, [4, 4, 1])\n",
        "m(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# m.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "subslide"
        },
        "tags": []
      },
      "source": [
        "draw_dot(m(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "So after manually iterating, we put it all together in a training loop. We can repeatedly execute the next cell to continue training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define 4 different sets of inputs\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "\n",
        "# For each input set, we have a desired target value -- binary classification\n",
        "# ys = [1.0, -1.0, -1.0, 1.0]\n",
        "ys = [1.0, 0.0, 0.0, 1.0]\n",
        "\n",
        "# Manually seed the Random Number Generator for Reproducibility\n",
        "# You can comment the next line out see the variability\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize an MLP with random weights\n",
        "m = MLP(3, [4, 4, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "tags": []
      },
      "source": [
        "losses = []\n",
        "niters = 100\n",
        "step_size = 0.01\n",
        "\n",
        "for k in range(niters):\n",
        "\n",
        "    # Training Step 1: forward pass\n",
        "    ypred = [m(x) for x in xs]\n",
        "    \n",
        "    # Training Step 2: Calculate the loss\n",
        "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "    losses.append(loss.data)\n",
        "\n",
        "    # Training Step 3: Zero the gradients and run the backward pass\n",
        "    m.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Training Step 4: Update parameters\n",
        "    for p in m.parameters():\n",
        "        p.data += -step_size * p.grad\n",
        "\n",
        "    # print(k, loss.data)\n",
        "\n",
        "print(\"Final Loss: \", loss.data)\n",
        "    \n",
        "plt.plot(losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.title(\"Loss Per Iteration\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Train the Equivalent MLP in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "\n",
        "# Manually seed the Random Number Generator for Reproducibility\n",
        "# You can comment the next line out see the variability\n",
        "torch.manual_seed(99)\n",
        "\n",
        "# Step 1: Define the MLP model\n",
        "class ptMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ptMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(3, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = ptMLP()\n",
        "print(model)\n",
        "\n",
        "# Step 2: Define a loss function and an optimizer\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step 3: Create a tiny dataset\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "\n",
        "# we had to transpose ys for torch.tensor\n",
        "ys_transpose = [[1.0], \n",
        "      [0.0], \n",
        "      [0.0], \n",
        "      [1.0]]\n",
        "\n",
        "inputs = torch.tensor(xs)\n",
        "outputs = torch.tensor(ys_transpose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now run the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Write the training loop\n",
        "losses = []\n",
        "niters = 100\n",
        "\n",
        "for epoch in range(niters):\n",
        "\n",
        "    # Training Step 1: Forward pass\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # Training Step 2: Calculate the loss\n",
        "    loss = criterion(predictions, outputs)\n",
        "\n",
        "    # Training Step 3: Zero the gradient and run backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Training Step 4: Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    # print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.title(\"Loss Per Iteration\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To Dig a Little Deeper\n",
        "\n",
        "[PyTorch Quick Start Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "\n",
        "[TensorFlow Playground](https://playground.tensorflow.org/#activation=relu&batchSize=30&dataset=gauss&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=4,4&seed=0.75152&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
        "\n",
        "## Summary\n",
        "\n",
        "So today we...\n",
        "\n",
        "* got a glimpse of the wide applications of neural networks\n",
        "* revisited loss functions\n",
        "* developed the notion of gradient descent first by intuition, then in the univariate case, then the multivariate case\n",
        "* defined artificial neurons\n",
        "* implemented a computation graph and visualization\n",
        "* implemented the chain rule as backpropagation on the computation graph\n",
        "* defined Neuron, Layer and MLP modules which completes are homegrown Neural Network Framework\n",
        "* then trained a small MLP on a tiny dataset\n",
        "* finally implemented the same in PyTorch\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tomg/Source/courses/tools4ds/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}