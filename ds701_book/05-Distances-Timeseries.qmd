---
title: Distances and Time Series
jupyter: python3
---

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)

We will start building some tools for making comparisons of data objects with particular attention to time series.

Working with data, we can encounter a wide variety of different data objects

::: {.incremental}
* records of users,
* images,
* videos,
* text (webpages, books),
* strings (DNA sequences), and
* time series.
:::

::: {.fragment}
How can we compare them?
:::

::: {.content-visible when-profile="slides"}
## Lecture Overview

We cover the following topics in today's lecture

:::: {.incremental}
- feature space and matrix representations of data,
- metrics, norms, similarity, and dissimilarity,
- bit vectors, sets, and time series.
::::
:::

::: {.content-visible when-profile="slides"}
## Time Series Data
Some examples of time series data

:::: {.incremental}
- stock prices,
- weather data,
- electricity consumption,
- website traffic,
- retail sales, and
- various economic indicators.
::::
:::

## Feature space representation

Usually a data object consists of a set of attributes.

These are also commonly called __features.__

* ("J. Smith", 25, \$ 200,000)
* ("M. Jones", 47, \$ 45,000)

If all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.
 
* `(25, USD 200000)` $\rightarrow \begin{bmatrix}25\\200000\end{bmatrix}$.

Likewise If all features are binary then we can think of each data object as a binary vector in vector space.

The space is called __feature space.__

::: {.content-hidden when-profile="web"}
## One-hot encoding
:::
Vector spaces are such a useful tool that we often use them even for non-numeric data.

For example, consider a categorical variable that can be only one of "house", "tree", or "moon". For such a variable, we can use a __one-hot__ encoding.  

::: {.content-hidden when-profile="slides"}
We would encode as follows:
:::

::: {.incremental}
* `house`: $[1, 0, 0]$
* `tree`:  $[0, 1, 0]$
* `moon`:  $[0, 0, 1]$
:::

::: {.fragment}
So an encoding of `(25, USD 200000, 'house')` could be: 
$$\begin{bmatrix}25\\200000\\1\\0\\0\end{bmatrix}.$$
::: 

::: {.content-hidden when-profile="web"}
## Encodings
:::

We will see many other encodings that take non-numeric data and encode them into vectors or matrices.

For example, there are vector or matrix encodings for

::: {.incremental}
* graphs,
* images, and
* text.
:::

## Matrix representation

We generally store data in a matrix form as

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} x_{11} \\ \vdots \\ x_{i1} \\ \vdots \\ x_{m1} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1j} \\ \vdots \\ x_{ij} \\ \vdots \\ x_{mj} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1n} \\ \vdots \\ x_{in} \\ \vdots \\ x_{mn} \end{array}
\end{array}\right]}^{\mbox{$n$ features}} 
$$

The number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.

## Metrics

A metric is a function $d(x, y)$ that satisfies the following properties.

:::: {.columns}
::: {.column width="70%"}
::: {.incremental}
* $d(x, x) = 0$
* $d(x, y) > 0 \hspace{1cm} \forall x\neq y$ (positivity)
* $d(x, y) = d(y, x)$ (symmetry)
* $d(x, y)\leq d(x, z) + d(z, y)$ (triangle inequality)
:::
:::
::: {.column width="30%"}
:::: {.fragment}
![](figs/Lec03-TriangleInequality.png){fig-align="center"}
::::
:::
::::

::: {.fragment}
<<<<<<< HEAD
<<<<<<< Updated upstream
We can use a metric to determine how __similar__ or __dissimilar__ two objects are.

A metric is a measure of the dissimilarity between two objects. The larger the
measure, the more dissimilar the objects are.

If the objects are vectors, then the metric is also commonly called a __distance__.
:::

::: {.content-visible when-profile="web"}
Sometimes we will use "distance" informally, i.e., to refer to a similarity or
dissimilarity function even if we are not sure it is a metric.   

We'll try to say "dissimilarity" in those cases though.
:::

::: {.content-hidden when-profile="web"}
## Distance Matrices
:::

The distance matrix is defined as

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;
\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} 0  \\  d(x_1, x_2) \\ d(x_1,x_3) \\ \vdots \\ d(x_1,x_m)  \end{array} &
\begin{array}{c} \; \\  0      \\ d(x_2,x_3) \\ \vdots \\ d(x_2,x_m)  \end{array} &
\begin{array}{c} \; \\ \;      \\ 0      \\ \vdots \\ \cdots   \end{array} &
\begin{array}{c} \; \\ \;      \\ \;     \\ \ddots \\ d(x_{m-1},x_m)   \end{array}  &
\begin{array}{c} \; \\ \;      \\ \;     \\ \;     \\[6pt] 0 \end{array} &
\end{array}\right]}^{\mbox{$m$ data objects}},
$$

where $x_i$ denotes the $i$-th column of the data matrix $X$.

## Norms

Let $\mathbf{u}, \mathbf{v}\in\mathbb{R}^{n}$ and $a\in\mathbb{R}$. The vector function $p(\mathbf{v})$ is called a __norm__ if

:::: {.fragment}

::: {.incremental}
* $p(a\mathbf{v}) = |a|p(\mathbf{v})$,
* $p(\mathbf{u} + \mathbf{v}) \leq p(\mathbf{u}) + p(\mathbf{v})$,
<<<<<<< HEAD
* $p(\mathbf{v}) = 0 \Leftrightarrow \mathbf{v}=0$.
:::
:::

:::: {.fragment}
=======
* $p(\mathbf{v}) = 0$ if and only if $\mathbf{v} = 0$.
:::

::: {.fragment}
>>>>>>> main
__Every norm defines a corresponding metric.__ 
::::

::: {.fragment}
In particular If $p()$ is a norm, then $d(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}-\mathbf{y})$ is a metric.
:::

::: {.content-visible when-profile="web"}
### $\ell_p$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_p$ norm
:::

A general class of norms are called __$\ell_p$__ norms, where $p \geq 1.$

$$\Vert \mathbf{x} \Vert_p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}}.$$ 

The corresponding distance that an $\ell_p$ norm defines is called the _Minkowski distance._

$$\Vert \mathbf{x} - \mathbf{y} \Vert_p = \left(\sum_{i=1}^d |x_i - y_i|^p\right)^{\frac{1}{p}}.$$

::: {.content-visible when-profile="web"}
### $\ell_2$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_2$ norm
:::

A special -- but very important -- case is the $\ell_2$ norm.

$$
\Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^d |x_i|^2}.
$$

We've already mentioned it: it is the __Euclidean__ norm.

The distance defined by the $\ell_2$ norm is the same as the Euclidean distance between two vectors $\mathbf{x}, \mathbf{y}$.

$$ 
\Vert \mathbf{x} - \mathbf{y} \Vert_2  = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}.
$$

::: {.content-visible when-profile="web"}
### $\ell_1$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_1$ norm
:::

Another important special case is the $\ell_1$ norm.

$$ \Vert \mathbf{x} \Vert_1 = \sum_{i=1}^d |x_i|.$$

This defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:

$$ \Vert \mathbf{x} - \mathbf{y} \Vert_1 = \sum_{i=1} |x_i - y_i|.$$

![](figs/L05-manhattan-distance.png){fig-align="center"}

::: {.content-visible when-profile="web"}
### $\ell_\infty$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_\infty$ norm
:::

If we take the limit of the $\ell_p$ norm as $p$ gets large we get the $\ell_\infty$ norm.  

We have that

$$
\Vert \mathbf{x} \Vert_{\infty} = \max_{i} \vert x_{i} \vert .
$$

::: {.fragment}
What is the metric that this norm induces?
:::

::: {.content-visible when-profile="web"}
## $\ell_0$ norm

Another related idea is the $\ell_0$ "norm," which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.

Note that this is __not__ a norm, but it gets called that anyway.   

This "norm" simply counts the number of __nonzero__ elements in a vector.

This is called the vector's __sparsity.__
:::

::: {.content-hidden when-profile="web"}
## Visualizing norms
:::

Here is the notion of a "circle" under each of three norms.

That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.

:::: {.columns}
::: {.column width="50%"}
![](figs/L5-Vector-Norms.png){fig-align="center"}
:::
::: {.column width="50%"}

<br><br>
$|x_1| + |x_2| = 1$

<br><br>
$\sqrt{x_1^2 + x_2^2} = 1$

<br><br>
$\max(|x_1|, |x_2|) = 1$
:::
::::


[Source](https://commons.wikimedia.org/w/index.php?curid=678101)


::: {.content-visible when-profile="slides"}
## What norm should I use?

The choice of norm depends on the characteristics of your data and the problem you're trying to solve.

:::: {.columns}
::: {.column width="33%"}
__$\ell_1$ norm__

- Use when your data is sparse.
- Robust to outliers.
:::
::: {.column width="33%"}
__$\ell_2$ norm__

- Use when measuring distances in Euclidean space.
- Smooth and differentiable.
:::
::: {.column width="33%"}
__$\ell_\infty$ norm__

- Use when you need uniform bounds.
- Maximum deviation.
:::
::::

:::


## Similarity and Dissimilarity Functions

:::: {.columns}
::: {.column width="44%"}
Similarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.

:::: {.fragment}
__Examples__

:::: {.incremental}
- cosine similarity,
- Jaccard similarity.
::::
::::
:::
::: {.column width="55%"}
Dissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.

:::: {.fragment}
__Examples__

:::: {.incremental}
- Manhattan distance,
- Hamming distance.
::::
::::
:::
::::

::: {.content-visible when-profile="web"}
### Similarity
:::

::: {.content-visible when-profile="slides"}
## Similarity
:::

We know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them

$$ \cos(\theta) = \frac{\mathbf{x}^T\mathbf{y}}{\Vert\mathbf{x}\Vert \Vert\mathbf{y}\Vert} \equiv \cos(\mathbf{x}, \mathbf{y})  .$$

This value is close to 1 when $\mathbf{x} \approx \mathbf{y}$. We can use this formula to define a __similarity__ function called the __cosine similarity__ $\cos(\mathbf{x}, \mathbf{y})$.

::: {.content-visible when-profile="web"}
### Dissimilarity
:::

::: {.content-visible when-profile="slides"}
## Dissimilarity
:::

:::: {.fragment}
Given a similarity function $s(\mathbf{x}, \mathbf{y})$, how could we convert it to a dissimilarity function $d(\mathbf{x}, \mathbf{y})$?
::::

:::: {.fragment}
Two straightforward ways of doing that are:

$$d(\mathbf{x},\mathbf{y}) = 1\,/\,s(\mathbf{x},\mathbf{y})$$

or 

$$d(\mathbf{x},\mathbf{y}) = k - s(\mathbf{x},\mathbf{y})$$

for some properly chosen $k$.
::::

:::: {.fragment}
For cosine similarity, one often uses:
    
$$ d(\mathbf{x}, \mathbf{y}) = 1 - \cos(\mathbf{x}, \mathbf{y})$$
::::

::: {.content-visible when-profile="web"}
Note however that this is __not a metric!__
:::


## Bit vectors and Sets

When working with bit vectors, the $\ell_1$ metric is commonly used and is called the __Hamming__ distance.

![](figs/L5-hamming-1.png){fig-align="center"}

This has a natural interpretation: "how well do the two vectors match?"

Or: "What is the smallest number of bit flips that will convert one vector into the other?"

::: {.content-hidden when-profile="web"}
## Hamming distance
:::

![](figs/L5-hamming-2.png){fig-align="center"}

::: {.content-visible when-profile="web"}
In other cases, the Hamming distance is not a very appropriate metric.
:::

::: {.content-hidden when-profile="web"}
## Hamming distance with sets
:::

Consider the case in which the bit vector is being used to represent a set.

In that case, Hamming distance measures the __size of the set difference.__

For example, consider two documents. We will use bit vectors to represent the sets of words in each document.

:::: {.incremental}
* Case 1: both documents are large, almost identical, but differ in 10 words.
* Case 2: both documents are small, disjoint, have 5 words each.
::::

:::: {.fragment}
What matters is not just the size of the set difference, but the size of the intersection.
::::


::: {.content-hidden when-profile="web"}
## Jaccard similarity
:::
This leads to the _Jaccard_ similarity:

$$J_{Sim}(\mathbf{x}, \mathbf{y}) = \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}.$$

:::: {.columns}
::: {.column width="60%"}
This takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$

In fact, this is a __metric!__

$$J_{Dist}(\mathbf{x}, \mathbf{y}) = 1- \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}.$$
:::
::: {.column width="40%"}
![](figs/L5-jaccard-1.png){fig-align="center"}
:::
::::


::: {.content-hidden when-profile="web"}
## Jaccard Similarity Example 1
:::
    
::: {.content-visible when-profile="web"}
Let's revisit the previously introduces cases comparing documents.
:::

Case 1: Very large almost identical documents.

![](figs/L5-jaccard-2.png){fig-align="center"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is almost 1.

::: {.content-hidden when-profile="web"}
## Jaccard Similarity Example 2
:::

Case 2: Very small disjoint documents.

![](figs/L5-jaccard-3.png){fig-align="center"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is 0.

## Time Series

A time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.

Some examples are

::: {.incremental}
* stock prices,
* the volume of sales over time, and
* daily temperature readings.
:::

::: {.fragment}
A time series database is a large collection of time series.
:::

## Similarity of Time Series

Suppose we wish to compare the following time series.

::: {.incremental}
* Stock price movements for companies over a time interval.
* The motion data of two people walking.
* Credit usage patterns for bank clients.
:::

:::: {.fragment}
How should we measure the "similarity" of these time series?
::::

::: {.fragment}
There are two problems to address.

::: {.incremental}
1. Defining a meaningful similarity (or distance) function.
2. Finding an efficient algorithm to compute it.
:::
:::

## Norm-based Similarity Measures

We could just view each sequence as a vector.

Then we could use a $p$-norm, e.g., $\ell_1, \ell_2,$ or $\ell_p$ to measure similarity.

::: {.fragment}

__Advantages__

::: {.incremental}    
1. Easy to compute - linear in the length of the time series (O(n)).
2. It is a metric.
:::
:::

::: {.content-hidden when-profile="web"}
## Beware of Norm-based Similarity
:::

::: {.content-visible when-profile="slides"}
:::: {.columns}
::: {.column width="40%"}
__Disadvantage__

1. May not be __meaningful!__

We may believe that $\mathbf{ts1}$ and $\mathbf{ts2}$ are the most "similar" pair of time series.
:::
::: {.column width="60%"}
![](figs/L5-ts-euclidean.png){fig-align="center"}
:::
::::
:::

::: {.content-visible when-profile="web"}
__Disadvantage__
1. May not be __meaningful!__

![](figs/L5-ts-euclidean.png){fig-align="center"}

We may believe that $\mathbf{ts1}$ and $\mathbf{ts2}$ are the most "similar" pair of time series.
:::

::: {.fragment}
However, according to Euclidean distance: 

$$ \Vert \mathbf{ts1} - \mathbf{ts2} \Vert_2 = 26.9,$$

while 

$$ \Vert \mathbf{ts1} - \mathbf{ts3} \Vert_2 = 23.2.$$
:::

::: {.content-visible when-profile="web"}
## Feature Engineering

In general, there may be different aspects of a time series that are important in different settings.

::: {.fragment}
The first step therefore is to ask yourself "what is important about time series in my application?"
:::

::: {.fragment}
This is an example of __feature engineering.__
:::
:::


::: {.content-visible when-profile="slides"}
## Feature Engineering
:::

Feature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.

::: {.fragment}
A reasonable approach is to

::: {.incremental}    
* extract the relevant features,
* use a simple method (e.g., a norm) to define similarity over those features.
:::
:::

::: {.fragment}
In the case above, one might think of using 

::: {.incremental}
* Fourier coefficients (to capture periodicity),
* histograms,
* or something else!
:::
:::

::: {.content-visible when-profile="web"}
## Dynamic Time Warping
:::

::: {.content-visible when-profile="slides"}
## Bump Hunting
:::

One case that arises often is something like the following:  "bump hunting"

![](figs/L5-DTW-1.png){fig-align="center"} 

Both time series have the same key characteristics: four bumps.

But a one-to-one match (ala Euclidean distance) will not detect the similarity.

::: {.content-visible when-profile="web"}
(Be sure to think about why Euclidean distance will fail here.)
:::

A solution to this is called __dynamic time warping.__


::: {.content-hidden when-profile="web"}
## Dynamic Time Warping
:::

The basic idea is to allow acceleration or deceleration of signals along the time dimension.

::: {.fragment}
__Classic applications__

::: {.incremental}
* speech recognition
* handwriting recognition
:::
:::

::: {.fragment}
Specifically

::: {.incremental}
* Consider $X = x_1, x_2, \dots, x_n$ and $Y = y_1, y_2, \dots, y_m$.
* We are allowed to modify each sequence by inserting, deleting, or matching elements to form $X'$ and $Y'$.
* We then calculate, e.g., Euclidean distance between the extended sequences $X'$ and $Y'$.
:::
:::

::: {.content-hidden when-profile="web"}
## Visualizing DTW
:::

There is a simple way to visualize this algorithm.

Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure).

![](figs/L5-DTW-2.png){fig-align="center"}

$M$ measures the amount of error we get if we match $x_i$ with $y_j$. 

So we seek a __path through $M$ that minimizes the total error.__

::: {.content-hidden when-profile="web"}
## DTW restrictions
:::
::: {.content-visible when-profile="web"}
We need to start in the lower left and work our way up via a continuous path.
:::

::: {.fragment}
The basic restrictions on path are:
    
::: {.incremental}
* Monotonicity
  * The path should not go down or to the left.
* Continuity
  * No elements may be skipped in a sequence.
:::
:::

::: {.fragment}
This can be solved via dynamic programming. However, the algorithm is still quadratic in $n$.
:::

::: {.content-hidden when-profile="web"}
## Improving DTW 
:::

To reduce the computational complexity, we can put a restriction on the amount that the path can deviate from the diagonal.

The basic algorithm looks like this:

```
D[0, 0] = 0
for i in range(n):
  for j in range(m):
    D[i,j] = M[i,j] + 
             min( D[i-1, j],    # insertion
                  D[i, j-1],    # deletion
                  D[i-1, j-1] ) # match
```

Unfortunately, the algorithm is still quadratic in $n$ -- it is $\mathcal{O}(nm)$.

Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.

This is implemented by not allowing the path to pass through locations where $|i - j| > w$.

Then the algorithm is $\mathcal{O}(nw)$.

::: {.content-hidden when-profile="slides"}
## From Time series to Strings

A closely related idea concerns strings.

The key point is that, like time series, strings are __sequences__.

Given two strings, one way to define a 'distance' between them is:

* the minimum number of __edit operations__ that are needed to transform one string into the other.

Edit operations are insertion, deletion, and substitution of single characters.

This is called __edit distance__ or __Levenshtein distance.__

For example, given strings:

``s = VIVALASVEGAS``
    
and

``t = VIVADAVIS``


we would like to 

* compute the edit distance, and
* obtain the optimal __alignment__.


![](figs/viva-las-vegas.png){fig-align="center"}

[Source](http://medicalbioinformaticsgroup.de/downloads/lectures/Algorithmen_und_Datenstrukturen/WS15-16/aldabi_ws15-16_woche6.pdf)

A dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__

In bioinformatics this algorithm is called __"Smith-Waterman" sequence alignment.__
:::

::: {.content-visible when-profile="slides"}
## Recap

We covered the following topics

:::: {.incremental}
- reviewed representations of data,
- discussed metrics and norms,
- discussed similarity and dissimilarity functions,
- introduced time series, 
- feature engineering, and
- dynamic time warping.
:::: 
:::