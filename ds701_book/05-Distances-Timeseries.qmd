---
title: Distances and Timeseries
jupyter: python3
---

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)

Today we will start building some tools for making comparisons of data objects, with particular attention to timeseries.

Working with data, we can encounter a wide variety of different data objects:

::: {.incremental}
* Records of users
* Graphs
* Images
* Videos
* Text (webpages, books)
* Strings (DNA sequences)
* Timeseries
:::

::: {.fragment}
How can we compare them?
:::

## Feature space representation

Usually a data object consists of a set of attributes.

These are also commonly called __features.__

* ("J. Smith", 25, \$ 200,000)
* ("M. Jones", 47, \$ 45,000)

If all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.
 
* `(25, USD 200000)` $\rightarrow \begin{bmatrix}25\\200000\end{bmatrix}$.

Likewise If all features are binary then we can think of each data object as a binary vector in vector space.

The space is called __feature space.__

::: {.content-hidden when-profile="web"}
## One-hot encoding
:::
Vector spaces are such a useful tool that we often use them even for non-numeric data.

For example, consider a categorical variable that can be only one of "house", "tree", or "moon". For such a variable, we can use a __one-hot__ encoding.  

::: {.content-hidden when-profile="slides"}
We would encode as follows:
:::

::: {.incremental}
* `house`: $[1, 0, 0]$
* `tree`:  $[0, 1, 0]$
* `moon`:  $[0, 0, 1]$
:::

::: {.fragment}
So an encoding of `(25, USD 200000, 'house')` could be: 
$$\begin{bmatrix}25\\200000\\1\\0\\0\end{bmatrix}.$$
::: 

::: {.content-hidden when-profile="web"}
## Encodings
:::

We will see many other encodings that take non-numeric data and encode into vectors or matrices.

For example, there are vector or matrix encodings for:

::: {.incremental}
* Graphs
* Images
* Text 
:::

## Metrics

A metric is a function $d(x, y)$ that satisfies the following properties.

::: {.incremental}
* $d(i, j) = 0 \leftrightarrow  i == j \quad\quad$ (identity of indiscernables)
* $d(i, j) = d(j, i) \qquad\qquad~~$ (symmetry)
* $d(i, j)\leq d(i, h) + d(h, j) ~~$ (triangle inequality)
:::

::: {.fragment}
We can use a metric to determine how __similar__ or __dissimilar__ two objects are.

A dissimilarity function takes two objects as input, and returns a large value when the two objects are not very similar.
:::

::: {.content-hidden when-profile="web"}
## Triangle inequality
:::
![](figs/TriangleInequality.png)
    
[Source](https://commons.wikimedia.org/w/index.php?curid=26047092)

::: {.content-hidden when-profile="web"}
## Distances
:::

A metric is also commonly called a __distance__.

Sometimes we will use "distance" informally, i.e, to refer to a dissimilarity function even if we are not sure it is a metric.   

We'll try to say "dissimilarity" in those cases though.

::: {.fragment}
Why is it important or valuable for a dissimilarity to be a metric?
:::

::: {.content-hidden when-profile="web"}
## Distances continued
:::

::: {.incremental}
- The additional constraints allow us to reason about and more easily visualize the data.
- The main way this happens is through the triangle inequality.   
- The triangle inequality basically says, if two objects are "close" to another object, then they are "close" to each other.  
- This is not always the case for real data, but when it is true, it can really help.
- Definitions of distance or dissimilarity functions are usually
diferent for real, boolean, categorical, and ordinal
variables.
- Weights may be associated with diferent variables
based on applications and data semantics.
:::

## Matrix representation

Very often we will manage data conveniently in matrix form.

The standard way of doing this is:

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} x_{11} \\ \vdots \\ x_{i1} \\ \vdots \\ x_{m1} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1j} \\ \vdots \\ x_{ij} \\ \vdots \\ x_{mj} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1n} \\ \vdots \\ x_{in} \\ \vdots \\ x_{mn} \end{array}
\end{array}\right]}^{\mbox{$n$ features}} 
$$

Where we typically use symbols $m$ for number of rows (objects) and $n$ for number of columns (features).

::: {.content-hidden when-profile="web"}
## Matrix represntation of distances
:::

When we are working with distances, the matrix representation is:

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;
\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} 0  \\  d(1,2) \\ d(1,3) \\ \vdots \\ d(1,m)  \end{array} &
\begin{array}{c} \; \\  0      \\ d(2,3) \\ \vdots \\ d(2,m)  \end{array} &
\begin{array}{c} \; \\ \;      \\ 0      \\ \vdots \\ \cdots   \end{array} &
\begin{array}{c} \; \\ \;      \\ \;     \\[-1pt] \ddots \\[-1pt] \cdots   \end{array}  &
\begin{array}{c} \; \\ \;      \\ \;     \\ \;     \\[4pt] d(m, n) \end{array} &
\end{array}\right]}^{\mbox{$n$ data features}}
$$

## Norms

Assume some function $p(\mathbf{v})$ which measures the "size" of the vector $\mathbf{v}$.

$p()$ is called a __norm__ if:

::: {.incremental}
* $p(a\mathbf{v}) = |a|\; p(\mathbf{v})\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;$ (absolute scaling)
* $p(\mathbf{u} + \mathbf{v}) \leq p(\mathbf{u}) + p(\mathbf{v})\;\;\;\;\;\;\;\;\;\;$  (subadditivity)
* $p(\mathbf{v}) = 0 \leftrightarrow \mathbf{v}$ is the zero vector $\;\;\;\;$(separates points)
:::

::: {.fragment}
Norms are important for this reason, among others:
:::

::: {.fragment}
__Every norm defines a corresponding metric.__
:::

::: {.fragment}
That is, if $p()$ is a norm, then $d(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}-\mathbf{y})$ is a metric.
:::

## Useful Norms

A general class of norms are called __$\ell_p$__ norms, where $p \geq 1.$

$$ \Vert \mathbf{x} \Vert_p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}} $$ 

Notice that we use this notation for the $p$-norm of a vector $\mathbf{x}$:

$$ \Vert \mathbf{x} \Vert_p $$

The corresponding distance that an $\ell_p$ norm defines is called the _Minkowski distance._

$$ \Vert \mathbf{x} - \mathbf{y} \Vert_p = \left(\sum_{i=1}^d |x_i - y_i|^p\right)^{\frac{1}{p}} $$

::: {.content-hidden when-profile="web"}
## $\ell_2$ norm
:::
A special -- but very important -- case is the $\ell_2$ norm.

$$ \Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^d |x_i|^2} $$

We've already mentioned it: it is the __Euclidean__ norm.

The distance defined by the $\ell_2$ norm is the same as the Euclidean distance between the vectors.

$$ \Vert \mathbf{x} - \mathbf{y} \Vert_2  = \sqrt{\sum_{i=1}^d (x_i - y_i)^2} $$

::: {.content-hidden when-profile="web"}
## $\ell_1$ norm
:::

Another important special case is the $\ell_1$ norm.

$$ \Vert \mathbf{x} \Vert_1 = \sum_{i=1}^d |x_i| $$

This defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:

$$ \Vert \mathbf{x} - \mathbf{y} \Vert_1 = \sum_{i=1} |x_i - y_i| $$

![](figs/L05-manhattan-distance.png){fig-align="center"}
    
::: {.content-hidden when-profile="web"}
## $\ell_\infty$ norm
:::

If we take the limit of the $\ell_p$ norm as $p$ gets large we get the $\ell_\infty$ norm.  

The value of the $\ell_\infty$ norm is simply the __largest element__ in a vector.

::: {.fragment}
What is the metric that this norm induces?
:::

::: {.content-hidden when-profile="web"}
## $\ell_0$ norm
:::

Another related idea is the $\ell_0$ "norm," which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.

Note that this is __not__ a norm, but it gets called that anyway.   

This "norm" simply counts the number of __nonzero__ elements in a vector.

This is called the vector's __sparsity.__

::: {.content-hidden when-profile="web"}
## Visualizing norms
:::

Here is the notion of a "circle" under each of three norms.

That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.

![](figs/L5-Vector-Norms.png){fig-align="center"}

[Source](https://commons.wikimedia.org/w/index.php?curid=678101)


## Similarity and Dissimilarity

From linear algebra, we learn that the inner product of two vectors can be used to compute the __cosine of the angle__ between them:

$$ \cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x}^T\mathbf{y}}{\Vert\mathbf{x}\Vert \Vert\mathbf{y}\Vert} $$

Note that this value is __large__ when $\mathbf{x} \approx \mathbf{y}.$  So it is a __similarity__ function.

We often find that we have a similarity function $s$ and need to convert it to a dissimilarity function $d$.   

Two straightforward ways of doing that are:

$$d(x,y) = 1\,/\,s(x,y)$$

or 

$$d(x,y) = k - s(x,y)$$

...for some properly chosen $k$.

For cosine similarity, one often uses:
    
$$ d(\mathbf{x}, \mathbf{y}) = 1 - \cos(\mathbf{x}, \mathbf{y})$$

Note however that this is __not a metric!__

However if we recover the actual angle beween $\mathbf{x}$ and $\mathbf{y}$, that is a metric.

## Bit vectors and Sets

When working with bit vectors, the $\ell_1$ metric is commonly used and is called the __Hamming__ distance.

![](figs/L5-hamming-1.png){fig-align="center"}

This has a natural interpretation: "how well do the two vectors match?"

Or: "What is the smallest number of bit flips that will convert one vector into the other?"

::: {.content-hidden when-profile="web"}
## Hamming distance
:::

![](figs/L5-hamming-2.png){fig-align="center"}

In other cases, the Hamming distance is not a very appropriate metric.

::: {.content-hidden when-profile="web"}
## Hamming distance with sets
:::

Consider the case in which the bit vector is being used to represent a set.

In that case, Hamming distance measures the __size of the set difference.__

For example, consider two documents.  We will use bit vectors to represent the sets of words in each document.

* Case 1: both documents are large, almost identical, but differ in 10 words.
* Case 2: both documents are small, disjoint, have 5 words each.

The situation can be represented as this:

![](figs/L5-jaccard-1.png){fig-align="center"}

What matters is not just the size of the set difference, but the size of the intersection as well.

::: {.content-hidden when-profile="web"}
## Jaccard similarity
:::

This leads to the _Jaccard_ similarity:

$$J_{Sim}(\mathbf{x}, \mathbf{y}) = \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}$$

This takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$

In fact, this is a __metric!__:

$$J_{Dist}(\mathbf{x}, \mathbf{y}) = 1- \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}$$

::: {.content-hidden when-profile="web"}
## Jaccard similarity continued
:::

Consider our two cases:
    
Case 1: (very large almost identical documents)

![](figs/L5-jaccard-2.png){fig-align="center"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is almost 1.

::: {.content-hidden when-profile="web"}
## Jaccard similarity continued
:::

Case 2: (small disjoint documents)

![](figs/L5-jaccard-3.png){fig-align="center"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is 0.

## Time Series

A time series is a sequence of real numbers, representing the measurements of a real variable at (equal) time intervals.

::: {.incremental}
* Stock prices
* Volume of sales over time
* Daily temperature readings
:::

::: {.fragment}
A time series database is a large collection of time series.
:::

## Similarity of Time Series

How should we measure the "similarity" of two timeseries?

We will assume they are the same length.

Examples:

::: {.incremental}
* Find companies with similar stock price movements over a time interval
* Find similar DNA sequences
* Find users with similar credit usage patterns
:::

::: {.fragment}
Two Problems:

::: {.incremental}
1. Defining a meaningful similarity (or distance) function.
2. Finding an efficient algorithm to compute it.
:::
:::

## Norm-based Similarity Measures

We could just view each sequence as a vector.

Then we could use a $p$-norm, e.g., $\ell_1, \ell_2,$ or $\ell_p$ to measure similarity.

::: {.fragment}

Advantages:

::: {.incremental}    
1. Easy to compute - linear in the length of the time series (O(n)).
2. It is a metric.
:::
:::

::: {.content-hidden when-profile="web"}
## Norm-based Similarity Measures continued
:::

Disadvantage:

::: {.fragment}
1. May not be __meaningful!__

![](figs/L5-ts-euclidean.png){fig-align="center"}

We may believe that $\mathbf{ts1}$ and $\mathbf{ts2}$ are the most "similar" pair of timeseries.
:::

::: {.fragment}
However, according to Euclidean distance: 

$$ \Vert \mathbf{ts1} - \mathbf{ts2} \Vert_2 = 26.9,$$

while 

$$ \Vert \mathbf{ts1} - \mathbf{ts3} \Vert_2 = 23.2.$$
:::

::: {.fragment}
Not good!
:::

## Feature Engineering

In general, there may be different aspects of a timeseries that are important in different settings.

::: {.fragment}
The first step therefore is to ask yourself "what is important about timeseries in my application?"
:::

::: {.fragment}
This is an example of __feature engineering.__
:::

::: {.content-hidden when-profile="web"}
## Feature engineering continued
:::

In other words, feature engineering is the art of computing some derived measure from your data object that makes its important properties usable in a subsequent step.

::: {.fragment}
A reasonable approach may then to be:

::: {.incremental}    
* extract the relevant features
* use a simple method (eg, a norm) to define similarity over those features.
:::
:::

::: {.fragment}
In the case above, one might think of using 

::: {.incremental}
* Fourier coefficients (to capture periodicity)
* Histograms
* Or something else!
:::
:::

## Dynamic Time Warping

One case that arises often is something like the following:  "bump hunting"

![](figs/L5-DTW-1.png){fig-align="center"} 

Both timeseries have the same key characteristics: four bumps.

But a one-one match (ala Euclidean distance) will not detect the similarity.

(Be sure to think about why Euclidean distance will fail here.)

A solution to this is called __dynamic time warping.__


::: {.content-hidden when-profile="web"}
## Dynamic time warping continued
:::

The basic idea is to allow acceleration or deceleration of signals along the time dimension.

::: {.fragmen}
Classic applications:

::: {.incremental}
* Speech recognition
* Handwriting recognition
:::
:::

::: {.fragment}
Specifically: 

::: {.incremental}
* Consider $X = x_1, x_2, \dots, x_n$ and $Y = y_1, y_2, \dots, y_n$.
* We are allowed to extend each sequence by repeating elements to form $X'$ and $Y'$.
* We then calculate, eg, Euclidean distance between the extended sequnces $X'$ and $Y'$
:::
:::

::: {.content-hidden when-profile="web"}
## Dynamic time warping continued
:::

There is a simple way to visualize this algorithm.

Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure).

![](figs/L5-DTW-2.png){fig-align="center"}

$M$ measures the amount of error we get if we match $x_i$ with $y_j$. 

So we seek a __path through $M$ that minimizes the total error.__

::: {.content-hidden when-profile="web"}
## Dynamic time warping continued
:::

We need to start in the lower left and work our way up via a continuous path.

::: {.fragment}
Basic restrictions on path:
    
::: {.incremental}
* Montonicity
  * path should not go down or to the left
* Continuity
  * No elements may be skipped in sequence
:::
:::

::: {.fragment}
This can be solved via dynamic programming.  However, the algorithm is still quadratic in $n$.
:::

::: {.content-hidden when-profile="web"}
## Dynamic time warping continued
:::

Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.

The basic algorithm looks like this:

```
D[0, 0] = 0
for i in range(n):
  for j in range(m):
    D[i,j] = M[i,j] + 
             min( D[i-1, j],    # insertion
                  D[i, j-1],    # deletion
                  D[i-1, j-1] ) # match
```

Unfortunately, the algorithm is still quadratic in $n$ -- it is $\mathcal{O}(nm)$.

Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.

This is implemented by not allowing the path to pass through locations where $|i - j| > w$.

Then the algorithm is $\mathcal{O}(nw)$.

## From Timeseries to Strings

A closely related idea concerns strings.

The key point is that, like timeseries, strings are __sequences__.

Given two strings, one way to define a 'distance' between them is:

* the minimum number of __edit operations__ that are needed to transform one string into the other.

Edit operations are insertion, deletion, and substitution of single characters.

This is called __edit distance__ or __Levenshtein distance.__

::: {.content-hidden when-profile="web"}
## Comparing strings
:::

For example, given strings:

``s = VIVALASVEGAS``
    
and

``t = VIVADAVIS``

::: {.fragment}

we would like to 

::: {.incremental}
* compute the edit distance, and
* obtain the optimal __alignment__
:::
:::

::: {.fragment}
![](figs/viva-las-vegas.png){fig-align="center"}

[Source](http://medicalbioinformaticsgroup.de/downloads/lectures/Algorithmen_und_Datenstrukturen/WS15-16/aldabi_ws15-16_woche6.pdf)
:::

::: {.content-hidden when-profile="web"}
## Comparing strings continued
:::

A dynamic programming algorithm can also be used to find this distance, 

and it is __very similar to dynamic time-warping.__

In bioinformatics this algorithm is called __"Smith-Waterman" sequence alignment.__
