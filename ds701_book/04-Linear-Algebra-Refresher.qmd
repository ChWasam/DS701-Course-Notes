---
title: Linear Algebra Refresher
jupyter: python3
---

Linear alegbra is the branch of mathematics involving vectors and matrices. In particular, how vectors are transformed.  Knowledge of linear algebra is essential in data science. This is because linear algebra underpins the representation of data and the algorithms that transform this data.

This lecture is a review of some aspects of linear algebra that are important for data science. Given the prerequisites for this course, I assume that you previously learned this material. 

The goal of this lecture is to refresh the following topics:

- vectors,
- matrices,
- operations with vectors and matrices,
- eigenvectors and eigenvalues,
- linear systems and least squares,
- matrix factorizations.

Below is a list of very useful resrouces for learning about linear algebra:

- __Linear Algebra and Its Applications (6th edition)__, David C. Lay, Judi J. McDonald, and Steven R. Lay, Pearson, 2021,
- __Introduction to Linear Algebra (6th edition)__, Gilbert Strang, Wellesley-Cambridge Press, 2023,
- __Linear Algebra and Learning from Data__, Gilbert Strang, Wellesley-Cambridge Press, 2019,
- __Numerical Linear Algebra__, Lloyn N. Trefethen and David Bau, SIAM, 1997.


## Vectors

A vector of length $n$, $\mathbf{x}\in\mathbb{R}^{n}$, is a 1-dimensional (1-D) array of real numbers

$$
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}.
$$


When discussing vectors we will only consider column vectors. A row vector can always be obtained from a column vector via transposition

$$
\mathbf{x}^{T} = [x_1, x_2, \ldots, x_n].
$$


Vectors in $\mathbb{R}^{2}$ can be visualized as points in a 2-D plane, whereas vectors in $\mathbb{R}^{3}$ can be visualized as points in a 3-D space.

Let 

$$\mathbf{x}=\begin{bmatrix} 2 \\ 2 \end{bmatrix},~
\mathbf{y} = \begin{bmatrix} 3 \\ -1 \end{bmatrix},~
\mathbf{z} = \begin{bmatrix} -2 \\ -1 \end{bmatrix}.
$$

These vectors are illustrated in @fig-vector-viz.

```{python}
#| label: fig-vector-viz
#| fig-cap: "Illustration of vectors"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
x = np.array([2, 2])
y = np.array([3, -1])
z = np.array([-2, -1])
V = np.array([x, y, z])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['r', 'b', 'g'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-6, 6])
ax.set_ylim([-2, 4])
ax.text(3.3, -1.1, '$(3,-1)$', size=16)
ax.text(2.3, 1.9, '$(2,2)$', size=16)
ax.text(-3.7, -1.3, '$(-2,-1)$', size=16);
ax.grid()
plt.show()
```

Recall the following definitions.

 1. Scalar multiplication: Let $c\in\mathbb{R}$, $\mathbf{x}\in\mathbb{R}^{n}$, then $$c\mathbf{x} = \begin{bmatrix} cx_1 \\ cx_2 \\ \vdots \\ cx_n \end{bmatrix}.$$
 1. Vector addition: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ then
$$ \mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}.$$
1. Dot product: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ then the dot product is defined as
$$ \mathbf{u}\cdot\mathbf{v} = \sum_{i=0}^n u_i v_i.$$
1. Vector 2-norm: The 2-norm of a vector $\mathbf{v}\in\mathbb{R}^{n}$ is defined as
    $$\Vert \mathbf{v}\Vert_2 = \sqrt{\mathbf{v}\cdot\mathbf{v}} = \sqrt{\sum_{i=1}^n v_i^2}.$$ 
    This norm is referred to as the $\ell_2$ norm. In these notes, the notation $\Vert \mathbf{v} \Vert$, indicates the 2-norm.
1. A unit vector $\mathbf{v}$ is a vector such that $\Vert \mathbf{v} \Vert_2 = 1$. All vectors of the form $\frac{\mathbf{v}}{\Vert \mathbf{v} \Vert_2 }$ are unit vectors.
1. Distance: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$, the distance between $\mathbf{u}$ and $\mathbf{v}$ is
$$ \Vert \mathbf{u} - \mathbf{v} \Vert_2.$$
1. Orthogonality: Two vectors $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ are orthogonal if and only if $\mathbf{u}\cdot\mathbf{v}=0$. 
1. Angle between vectors:  Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$, the angle between these vectors is $$\cos{\theta} = \frac{\mathbf{u}\cdot\mathbf{v}}{\Vert \mathbf{u}\Vert_2 \Vert\mathbf{v}\Vert_2}.$$
1. A set of $n$ vectors $\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\in\mathbf{R}^n$ is linearly dependent if there exists scalars $c_1,\ldots, c_n$ not all zero such that 
    $$
    \sum_{i=1}^{n} a_i \mathbf{v}_i = 0.
    $$

    The vectors $\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}$ are linearly independent if they are not linearly independent, i.e., the equation
    $$
    a_1 \mathbf{v}_1 + \cdots + a_n \mathbf{v}_n = 0,
    $$
    is only satisfied if $a_i=0$ for $i=1, \ldots,n$.

### Visualizing vector operations

For helpful visualizations of the above defintions, we'll restrict ourselves to $\mathbb{R}^{2}.$

#### Scalar multiplication

Multiplication by a scalar $c\in\mathbb{R}$. For $c>1$ the vector is lengthened. For $0<c<1$ the vector shrinks. If we negate $c$ the direction of the vector is flipped 180 degrees. This is shown in @fig-vector-scaling. In this figure we show the vector $\mathbf{x} = [2, 2]$ multiplied by the scalar value $c=2$.

```{python}
#| label: fig-vector-scaling
#| fig-cap: "Scalar multiplication of a vector"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
x = np.array([2, 2])
y = np.array([4, 4])
V = np.array([x, y])
origin = np.array([[0, 0], [0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['r', 'b'], 
           angles='xy', 
           scale_units='xy', 
           scale=1,
           alpha= 0.5)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 5])
ax.text(2.3, 1.9, '$x$', size=16)
ax.text(4.3, 3.9, '$cx$', size=16)
ax.grid()
plt.show()
```

#### Vector addition

We plot the sum of $\mathbf{u} = [1, 2]$ and $\mathbf{v} = [4, 1]$ in @fig-vector-addition. The sum $\mathbf{u} + \mathbf{v} = [5, 3]$ is obtained by placing the tip of one vector to the tail of the other vector. The sum is the vector that connects the unconnected tip to the unconnected tail. 

```{python}
#| label: fig-vector-addition
#| fig-cap: "Vector addition"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
u = np.array([1, 2])
v = np.array([4, 1])
w = np.array([5, 3])
V = np.array([u, v, w])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['b', 'b', 'r'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 4])
ax.text(1.3, 1.9, '$u$', size=16)
ax.text(4.3, 1.2, '$v$', size=16)
ax.text(5.3, 2.9, '$u+v$', size=16)
plt.plot([1, 5], [2, 3], 'g--')
plt.plot([4, 5], [1, 3], 'g--')
ax.grid()
plt.show()
```

#### Dot prodcut

The dot product of two vectors $\mathbf{u}, \mathbf{v}$ can be used to project $\mathbf{u}$ onto $\mathbf{v}$. This is illustrated in Figure @fig-dot-product.

```{python}
#| label: fig-dot-product
#| fig-cap: "Dot product"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
u = np.array([1, 2])
v = np.array([4, 1])
w = np.dot(u, v) * v / np.dot(v, v)
V = np.array([u, v, w])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['b', 'b', 'r'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 4])
ax.text(1.3, 1.9, '$u$', size=16)
ax.text(4.3, 1.2, '$v$', size=16)
ax.text(0.4, -0.3, r'$\frac{u\cdot v}{\Vert v \Vert}$', size=16)
plt.plot([u[0], w[0]], [u[1], w[1]], 'g--')
# plt.plot([4, 5], [1, 3], 'g--')
ax.grid()
plt.show()
```

Observe that a right angle forms between the vectors $\mathbf{u}$ and $\mathbf{v}$ when $\mathbf{u}\cdot \mathbf{v} = 0$. 

## Matrices

A matrix $A\in\mathbb{R}^{m\times n}$ is a 2-D array of numbers

$$
A = 
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} &a_{m2} & \cdots & a_{mn} \\
\end{bmatrix},
$$

with $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.

Similar to vectors, we can multiply matrices by scalar values and add matrices of the same dimension, i.e.,

1. Let $c\in\mathbb{R}$ and $A\in\mathbb{R}^{m\times n}$, then
$$
cA =
\begin{bmatrix}
ca_{11} & ca_{12} & \cdots & ca_{1n} \\
ca_{21} & ca_{22} & \cdots & ca_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
ca_{m1} & ca_{m2} & \cdots & ca_{mn} \\
\end{bmatrix}.
$$
1. Let $A, B\in\mathbb{R}^{m\times n}$, then 
$$
A + B =
\begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn} \\
\end{bmatrix}
$$

The transpose $A^{T}$ is defined as

$$
A^{T} = 
\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\
a_{12} & a_{22} & \cdots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} &a_{2n} & \cdots & a_{nm} \\
\end{bmatrix}.
$$

The transpose turns columns of the matrix into rows (equivalently rows into columns). A square matrix is called symmetric if $A=A^{T}$.

## Matrix multiplication

We discuss the following two important matrix multiplication operations

- matrix-vector multiplication,
- matrix-matrix multiplication.

### Matrix-vector multiplication

Let $A\in\mathbb{R}^{m\times n}$ and $\mathbf{x}\in\mathbb{R}^{n}$, then $A\mathbf{x}\in\mathbb{R}^{m}$ is defined as 

$$
A\mathbf{x} = 
\begin{bmatrix}
x_1a_{11} + x_2 a_{12} + \cdots + x_na_{1n} \\
x_1a_{21} + x_2 a_{22} + \cdots + x_na_{2n} \\
\vdots \\
x_1a_{m1} + x_2 a_{m2} + \cdots + x_na_{mn} \\
\end{bmatrix}.
$$

Equivalently, this means that $A\mathbf{x}$ is a linear combination of the columns of $A$, i.e.,

$$
A\mathbf{x} = 
x_1 \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}  \end{bmatrix} 
+ 
x_2  \begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}  \end{bmatrix}
+
\cdots
+
x_n \begin{bmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}  \end{bmatrix}.
$$

Observe that the matrix $A$ is a linear transformation that maps vectors in $R^{n}$ to $\mathbb{R}^{m}$.

### Matrix-matrix multiplication

Let $A\in\mathbb{R}^{m\times n}$ and $B\in\mathbb{R}^{n\times p}$, then the elements of $C=AB\in\mathbb{R}^{m\times p}$ are

$$
c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj},
$$

for $i=1,\ldots, m$ and $j=1, \ldots, p$.

## Important matrices

We introduce notation for some commonly used and important matrices.

The $n \times n$ identity matrix is

$$
I = 
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{bmatrix}.
$$.

For every $A\in\mathbb{R}^{n\times n}$, then $AI = IA$. 

The inverse $A^{-1}\in\mathbb{R}^{n\times n}$ is defined as the matrix for which $AA^{-1} = A^{-1}A = I$. When $A^{-1}$ exists the matrix is said to be invertible. Note that $(AB)^{-1} = B^{-1}A^{-1}$ for invertible $B\in\mathbb{R}^{n\times n}$.

A diagonal matrix $D\in\mathbb{R}^{n\times n}$ has entries $d_{ij}=0$ if $i\neq j$, i.e.,

$$
D =
\begin{bmatrix}
d_{11} & 0 & \cdots & 0 \\
0 & d_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_{nn} \\
\end{bmatrix}.
$$

A square matrix $Q\in\mathbb{R}^{n}$ is orthogonal if $QQ^{T}=Q^{T}Q=I$. In particular, the inverse of an orthogonal matrix is it's transpose.

A lower triangular matrix $L\in\mathbb{R}^{n\times n}$ is a matrix where all the entries above the main diagonal are zero

$$
L =
\begin{bmatrix}
l_{11} & 0 & \cdots & 0 \\
l_{12} & l_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{1n} & l_{n2} & \cdots & l_{nn} \\
\end{bmatrix}.
$$

An upper triangular matrix $U\in\mathbb{R}^{n\times n}$ is a matrix where all the entries below the main diagonal are zero

$$
U = 
\begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{nn} \\
\end{bmatrix}.
$$

The inverse of a lower triangular matrix is itself a lower triangular matrix. This is also true for upper triangular matrices, i.e., the inverse is also upper triangular.

## Eigenvalues and eigenvectors

An eigenvector of an $n\times n$ matrix $A$ is a nonzero vector $\mathbf{x}$ such that $A\mathbf{x} = \lambda\mathbf{x}$ for some scalar $\lambda.$  The scalar $\lambda$ is called an eigenvalue.

An $n \times n$ matrix has at most $n$ distinct eigenvectors and at most $n$ distinct eigenvalues.

## Matrix decompositions

We introduce here important matrix decompositions. These are useful in solving linear equations. Furthermore they play an important role in various data science applications.

### LU factorization

An LU decomposition of a square matrix $A\in\mathbb{R}^{n\times n}$ is a factorization of $A$ into a product of matrices

$$ 
A = LU,
$$

where $L$ is a lower triangular square matrix and $U$ is an triangular square matrix. For example, when $n=3$, we have

$$ 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{bmatrix}
=
\begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0\\
l_{31} & l_{32} & a_{33} \\
\end{bmatrix}
\begin{bmatrix}
u_{11} & u_{12} & u_{13} \\
0 & u_{22} & u_{23} \\
0 & 0 & u_{33} \\
\end{bmatrix}
$$


### QR decomposition

A QR decomposition of a square matrix $A\in\mathbb{R}^{n\times n}$ is a factorization of $A$ into a product of matrices

$$
A=QR,
$$
where $Q$ is an orthogonal square matrix and $R$ is an upper-triangular square matrix.

### Eigendecomposition

Let $A\in\mathbb{R}^{n}$ have n linearly independent eigenvectors $\mathbf{x}_i$ for $i=1,\ldots, n$, then $A$ can be factorized as

$$
A = X\Lambda X^{-1},
$$
where $X$ is a matrix of the eigenvectors, and 

$$
\Lambda =
\begin{bmatrix}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{n}  \\
\end{bmatrix},
$$
is a diagonal matrix of the eigenvalues. 

In this case, the matrix $A$ is said to be diagonalizable.

A special case occurs when $A$ is symmetric. Recall that a matrix is symmetric when $A = A^T.$

In this case, it can be shown that the eigenvectors of $A$ are all mutually orthogonal. Consequently, $X^{-1} = X^{T}$ and we can decompose $A$ as:

$$A = XDX^T.$$

This amazing fact is known as the spectral theorem and this decomposition of $A$ is its spectral decomposition. The eigenvalues of a matrix are also called its spectrum.

### Singular value decomposition

Let $A\in\mathbb{R}^{m\times n}$ with $m>n$, then $A$ admits a decomposition

$$
A = U\Sigma V^{T}.
$$
The matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ are orthogonal. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors.

The matrix $\Sigma\in\mathbb{R}^{m\times n}$ is a diagonal matrix of the form

$$
\Sigma = 
\begin{bmatrix}
\sigma_{11} & 0 & \cdots & 0 \\
0 & \sigma_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{mn} \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 \\
\end{bmatrix}.
$$
The values $\sigma_{ij}$ are the singular values of the matrix $A$. Amazingly, it can be proven that every matrix $A\in\mathbb{R}^{m\times n}$ has a singular value decomposition.

## Linear systems of equations

A system of $m$ linear equations in $n$ unknowns can be written as

$$
\begin{align*}
a_{11} x_{1} + a_{12} x_{2} + \cdots + a_{1n} x_{n} &= b_1 \\
a_{21} x_{1} + a_{22} x_{2} + \cdots + a_{2n} x_{n} &= b_2 \\
\vdots  \qquad \qquad \quad \\
a_{m1} x_{1} + a_{m2} x_{2} + \cdots + a_{mn} x_{n} &= b_m,\\
\end{align*}
$$
is simply the matrix vector equation

$$
A\mathbf{x}=\mathbf{b}.
$$

A linear system of equations may have:

- infinitely many solutions,
- a unique solution,
- no solutions.

When $m > n$, the system is said to be overdetermined and in general has infinitely many solutions. When $m<n$ the system is underdetermined and in general has no solution. For the case when $m=n$ and the matrix has $n$ linearly dependent columns, the solution is always unique.

For an invertible square matrix $A\mathbf{x}=\mathbf{b}$, the solution is always $\mathbf{x}=A^{-1}\mathbf{b}$. 

Matrix factorizations to help us solve linear systems of equations 
For square matrices, we can use 

We can use matrix factorizations to help us solve a linear system of equation. We demonstrate how to do this with the LU decomposition. Observe that $A\mathbf{x} = LU\mathbf{x} = \mathbf{b}$. Then 

$$
\mathbf{x} = U^{-1}L^{-1}\mathbf{b}.
$$ 

The process of inverting $L$ and $U$ is called backward and forward substitution.


## Least squares

In data science it is often the case that we have to solve the linear system 

$$ A \mathbf{x} = \mathbf{b},$$ 

This problem may have no solution -- perhaps due to noise or measurement error.

In such a case, we look for a vector $\mathbf{x}$ such that $A\mathbf{x}$ is a good approximation to $\mathbf{b}.$

The quality of the approximation can be measured using the distance from $A\mathbf{x}$ to $\mathbf{b},$ i.e.,

$$\Vert A\mathbf{x} - \mathbf{b}\Vert_2.$$

The general least-squares problem is given $A\in\mathbb{R}^{m\times n}$ and and $\mathbf{b}\in\mathbb{R}^{m}$, find a vector $\hat{\mathbf{x}}\in\mathbb{R}^{n}$ such that $\Vert A\mathbf{x}-\mathbf{b}\Vert_2$ is minimized, i.e. 

$$\hat{\mathbf{x}} = \arg\min_\mathbf{x} \Vert A\mathbf{x} - \mathbf{b}\Vert.$$

This emphasizes the fact that the least squares problem is a minimization problem. Minimizations problems are an example of a broad class of problems called _optimization_ problems. In optimization problems we attempt to find an optimal solution that minimizes (or maximizes) a set particular set of equations (and possibly constraints). 

We can connect the above minimization of the distance between vectors to the minimization of the sum of squared errors. Let $\mathbf{y} = A\mathbf{x}$ and observe that

$$\Vert A\mathbf{x}-\mathbf{b}\Vert_2^2 = \Vert \mathbf{y}-\mathbf{b}\Vert_2^2 =  \sum_i (y_i-b_i)^2.$$

The above expression is the sum of squared errors. In statistics, the $y_i$ are the estimated values and the $b_i$ are the measured values. This is the most common measure of error used in statistics and is a key principle. 

Minimizing the length of $A\mathbf{x} - \mathbf{b}$ is equivalent to minimizing the sum of the squared errors. 

We can find $\hat{\mathbf{x}}$ using either 

* geometric arguments based on projections of the vector $\mathbf{b}$,
* by calculus (taking the derivative of the right-hand-side expression above and setting it equal to zero).

Either way, we obtain the result that $\hat{\mathbf{x}}$ is the solution of:
    
$$A^TA\mathbf{x} = A^T\mathbf{b}.$$

This system of equations is called the normal equations.

We can prove that these equations always have at least one solution. 

When $A^TA$ is invertible, the system is said to be overdetermined.  This means that there is a unique solution

$$\hat{\mathbf{x}} = (A^TA)^{-1}A^T\mathbf{b}.$$

Be aware that computing the solution using $(A^TA)^{-1}A^T$ can be numerically unstable. A more stable method is to use the QR decomposition of $A$, i.e., $\hat{\mathbf{x}} = R^{-1}Q^T\mathbf{b}$.
