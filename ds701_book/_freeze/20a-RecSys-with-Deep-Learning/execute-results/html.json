{
  "hash": "c4b13f5f6a8a267425829ce03beca803",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Recommendation Systems Part II -- Deep Learning Based\nbibliography: references.bib\njupyter: python3\nnocite: |\n    @ricci2022recommender\n---\n\n\n# Deep Learning for Recommender Systems\n\nBased on [@zhang2022deep].\n\n## Introduction\n- **Deep Learning in Recommender Systems**:\n  - Revolutionized AI applications across fields like computer vision and NLP.\n  - Reduces feature engineering effort and supports diverse data (e.g., text, images).\n  - Enhances tasks such as cold-start problems, temporal dynamics, and explainability.\n\n## Key Techniques\n1. **Multi-layer Perceptrons (MLPs)**:\n   - Flexible, hierarchical networks for feature interaction.\n   - Universal approximators (@fig-dl-recsys-mlp).\n2. **Convolutional Neural Networks (CNNs)**:\n   - Efficiently capture spatial patterns in grid-like data (@fig-dl-recsys-cnn).\n3. **Recurrent Neural Networks (RNNs)**:\n   - Models sequential data with memory states (@fig-dl-recsys-rnn).\n4. **Graph Neural Networks (GNNs)**:\n   - Handles graph-structured data like social and knowledge graphs.\n5. **Autoencoders and GANs**:\n   - For representation learning and data generation.\n\n![Multi-layer Perceptrons](figs/RecSys-figs/dl-recsys-mlp.png){width=30% fig-align=\"center\"  #fig-dl-recsys-mlp}\n\n![Convolutional Neural Networks](figs/RecSys-figs/dl-recsys-cnn.png){width=30% fig-align=\"center\" #fig-dl-recsys-cnn}\n\n![Recurrent Neural Networks](figs/RecSys-figs/dl-recsys-rnn.png){width=30% fig-align=\"center\" #fig-dl-recsys-rnn}\n\n---\n\n## Challenges in Recommender Systems\n### Interaction Modeling\n- Captures relationships in sparse user-item matrices.\n- Approaches:\n  - **NeuMF**: Replaces dot product with MLPs.\n  - **Outer Product + CNNs**: Higher-order correlations (Figure 6).\n\n---\n\n### User Modeling\n1. **Temporal Dynamics**:\n   - Sequence-aware recommendations using RNNs, CNNs, and attention mechanisms (Figure 8).\n2. **Diverse Interests**:\n   - Models multiple user preferences via clustering and disentanglement.\n\n---\n\n## Content Representation Learning\n1. **Text Features**:\n   - Leverages reviews and descriptions using CNNs, RNNs, and attention mechanisms.\n2. **Image Features**:\n   - Integrates CNN-extracted visual data for applications like fashion and social media.\n3. **Video/Audio Features**:\n   - Processes multimedia content for personalized music and video recommendations.\n\n---\n\n## Advanced Applications\n### Graph-Structured Data\n- Incorporates graphs like user-item networks for collaborative filtering (Figure 10).\n- Examples:\n  - **Session-based Recommendations**: Sequence learning from click patterns.\n  - **Knowledge Graphs**: Path-based reasoning for explainability.\n\n### Cold-Start Recommendations\n- Tackles sparse data with side information and meta-learning.\n- GNN-based approaches predict embeddings for new users/items.\n\n---\n\n## Beyond Accuracy\n### Explainability\n- Enhances transparency and trust:\n  - Attention mechanisms highlight critical features.\n  - Knowledge graph paths provide reasoning for recommendations.\n\n### Robustness\n- Defends against adversarial attacks with perturbation-based training.\n\n---\n\n## Applications of Deep Learning in Recommendation\n1. **E-commerce**:\n   - Amazon, eBay, Alibaba use deep learning for personalized shopping.\n2. **Entertainment**:\n   - YouTube employs candidate generation and ranking modules.\n3. **News**:\n   - Self-attention mechanisms capture user preferences across multiple views.\n4. **Point-of-Interest**:\n   - Combines visual and textual data for location-based recommendations.\n\n---\n\n## Conclusion\n- **Deep learning transforms recommender systems**:\n  - Combines memorization and generalization.\n  - Supports diverse data types and complex challenges.\n- Future directions include improving scalability and real-time adaptability.\n\nThis presentation references key figures and tables from the document and is paced for a 15-minute delivery. Let me know if further adjustments are needed!\n\n\n# Wide and Deep Learning for Recommender Systems\n\nBased on [@cheng2016wide].\n\n## Introduction\n- **Problem**: Balancing memorization and generalization in recommender systems.\n  - **Memorization**: Learns frequent co-occurrences of features for relevant recommendations.\n  - **Generalization**: Predicts unseen feature combinations for diverse recommendations.\n- **Wide & Deep Learning Framework**:\n  - Combines linear models (memorization) and neural networks (generalization).\n  - Evaluated on Google Play, with over 1 billion users and 1 million apps.\n- **Key Results**:\n  - Significant improvements in app acquisitions.\n  - Open-sourced implementation in TensorFlow.\n\n---\n\n## Recommender System Overview\n- Workflow (Figure 2):\n  - **Query**: User and contextual features (e.g., demographics, app usage).\n  - **Retrieval**: Filters 100 items based on relevance.\n  - **Ranking**: Scores and ranks items using Wide & Deep Learning.\n- Challenges:\n  - High throughput with low latency (e.g., scoring over 10 million apps per second).\n\n---\n\n## Wide & Deep Learning Framework\n### Wide Component\n- Linear model (Figure 1, left):\n  - Cross-product transformations capture interactions (e.g., \"gender=female\" AND \"language=en\").\n  - Effective for memorization but limited in generalization.\n\n### Deep Component\n- Feed-forward neural network (Figure 1, right):\n  - Converts sparse categorical features into dense embeddings.\n  - Layers compute activations using ReLU functions.\n  - Learns complex, nonlinear feature interactions.\n\n---\n\n### Joint Training\n- Combines wide and deep components (Figure 1, center):\n  - Jointly optimized via a shared logistic loss function.\n  - Uses:\n    - **FTRL** optimizer for wide part.\n    - **AdaGrad** for deep part.\n- Distinction:\n  - Joint training integrates components during training, unlike ensembles.\n\n---\n\n## System Implementation\n### Data Generation\n- **Training Data**:\n  - Generated from user-app interactions.\n  - Labels: 1 for app installs, 0 otherwise.\n- **Feature Engineering**:\n  - Maps categorical strings to integer IDs (vocabulary generation).\n  - Normalizes continuous features using quantile-based scaling.\n\n### Model Training\n- Structure (Figure 4):\n  - Cross-product transformations for wide component.\n  - Dense embeddings (32 dimensions each) for deep component.\n  - Three ReLU layers process embeddings and continuous features.\n- **Warm-Starting**:\n  - Retrains models incrementally using weights from previous models.\n\n---\n\n### Model Serving\n- Scoring:\n  - Scores candidate apps using forward inference on Wide & Deep models.\n- Optimization:\n  - Multithreading reduces latency from 31 ms to 14 ms (Table 2).\n\n---\n\n## Experiment Results\n### App Acquisitions\n- A/B Testing (Table 1):\n  - **Wide-only**: Baseline model.\n  - **Deep-only**: +2.9% acquisition rate.\n  - **Wide & Deep**: +3.9% acquisition rate over baseline.\n- Insights:\n  - Joint training enables exploratory recommendations for new user responses.\n\n### Serving Performance\n- At peak, servers score over 10 million apps/second.\n- Optimized serving reduces latency significantly.\n\n---\n\n## Related Work\n- **Factorization Machines**:\n  - Generalize linear models but lack nonlinear interaction modeling.\n- **Collaborative Deep Learning**:\n  - Combines deep learning with collaborative filtering.\n- Wide & Deep innovates by integrating linear models with deep networks.\n\n---\n\n## Conclusion\n- **Key Contributions**:\n  - Combines memorization (wide) and generalization (deep) in a single model.\n  - Scalable and effective for massive datasets (e.g., Google Play).\n- **Impact**:\n  - Open-source implementation facilitates adoption in diverse applications.\n\nThis structure references critical figures (Figure 1, 2, 4) and tables (Table 1, 2) and is timed for a detailed explanation within 25 minutes. Let me know if adjustments are needed!\n\n\n# Deep Learning Recommender Model\n\nBased on [@naumov2019deep].\n\n## Introduction\n- Personalization and recommendation systems:\n  - Widely used in industries like e-commerce, entertainment, and advertising.\n  - Deep Learning Recommendation Model (DLRM) introduced for categorical data handling.\n- **Key Features**:\n  - Embeddings for categorical data.\n  - Multi-layer perceptrons (MLPs) for dense data processing.\n  - Combines statistical techniques like matrix factorization and factorization machines.\n\n## DLRM Architecture\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- Components (@fig-dlrm-model):\n  1. **Embeddings**: Dense representations for categorical data.\n  2. **Bottom MLP**: Transforms dense continuous features.\n  3. **Feature Interaction**: Dot-product of embeddings and dense features.\n  4. **Top MLP**: Processes interactions and outputs probabilities.\n\n:::\n::: {.column width=\"50%\"}\n\n![DLRM Architecture](figs/RecSys-figs/dlrm-model.png){width=80% fig-align=\"center\" #fig-dlrm-model}\n\n:::\n::::\n\n## Embeddings and Feature Interactions\n1. **Embeddings**:\n   - Maps categorical inputs to latent factor space.\n   - Multi-hot vectors allow weighted combinations (Equation 2).\n2. **Feature Interaction**:\n   - Second-order interactions modeled via dot-products.\n   - Mimics Factorization Machines for efficiency (Equation 4).\n\n---\n\n## Model Training and Parallelism\n- **Training Challenges**:\n  - Large embeddings exceed single-device memory.\n  - Requires efficient parallelization of computations.\n- **Parallelism Strategy**:\n  - **Model Parallelism**: Distributes embeddings across devices.\n  - **Data Parallelism**: Replicates MLPs for concurrent mini-batch processing.\n  - Butterfly shuffle for all-to-all communication (Figure 2).\n\n---\n\n## Data Handling\n1. **Random and Synthetic Data**:\n   - Facilitates system testing and preserves data privacy.\n   - Techniques for generating synthetic categorical data (Figure 3).\n2. **Public Datasets**:\n   - Criteo AI Labs Ad Kaggle Dataset.\n   - Used for evaluating click-through rate (CTR) prediction models.\n\n---\n\n## Experiments: Accuracy and Performance\n1. **Accuracy**:\n   - Evaluated on Criteo dataset (Figure 5).\n   - Compared with Deep & Cross Network (DCN).\n   - DLRM shows superior training and validation accuracy with both SGD and Adagrad.\n2. **Performance Profiling**:\n   - Tested on Big Basin AI platform (Figure 4).\n   - GPU significantly outperforms CPU, particularly in MLP computations (Figure 6).\n\n---\n\n## Comparison with Prior Models\n- DLRM vs Other Networks:\n  - Simplified interactions reduce dimensionality.\n  - Focuses on second-order interactions for computational efficiency.\n  - Outperforms alternatives like Wide & Deep, DeepFM, and xDeepFM.\n\n---\n\n## Conclusion\n- **Key Takeaways**:\n  - DLRM effectively combines embeddings, MLPs, and interaction layers for personalization tasks.\n  - Offers a scalable solution for large-scale recommendation systems.\n  - Open-source implementation fosters further research and system design.\n- **Future Directions**:\n  - Optimization of communication primitives.\n  - Exploring higher-order interactions with minimal computational costs.\n\n\n# Recap and References\n\n\n## Recap\n\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "20a-RecSys-with-Deep-Learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}