{
<<<<<<< HEAD
  "hash": "ddddb5d8b9db670803fa09fd3db2bca8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: SVD - Low Rank Approximations\njupyter: python3\n---\n\n\n# Low Rank Approximations\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/10-Low-Rank-and-SVD.ipynb)\n\nWe now consider applications of the Singular Value Decomposition (SVD).\n\n> SVD is \"the Swiss Army Knife of Numerical Linear Algebra.”\n\nDianne O’Leary, MMDS ’06 (Workshop on Algorithms for Modern Massive Data Sets)\n\n---\n\nWe will see how the SVD is used for\n\n:::: {.incremental}\n- low rank approximations\n- dimensionality reduction\n::::\n\n\n## Singular Value Decomposition\n\nThe SVD of a matrix $A\\in\\mathbb{R}^{m\\times n}$ (where $m>n$) is\n\n$$\nA = U\\Sigma V^{T},\n$$\n\nwhere\n\n:::: {.incremental}\n- $U$ has dimension $m\\times n$. The columns of $U$ are orthogonal. The columns of $U$ are the left singular vectors.\n- $\\Sigma$ has dimension $n\\times n$. The only non-zero values are on the main diagonal and they are nonnegative real numbers  $\\sigma_1\\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_n$.\n- $V$ has dimension $n \\times n$. The columns of $V$ are orthogonal. The columns of $V$ are the right singular vectors.\n::::\n\n---\n\n::: {#054f02c1 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Draw matrix A\nrect_A = patches.Rectangle((0, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_A)\nax.text(1, 1.5, r'$A$', fontsize=20, ha='center', va='center')\nax.text(1, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw equal sign\nax.text(2.5, 1.5, r'$=$', fontsize=20, ha='center', va='center')\n\n# Draw matrix U\nrect_U = patches.Rectangle((3, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_U)\nax.text(4, 1.5, r'$U$', fontsize=20, ha='center', va='center')\nax.text(4, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw Sigma\nrect_Sigma = patches.Rectangle((5.5, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_Sigma)\nax.text(6.5, 2, r'$\\Sigma$', fontsize=20, ha='center', va='center')\nax.text(6.5, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw matrix V^T with the same dimensions as Sigma\nrect_VT = patches.Rectangle((8, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_VT)\nax.text(9, 2, r'$V^T$', fontsize=20, ha='center', va='center')\nax.text(9, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Set limits and remove axes\nax.set_xlim(-1, 11)\nax.set_ylim(-2, 4)\nax.axis('off')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-2-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nThe SVD of a matrix always exists.\n\nThe existence of the SVD was proven in 1936 by Carl Eckart and Gale Young.\n\nThe singular values are uniquely determined.\n\nThe left and right singular vectors are uniquely determined up to a complex sign (complex factor of modulus 1).\n\n## Outer Products\n\nThe SVD can also be represented as a sum of outer products\n\n$$ \nA = \\sum_{i=1}^{n} \\sigma_{i}\\mathbf{u}_i\\mathbf{v}_{i}^{T},\n$$\n\nwhere $\\mathbf{u}_i, \\mathbf{v}_{i}$ are the $i$-th columns of $U$ and $V$, respectively.\n\n---\n\nAn outer product \n\n$$\\mathbf{u}_i\\mathbf{v}_{i}^{T}=\n\\begin{bmatrix}\nu_{i1}v_{i1} & u_{i1}v_{i2} & \\cdots & u_{i1}v_{in} \\\\\nu_{i2}v_{i1} & u_{i2}v_{i2}  & \\cdots & u_{i2}v_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_{im}v_{i1} &  u_{i2}v_{in}  & \\cdots & u_{im}v_{in} \\\\\n\\end{bmatrix}\n$$\n\nis a rank-1 matrix.\n\nThe SVD decomposes $A$ into a linear combination of rank-1 matrices. \n\nThe singular value tells us the weight (contribution) of each rank-1 matrix to the matrix $A$.\n\n## Topics Covered\n\nIn this lecture we first discuss:\n\nTheoretical properties of the SVD related to\n    \n- matrix rank\n- determining the best low rank approximations to a matrix\n\nWe will then apply these results when we consider data matrices from the following applications \n\n- internet traffic data\n- social media data\n- image data\n- movie data\n\n# SVD Properties\n\n## Matrix Rank\n\n::: {.content-visible when-profile=\"web\"}\nLet's review some definitions.\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ be a real matrix such that with $m>n$.\n\n:::: {.fragment}\nThe __rank__ of $A$ is the number of linearly independent rows or columns of the matrix. \n::::\n\n:::: {.fragment}\nThe largest value that a matrix rank can take is $\\min(m,n)$. Since we assumed $m>n$, the largest value of the rank is $n$.\n::::\n\n:::: {.fragment}\nIf the matrix $A$ has rank equal to $n$, then we say it is full rank.\n::::\n\n:::: {.fragment}\nHowever, it can happen that the rank of a matrix is __less__ than $\\min(m,n)$. In this case we say that $A$ is rank-deficient.\n::::\n\n---\n\n::: {.content-visible when-profile=\"web\"}\nRecall that the dimension of a vector space is the smallest number of linearly independent vectors needed to span the space.\n:::\n\nThe dimension of the column space of $A$ is the __smallest number of vectors that suffice to construct the columns of $A$.__\n\n:::: {.fragment}\nIf the dimension of the column spaces is $k$, then there exists a set of vectors $\\{\\mathbf{c}_1, \\mathbf{c}_2, \\dots, \\mathbf{c}_k\\}$ such that every column $\\mathbf{a}_i$ of $A$ can be expressed as:\n\n$$\\mathbf{a}_i = r_{1i}\\mathbf{c}_1 + r_{2i}\\mathbf{c}_2 + \\dots + r_{ki}\\mathbf{c}_k\\quad i=1,\\ldots,n.$$\n::::\n\n---\n\nTo store a matrix $A \\in \\mathbb{R}^{m\\times n}$ we need to store $mn$ values.\n\nHowever, if $A$ has rank $k$, it can be factored as $A = CR$,\n$$\nA =\n\\begin{bmatrix} \n\\bigg| & \\bigg| &   & \\bigg| \\\\\n\\mathbf{c}_1   & \\mathbf{c}_2  & \\dots  & \\mathbf{c}_k  \\\\\n\\bigg| & \\bigg| &  & \\bigg|\n\\end{bmatrix}\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{r}_1   & \\mathbf{r}_2  & \\dots  & \\mathbf{r}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n$$\n\nwhere $C \\in \\mathbb{R}^{m\\times k}$ and $R \\in \\mathbb{R}^{k \\times n}$.\n\nThis only requires $k(m+n)$ values, which could be much smaller than $mn$.\n\n## Low Effective Rank\n\nIn many situations we want to __approximate__ a matrix $A$ with a low-rank matrix $A^{(k)}.$\n\n:::: {.fragment}\nTo talk about when one matrix *approximates* another, we need a norm for matrices.  \n::::\n\n:::: {.fragment}\nWe will use the __Frobenius norm__, which is defined as\n\n$$\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.$$\n::::\n\n:::: {.fragment}\nObserve that this is the $\\ell_2$ norm for a vectorized matrix, i.e., by  stacking the columns of the matrix $A$ to form a vector of length $mn$. \n::::\n\n---\n\nTo quantify when one matrix is *close* to another, we define the distance function:\n\n$$ \n\\operatorname{dist}(A,B) = \\Vert A-B\\Vert_F. \n$$\n\nThis can be viewed as Euclidean distance between $mn$-dimensional vectors.\n\nWe define the optimal __rank-$k$ approximation__ to $A$ as \n\n$$\nA^{(k)} =\\mathop{\\arg\\min}\\limits_{\\{B~|~\\operatorname{Rank} B = k\\}} \\Vert A-B\\Vert_F.\n$$\n\nIn other words, $A^{(k)}$ is the closest rank-$k$ matrix to $A$.\n\n## Finding Rank-$k$ Approximations\n\nHow can we find the optimal rank-$k$ approximation to a matrix $A$?\n\n:::: {.fragment}\nThe __Singular Value Decomposition (SVD).__\n::::\n\n:::: {.fragment}\nWhy?\n::::\n\n:::: {.fragment}\nThe SVD  gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n::::\n\n---\n\nTo form the best rank-$k$ approximation to using the SVD you calculate\n\n$$ A^{(k)} = U'\\Sigma'(V')^T,$$\n\nwhere\n\n:::: {.incremental}\n* $U'$ are the $k$ leftmost columns of $U$, \n* $\\Sigma'$ is the $k\\times k$ upper left sub-matrix of $\\Sigma$, and \n* $V'$ is the $k$ leftmost columns of $V$.\n::::\n\n\n---\n\nFor a matrix $A$ of rank $n$, we can prove that\n\n$$\\Vert A-A^{(k)}\\Vert_F^2 = \\sum_{i=k+1}^n\\sigma^2_i.$$\n\nThis means that the distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^n\\sigma^2_i}$.\n\n# Low Rank Approximations in Practice \n\n## Models are simplifications\n\nOne way of thinking about modeling or clustering is that we are building a __simplification__ of the data. \n\nThat is, a model is a description of the data, that is simpler than the data.\n\nIn particular, instead of thinking of the data as thousands or millions of individual data points, we might think of it in terms of a small number of clusters, or a parametric distribution, etc.\n\nFrom this simpler description, we hope to gain __insight.__\n\nThere is an interesting question here:  __why__ does this process often lead to insight?   \n\nThat is, why does it happen so often that a large dataset can be described in terms of a much simpler model?\n\nI don't know.\n\n## William of Ockham\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n![](figs/L10-William-of-Ockham.png){width=80%}\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=5523066)\n:::\n::: {.column width=\"60%\"}\nHowever, I think that William of Ockham (c. 1300 AD) was on the right track.\n\n:::: {.fragment}\nHe said:\n\n> Non sunt multiplicanda entia sine necessitate\n::::\n\n:::: {.fragment}\nor, in other words:\n\n> Entities must not be multiplied beyond necessity.\n::::\n\n:::: {.fragment}\nby which he meant:\n\n> Among competing hypotheses, the one with the fewest assumptions should be selected.\n::::\n\n:::\n::::\n\n::: aside\nThis has come to be known as \"Occam's razor.\"\n:::\n\n\n## Occam's Razor\n\nWilliam was saying that it is more common for a set of observations to be determined by a simple process than a complex process.\n\n:::: {.fragment}\nIn other words, the world is full of simple (but often hidden) patterns.\n::::\n\n:::: {.fragment}\nFrom which one can justify the observation that *modeling works surprisingly often*.\n::::\n\n## Low Effective Rank of Data Matrices\n\nIn general, a data matrix $A\\in\\mathbb{R}^{m\\times n}$  is usually __full rank__, meaning that $\\operatorname{Rank}(A)\\equiv p = \\min(m, n)$.\n\n:::: {.fragment}\nHowever, it is possible to encounter data matrices that have __low effective rank__.\n::::\n\n:::: {.fragment}\nThis means that we can approximate $A$ by some $A^{(k)}$ for which $k \\ll p$.\n::::\n\n:::: {.fragment}\nFor any data matrix, we can judge when this is the case by looking at its singular values, because the singular values tell us the distance to the nearest rank-$k$ matrix.\n::::\n\n## Traffic Data\n\nLet's see how this theory can be used in practice  and investigate some real data.\n\nWe'll look at data traffic on the Abilene network:\n\n![](figs/L10-Abilene-map.png)\n\nSource: Internet2, circa 2005\n\n---\n\n::: {#a66d5d30 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq = '10min', periods = 1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nAtraf\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ATLA-ATLA</th>\n      <th>ATLA-CHIN</th>\n      <th>ATLA-DNVR</th>\n      <th>ATLA-HSTN</th>\n      <th>ATLA-IPLS</th>\n      <th>ATLA-KSCY</th>\n      <th>ATLA-LOSA</th>\n      <th>ATLA-NYCM</th>\n      <th>ATLA-SNVA</th>\n      <th>ATLA-STTL</th>\n      <th>...</th>\n      <th>WASH-CHIN</th>\n      <th>WASH-DNVR</th>\n      <th>WASH-HSTN</th>\n      <th>WASH-IPLS</th>\n      <th>WASH-KSCY</th>\n      <th>WASH-LOSA</th>\n      <th>WASH-NYCM</th>\n      <th>WASH-SNVA</th>\n      <th>WASH-STTL</th>\n      <th>WASH-WASH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2003-09-01 00:00:00</th>\n      <td>8466132.0</td>\n      <td>29346537.0</td>\n      <td>15792104.0</td>\n      <td>3646187.0</td>\n      <td>21756443.0</td>\n      <td>10792818.0</td>\n      <td>14220940.0</td>\n      <td>25014340.0</td>\n      <td>13677284.0</td>\n      <td>10591345.0</td>\n      <td>...</td>\n      <td>53296727.0</td>\n      <td>18724766.0</td>\n      <td>12238893.0</td>\n      <td>52782009.0</td>\n      <td>12836459.0</td>\n      <td>31460190.0</td>\n      <td>105796930.0</td>\n      <td>13756184.0</td>\n      <td>13582945.0</td>\n      <td>120384980.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:10:00</th>\n      <td>20524567.0</td>\n      <td>28726106.0</td>\n      <td>8030109.0</td>\n      <td>4175817.0</td>\n      <td>24497174.0</td>\n      <td>8623734.0</td>\n      <td>15695839.0</td>\n      <td>36788680.0</td>\n      <td>5607086.0</td>\n      <td>10714795.0</td>\n      <td>...</td>\n      <td>68413060.0</td>\n      <td>28522606.0</td>\n      <td>11377094.0</td>\n      <td>60006620.0</td>\n      <td>12556471.0</td>\n      <td>32450393.0</td>\n      <td>70665497.0</td>\n      <td>13968786.0</td>\n      <td>16144471.0</td>\n      <td>135679630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:20:00</th>\n      <td>12864863.0</td>\n      <td>27630217.0</td>\n      <td>7417228.0</td>\n      <td>5337471.0</td>\n      <td>23254392.0</td>\n      <td>7882377.0</td>\n      <td>16176022.0</td>\n      <td>31682355.0</td>\n      <td>6354657.0</td>\n      <td>12205515.0</td>\n      <td>...</td>\n      <td>67969461.0</td>\n      <td>37073856.0</td>\n      <td>15680615.0</td>\n      <td>61484233.0</td>\n      <td>16318506.0</td>\n      <td>33768245.0</td>\n      <td>71577084.0</td>\n      <td>13938533.0</td>\n      <td>14959708.0</td>\n      <td>126175780.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:30:00</th>\n      <td>10856263.0</td>\n      <td>32243146.0</td>\n      <td>7136130.0</td>\n      <td>3695059.0</td>\n      <td>28747761.0</td>\n      <td>9102603.0</td>\n      <td>16200072.0</td>\n      <td>27472465.0</td>\n      <td>9402609.0</td>\n      <td>10934084.0</td>\n      <td>...</td>\n      <td>66616097.0</td>\n      <td>43019246.0</td>\n      <td>12726958.0</td>\n      <td>64027333.0</td>\n      <td>16394673.0</td>\n      <td>33440318.0</td>\n      <td>79682647.0</td>\n      <td>16212806.0</td>\n      <td>16425845.0</td>\n      <td>112891500.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:40:00</th>\n      <td>10068533.0</td>\n      <td>30164311.0</td>\n      <td>8061482.0</td>\n      <td>2922271.0</td>\n      <td>35642229.0</td>\n      <td>9104036.0</td>\n      <td>12279530.0</td>\n      <td>29171205.0</td>\n      <td>7624924.0</td>\n      <td>11327807.0</td>\n      <td>...</td>\n      <td>66797282.0</td>\n      <td>40408580.0</td>\n      <td>11733121.0</td>\n      <td>54541962.0</td>\n      <td>16769259.0</td>\n      <td>33927515.0</td>\n      <td>81480788.0</td>\n      <td>16757707.0</td>\n      <td>15158825.0</td>\n      <td>123140310.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:10:00</th>\n      <td>8849096.0</td>\n      <td>33461807.0</td>\n      <td>5866138.0</td>\n      <td>3786793.0</td>\n      <td>19097140.0</td>\n      <td>10561532.0</td>\n      <td>26092040.0</td>\n      <td>28640962.0</td>\n      <td>8343867.0</td>\n      <td>8820650.0</td>\n      <td>...</td>\n      <td>65925313.0</td>\n      <td>21751316.0</td>\n      <td>11058944.0</td>\n      <td>58591021.0</td>\n      <td>17137907.0</td>\n      <td>24297674.0</td>\n      <td>83293655.0</td>\n      <td>17329425.0</td>\n      <td>20865535.0</td>\n      <td>123125390.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:20:00</th>\n      <td>9776675.0</td>\n      <td>31474607.0</td>\n      <td>5874654.0</td>\n      <td>11277465.0</td>\n      <td>14314837.0</td>\n      <td>9106198.0</td>\n      <td>26412752.0</td>\n      <td>26168288.0</td>\n      <td>8638782.0</td>\n      <td>9193717.0</td>\n      <td>...</td>\n      <td>70075490.0</td>\n      <td>29126443.0</td>\n      <td>12667321.0</td>\n      <td>54571764.0</td>\n      <td>15383038.0</td>\n      <td>25238842.0</td>\n      <td>70015955.0</td>\n      <td>16526455.0</td>\n      <td>16881206.0</td>\n      <td>142106800.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:30:00</th>\n      <td>9144621.0</td>\n      <td>32117262.0</td>\n      <td>5762691.0</td>\n      <td>7154577.0</td>\n      <td>17771350.0</td>\n      <td>10149256.0</td>\n      <td>29501669.0</td>\n      <td>25998158.0</td>\n      <td>11343171.0</td>\n      <td>9423042.0</td>\n      <td>...</td>\n      <td>68544458.0</td>\n      <td>27817836.0</td>\n      <td>15892668.0</td>\n      <td>50326213.0</td>\n      <td>12098328.0</td>\n      <td>27689197.0</td>\n      <td>73553203.0</td>\n      <td>18022288.0</td>\n      <td>18471915.0</td>\n      <td>127918530.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:40:00</th>\n      <td>8802106.0</td>\n      <td>29932510.0</td>\n      <td>5279285.0</td>\n      <td>5950898.0</td>\n      <td>20222187.0</td>\n      <td>10636832.0</td>\n      <td>19613671.0</td>\n      <td>26124024.0</td>\n      <td>8732768.0</td>\n      <td>8217873.0</td>\n      <td>...</td>\n      <td>65087776.0</td>\n      <td>28836922.0</td>\n      <td>11075541.0</td>\n      <td>52574692.0</td>\n      <td>11933512.0</td>\n      <td>31632344.0</td>\n      <td>81693475.0</td>\n      <td>16677568.0</td>\n      <td>16766967.0</td>\n      <td>138180630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:50:00</th>\n      <td>8716795.6</td>\n      <td>22660870.0</td>\n      <td>6240626.4</td>\n      <td>5657380.6</td>\n      <td>17406086.0</td>\n      <td>8808588.5</td>\n      <td>15962917.0</td>\n      <td>18367639.0</td>\n      <td>7767967.3</td>\n      <td>7470650.1</td>\n      <td>...</td>\n      <td>65599891.0</td>\n      <td>25862152.0</td>\n      <td>11673804.0</td>\n      <td>60086953.0</td>\n      <td>11851656.0</td>\n      <td>30979811.0</td>\n      <td>73577193.0</td>\n      <td>19167646.0</td>\n      <td>19402758.0</td>\n      <td>137288810.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1008 rows × 121 columns</p>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: {#74f10386 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nAtraf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n(1008, 121)\n```\n:::\n:::\n\n\nAs we would expect, our traffic matrix has rank 121:\n\n::: {#cb9e2de3 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nnp.linalg.matrix_rank(Atraf)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\nnp.int64(121)\n```\n:::\n:::\n\n\nHowever -- perhaps it has low __effective__ rank.\n\nThe `numpy` routine for computing the SVD is `np.linalg.svd`:\n\n::: {#a1556f5b .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(Atraf)\n```\n:::\n\n\n---\n\nNow let's look at the singular values of `Atraf` to see if it can be usefully approximated as a low-rank matrix:\n\n::: {#88f4c620 .cell execution_count=7}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nplt.plot(range(1, 1+len(s)), s)\nplt.xlabel(r'$k$', size=20)\nplt.ylabel(r'$\\sigma_k$', size=20)\nplt.ylim(ymin=0)\nplt.xlim(xmin=-1)\nplt.title(r'Singular Values of $A$', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-7-output-1.png){width=453 height=324}\n:::\n:::\n\n\nThis classic, sharp-elbow tells us that a few singular values are very large, and most singular values are quite small.\n\n---\n\nZooming in for just small $k$ values, we can see that the elbow is around 4 - 6 singular values:\n\n::: {#72d9606b .cell execution_count=8}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nplt.plot(range(1, 21), s[0:20]/Anorm, '.-')\nplt.xlim([0.5, 20])\nplt.ylim([0, 1])\nplt.xlabel(r'$k$', size=20)\nplt.xticks(range(1, 21))\nplt.ylabel(r'$\\sigma_k$', size=20);\nplt.title(r'Singular Values of $A$',size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-8-output-1.png){width=462 height=324}\n:::\n:::\n\n\nThis pattern of singular values suggests __low effective rank.__\n\n---\n\nLet's use the formula above to compute the relative error of a rank-$k$ approximation to $A$:\n\n::: {#27f041a2 .cell execution_count=9}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nerr = np.cumsum(s[::-1]**2)\nerr = np.sqrt(err[::-1])\nplt.plot(range(0, 20), err[:20]/Anorm, '.-')\nplt.xlim([0, 20])\nplt.ylim([0, 1])\nplt.xticks(range(1, 21))\nplt.xlabel(r'$k$', size = 16)\nplt.ylabel(r'relative F-norm error', size=16)\nplt.title(r'Relative Error of rank-$k$ approximation to $A$', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-9-output-1.png){width=492 height=315}\n:::\n:::\n\n\nRemarkably, we are down to 9% relative error using only a rank 20 approximation to $A$.\n\n---\n\nSo instead of storing \n\n* $mn =$ (1008 $\\cdot$ 121) = 121,968 values, \n\nwe only need to store \n\n* $k(m+n)$ = 20 $\\cdot$ (1008 + 121) = 22,580 values, \n\nwhich is an 81% reduction in size.\n\n## Low Effective Rank is Common\n\nIn practice __many__ datasets have low effective rank.   \n\nWe consider the following examples:\n\n:::: {.incremental}\n* Likes on Facebook,\n* Yelp reviews and Tweets (the site formerly known as twitter),\n* User preferences over time,\n* Images.\n::::\n\n## Likes on Facebook\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of likes:  Timebins $\\times$ Users\n2. Number of likes:  Users $\\times$ Page Categories\n3. Entropy of likes across categories:  Timebins $\\times$ Users\n::::\n\n:::: {.fragment}\n![](figs/L10-facebook.png){fig-align=\"center\"}\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n## Social Media Activity\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of Yelp reviews:  Timebins $\\times$ Users\n2. Number of Yelp reviews:  Users $\\times$ Yelp Categories\n3. Number of Tweets:  Users $\\times$ Topic Categories\n::::\n\n:::: {.fragment}\n![](figs/L10-yelp-twitter.png)\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n\n## Netflix\n\nExample: the Netflix prize worked with partially-observed matrices like this:\n\n$$\n\\begin{bmatrix}\n & & & \\vdots & & & \\\\\n & & 3 & 2 & & 1 &\\\\\n & 1 & & 1 & & & \\\\\n\\dots & & 2 & & 4 & & \\dots\\\\\n & 5 & 5 & & 4 & & \\\\\n & 1 & & & 1 & 5 & \\\\\n & & & \\vdots & & & \\\\\n\\end{bmatrix},\n$$\n\n:::: {.fragment}\nwhere the rows correspond to users, the columns to movies, and the entries are ratings.\n::::\n\n:::: {.fragment}\nAlthough the problem matrix was of size 500,000 $\\times$ 18,000, the winning approach modeled the matrix as having __rank 20 to 40.__\n\nSource: [Koren et al, IEEE Computer, 2009]\n::::\n\n\n## Images\n\nImage data often shows low effective rank.\n\nFor example, here is an original photo:\n\n::: {#34a54d3d .cell execution_count=10}\n``` {.python .cell-code}\nboat = np.loadtxt('data/images/boat/boat.dat')\nimport matplotlib.cm as cm\nplt.figure()\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-10-output-1.png){width=389 height=389}\n:::\n:::\n\n\n---\n\nLet's look at the singular values.\n\n::: {#5e81f3dc .cell execution_count=11}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\nplt.plot(s)\nplt.xlabel('$k$', size=16)\nplt.ylabel(r'$\\sigma_k$', size=16)\nplt.title('Singular Values of Boat Image', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-11-output-1.png){width=841 height=462}\n:::\n:::\n\n\n---\n\nThis image is 512 $\\times$ 512. As a matrix, it has rank of 512.   \n\nBut its _effective_ rank is low.\n\nBased on the previous plot, its effective rank is perhaps 40.\n\nLet's find the closest rank-40 matrix and view it.\n\n::: {#ced6f751 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\ns[40:] = 0\nboatApprox = u @ np.diag(s) @ vt\n```\n:::\n\n\n::: {#fea5894d .cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(9, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(boatApprox, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 40 Boat')\nplt.subplot(1, 2, 2)\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 512 Boat')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-13-output-1.png){width=689 height=344}\n:::\n:::\n\n\n## Interpretations of Low Effective Rank\n\nHow can we understand the low-effective-rank phenomenon in general?\n\n:::: {.fragment}\nThere are two helpful interpretations:\n\n:::: {.incremental}\n1. Common Patterns\n2. Latent Factors\n::::\n::::\n\n## Low Rank Implies Common Patterns\n\nThe first interpretation of low-rank behavior is in answering the question:\n\n:::: {.fragment}\n\"What is the strongest pattern in the data?\"\n::::\n\n:::: {.fragment}\nRemember that using the SVD we form the low-rank approximation as\n\n$$ A^{(k)} =  U'\\Sigma'(V')^T$$\n\nand\n\n:::: {.incremental}\n\n- $U'$ are the $k$ leftmost columns of $U$, \n- $\\Sigma'$ is the $k\\times k$ upper left submatrix of $\\Sigma$, and \n- $V'$ are the $k$ leftmost columns of $V$.\n::::\n::::\n \n:::: {.fragment}\nIn this interpretation, we think of each column of $A^{(k)}$ as a combination of the columns of $U'$.\n::::\n\n:::: {.fragment}\nHow can this be helpful? \n::::\n\n\n## Common Patterns: Traffic Example\n\nConsider the set of traffic traces. There are clearly some common patterns. How can we find them?\n\n::: {#a54dbd44 .cell execution_count=14}\n``` {.python .cell-code}\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq='10min', periods=1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nplt.figure(figsize=(10, 8))\nfor i in range(1, 13):\n    ax = plt.subplot(4, 3, i)\n    Atraf.iloc[:, i-1].plot()\n    plt.title(odnames[i])\nplt.subplots_adjust(hspace=1)\nplt.suptitle('Twelve Example Traffic Traces', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-14-output-1.png){width=781 height=740}\n:::\n:::\n\n\n---\n\nLet's use as our example $\\mathbf{a}_1,$ the first column of $A$.\n\nThis happens to be the ATLA-CHIN flow.\n\nThe equation above tells us that\n\n$$\\mathbf{a}_1 \\approx v_{11}\\sigma_1\\mathbf{u}_1 + v_{12}\\sigma_2\\mathbf{u}_2 + \\dots + v_{1k}\\sigma_k\\mathbf{u}_k.$$\n\nIn other words, $\\mathbf{u}_1$ (the first column of $U$) is the \"strongest\" pattern occurring in $A$, and its strength is measured by $\\sigma_1$.\n\n---\n\nHere is a view of the first 2 columns of $U\\Sigma$ for the traffic matrix data.\n\nThese are the strongest patterns occurring across all of the 121 traces.\n\n::: {#1867a59a .cell execution_count=15}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(Atraf, full_matrices=False)\nuframe = pd.DataFrame(u @ np.diag(s), index=pd.date_range('9/1/2003', freq = '10min', periods = 1008))\nuframe[0].plot(color='r', label='Column 1')\nuframe[1].plot(label='Column 2')\nplt.legend(loc='best')\nplt.title('First Two Columns of $U$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-15-output-1.png){width=792 height=461}\n:::\n:::\n\n\n## Low Rank Defines Latent Factors\n\nThe next interpretation of low-rank behavior is that it exposes \"latent factors\" that describe the data.\n\n:::: {.fragment}\nIn this interpretation, we think of each element of $A^{(k)}=U'\\Sigma'(V')^T$ as the inner product of a row of $U'\\Sigma'$ and a column of $(V')^{T}$ (equivalently a row of $V'$).\n::::\n\n:::: {.fragment}\nLet's say we are working with a matrix of users and items.\n::::\n\n:::: {.fragment}\nIn particular, let the items be movies and matrix entries be ratings, as in the Netflix prize.\n::::\n\n## Latent Factors: Netflix example\n\nRecall the structure from earlier:\n\n$$\n\\begin{bmatrix}\n\\vdots & \\vdots &  & \\vdots \\\\\n\\mathbf{a}_{1} & \\mathbf{a}_{2} & \\cdots & \\mathbf{a}_{n} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\n\\end{bmatrix} \n\\approx\n\\underbrace{\n\\begin{bmatrix}\n\\vdots &  \\vdots  \\\\\n\\sigma_1 \\mathbf{u}_1 & \\sigma_k \\mathbf{u}_{k} \\\\\n\\vdots& \\vdots  \\\\\n\\end{bmatrix}\n}_{\\tilde{U}\\in\\mathbb{R}^{m\\times k}}\n\\underbrace{\n\\begin{bmatrix}\n\\cdots & \\mathbf{v}_{1}^{T} & \\cdots   \\\\\n\\cdots & \\mathbf{v}_{k}^{T} & \\cdots   \\\\\n\\end{bmatrix}\n}_{\\tilde{V}\\in\\mathbb{R}^{k\\times n}},\n$$\n\nwhere the rows of $A$ are the users and the columns are movie ratings.\n\nThen the rating that a user gives a movie is the inner product of a $k$-element vector that corresponds to the user, and a $k$-element vector that corresponds to the movie.\n\nIn other words we have:\n    \n$$ \na_{ij} = \\sum_{p=1}^{k} \\tilde{U}_{ip} \\tilde{V}_{pj}.\n$$\n\n---\n\nWe can think of user $i$'s preferences as being captured by row $i$ of $\\tilde{U}$, which is a point in $\\mathbb{R}^k$.  \n\n:::: {.fragment}\nWe have described everything we need to know to predict user $i$'s ratings via a $k$-element vector.\n::::\n\n:::: {.fragment}\nThe $k$-element vector is called a __latent factor.__\n::::\n\n:::: {.fragment}\nLikewise, we can think of column $j$ of $\\tilde{V}$ as a \"description\" of movie $j$ (another latent factor).\n::::\n\n:::: {.fragment}\nThe value in using latent factors comes from the summarization of user preferences, and the predictive power one obtains.\n::::\n\n:::: {.fragment}\nFor example, the winning entry in the Netflix prize competition modeled user preferences with 20 latent factors.\n::::\n\n:::: {.fragment}\nThe remarkable thing is that a person's preferences for all 18,000 movies can be reasonably well captured in a vector of dimension 20!\n::::\n\n---\n\nHere is a figure from the paper that described the winning strategy in the Netflix prize.\n\nIt shows a hypothetical __latent space__ in which each user, and each movie, is represented by a latent vector.\n\n![](figs/L10-Movie-Latent-Space.png)\n\nSource: Koren et al, IEEE Computer, 2009 \n\nIn practice, this is perhaps a 20- or 40-dimensional space.\n\n---\n\nHere are some representations of movies in that space (reduced to 2-D).\n\nNotice how the space seems to capture similarity among movies!\n\n![](figs/L10-Netflix-Latent-Factors.png)\n\nSource: Koren et al, IEEE Computer, 2009 \n\n## Summary\n\n:::: {.incremental}\n* When we are working with data matrices, it is valuable to consider the __effective rank__.\n* Many (many) datasets in real life show __low effective rank__.\n* This property can be explored precisely using the Singular Value Decomposition of the matrix.\n* When low effective rank is present\n    * the matrix can be compressed with only small loss of accuracy,\n    * we can extract the *strongest* patterns in the data,\n    * we can describe each data item in terms of the inner product of __latent factors__.\n::::\n\n",
=======
  "hash": "5e730d273c981e26e567710e2db5858a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: SVD - Low Rank Approximations\njupyter: python3\n---\n\n\n# Low Rank Approximations\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/10-Low-Rank-and-SVD.ipynb)\n\nWe now consider applications of the Singular Value Decomposition (SVD).\n\n> SVD is \"the Swiss Army Knife of Numerical Linear Algebra.”\n\nDianne O’Leary, MMDS ’06 (Workshop on Algorithms for Modern Massive Data Sets)\n\n---\n\nWe will see how the SVD is used for\n\n:::: {.incremental}\n- low rank approximations\n- dimensionality reduction\n::::\n\n\n## Singular Vectors and Values\n\nFor $A\\in\\mathbb{R}^{m\\times n}$ with $m>n$ and rank $r$,\nthere exists a set of orthogonal vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}$ \nand a set of orthogonal vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_m\\}$ such that\n\n$$\nA\\mathbf{v}_1 = \\sigma_1 \\mathbf{u}_1 \\cdots A\\mathbf{v}_r = \\sigma_r \\mathbf{u}_r \\quad\\quad A\\mathbf{v}_{r+1} = 0 \\cdots A\\mathbf{v}_n = 0\n$$\n\n:::: {.fragment}\n\nWe can collect the vectors $\\mathbf{v}_i$ into a matrix $V$ and the vectors $\\mathbf{u}_i$ into a matrix $U$.\n$$\nA\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_r\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n$$\n\nWe call the $\\mathbf{v}_i$ the __right singular vectors__ and the $\\mathbf{u}_i$ the __left singular vectors__.\n\n::::\n\n:::: {.fragment}\n\nAnd because $V$ is an orthogonal matrix, we have $V V^T = I$, so we can right multiply both sides by $V^T$ to get\n\n$$\nA\n =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_r\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}^T\n$$\n\n::::\n\n:::: {.fragment}\n\nWe can write this as\n\n$$\nA = U\\Sigma V^{T},\n$$\n::::\n\n## Singular Value Decomposition\n\nThe SVD of a matrix $A\\in\\mathbb{R}^{m\\times n}$ (where $m>n$) is\n\n$$\nA = U\\Sigma V^{T},\n$$\n\nwhere\n\n:::: {.incremental}\n- $U$ has dimension $m\\times n$. The columns of $U$ are orthogonal. The columns of $U$ are the __left singular vectors__.\n- $\\Sigma$ has dimension $n\\times n$. The only non-zero values are on the main diagonal and they are nonnegative real numbers  $\\sigma_1\\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r$ and $\\sigma_{r+1} = \\ldots = \\sigma_n = 0$. These are called the __singular values__ of $A$.\n- $V$ has dimension $n \\times n$. The columns of $V$ are orthogonal. The columns of $V$ are the __right singular vectors__.\n::::\n\n---\n\n::: {#003fc722 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Draw matrix A\nrect_A = patches.Rectangle((0, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_A)\nax.text(1, 1.5, r'$A$', fontsize=20, ha='center', va='center')\nax.text(1, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw equal sign\nax.text(2.5, 1.5, r'$=$', fontsize=20, ha='center', va='center')\n\n# Draw matrix U\nrect_U = patches.Rectangle((3, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_U)\nax.text(4, 1.5, r'$U$', fontsize=20, ha='center', va='center')\nax.text(4, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw Sigma\nrect_Sigma = patches.Rectangle((5.5, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_Sigma)\nax.text(6.5, 2, r'$\\Sigma$', fontsize=20, ha='center', va='center')\nax.text(6.5, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw matrix V^T with the same dimensions as Sigma\nrect_VT = patches.Rectangle((8, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_VT)\nax.text(9, 2, r'$V^T$', fontsize=20, ha='center', va='center')\nax.text(9, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Set limits and remove axes\nax.set_xlim(-1, 11)\nax.set_ylim(-2, 4)\nax.axis('off')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-2-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nThe SVD of a matrix always exists.\n\nThe existence of the SVD was proven in 1936 by Carl Eckart and Gale Young.\n\nThe singular values are uniquely determined.\n\nThe left and right singular vectors are uniquely determined up to a complex sign (complex factor of modulus 1).\n\n## Outer Products\n\nThe SVD can also be represented as a sum of outer products\n\n$$ \nA = \\sum_{i=1}^{n} \\sigma_{i}\\mathbf{u}_i\\mathbf{v}_{i}^{T},\n$$\n\nwhere $\\mathbf{u}_i, \\mathbf{v}_{i}$ are the $i$-th columns of $U$ and $V$, respectively.\n\n---\n\nAn outer product of a $m\\times 1$ vector and a $1\\times n$ vector is a $m\\times n$ matrix.\n\n$$\\mathbf{u}_i\\mathbf{v}_{i}^{T}=\n\\begin{bmatrix}\nu_{i1}v_{i1} & u_{i1}v_{i2} & \\cdots & u_{i1}v_{in} \\\\\nu_{i2}v_{i1} & u_{i2}v_{i2}  & \\cdots & u_{i2}v_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_{im}v_{i1} &  u_{i2}v_{in}  & \\cdots & u_{im}v_{in} \\\\\n\\end{bmatrix}\n$$\n\nis a rank-1 matrix.\n\n:::: {.fragment}\n**Alternate Interpretation:**\n\nThe SVD decomposes $A$ into a linear combination of rank-1 matrices. \n\nThe singular value tells us the weight (contribution) of each rank-1 matrix to the matrix $A$.\n::::\n\n## Topics Covered\n\nIn this lecture we first discuss:\n\nTheoretical properties of the SVD related to\n    \n- matrix rank\n- determining the best low rank approximations to a matrix\n\nWe will then apply these results when we consider data matrices from the following applications \n\n- traffic data\n- social media data\n- image data\n- traffic data\n- movie data\n\n# SVD Properties\n\n## Matrix Rank\n\n::: {.content-visible when-profile=\"web\"}\nLet's review some definitions.\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ be a real matrix such that with $m>n$.\n\n:::: {.fragment}\nThe __rank__ of $A$ is the number of linearly independent rows or columns of the matrix. \n::::\n\n:::: {.fragment}\nThe largest value that a matrix rank can take is $\\min(m,n)$. Since we assumed $m>n$, the largest value of the rank is $n$.\n::::\n\n:::: {.fragment}\nIf the matrix $A$ has rank equal to $n$, then we say it is full rank.\n::::\n\n:::: {.fragment}\nHowever, it can happen that the rank of a matrix is __less__ than $\\min(m,n)$. In this case we say that $A$ is rank-deficient.\n::::\n\n---\n\n::: {.content-visible when-profile=\"web\"}\nRecall that the dimension of a vector space is the smallest number of linearly independent vectors needed to span the space.\n:::\n\nThe dimension of the column space of $A$ is the __smallest number of vectors that suffice to construct the columns of $A$.__\n\n:::: {.fragment}\nIf the dimension of the column spaces is $k$, then there exists a set of vectors $\\{\\mathbf{c}_1, \\mathbf{c}_2, \\dots, \\mathbf{c}_k\\}$ such that every column $\\mathbf{a}_i$ of $A$ can be expressed as:\n\n$$\\mathbf{a}_i = r_{1i}\\mathbf{c}_1 + r_{2i}\\mathbf{c}_2 + \\dots + r_{ki}\\mathbf{c}_k\\quad i=1,\\ldots,n.$$\n::::\n\n---\n\nTo store a matrix $A \\in \\mathbb{R}^{m\\times n}$ we need to store $mn$ values.\n\nHowever, if $A$ has rank $k$, it can be factored as $A = CR$,\n$$\nA =\n\\begin{bmatrix} \n\\bigg| & \\bigg| &   & \\bigg| \\\\\n\\mathbf{c}_1   & \\mathbf{c}_2  & \\dots  & \\mathbf{c}_k  \\\\\n\\bigg| & \\bigg| &  & \\bigg|\n\\end{bmatrix}\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{r}_1   & \\mathbf{r}_2  & \\dots  & \\mathbf{r}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n$$\n\nwhere $C \\in \\mathbb{R}^{m\\times k}$ and $R \\in \\mathbb{R}^{k \\times n}$.\n\nThis only requires $k(m+n)$ values, which could be much smaller than $mn$.\n\n## Low Effective Rank\n\nIn many situations we want to __approximate__ a matrix $A$ with a low-rank matrix $A^{(k)}.$\n\n:::: {.fragment}\nTo talk about when one matrix *approximates* another, we need a norm for matrices.  \n::::\n\n:::: {.fragment}\nWe will use the __Frobenius norm__, which is defined as\n\n$$\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.$$\n::::\n\n:::: {.fragment}\nObserve that this is the $\\ell_2$ norm for a vectorized matrix, i.e., by  stacking the columns of the matrix $A$ to form a vector of length $mn$. \n::::\n\n---\n\nTo quantify when one matrix is *close* to another, we define the distance function:\n\n$$ \n\\operatorname{dist}(A,B) = \\Vert A-B\\Vert_F. \n$$\n\nThis can be viewed as Euclidean distance between $mn$-dimensional vectors.\n\nWe define the optimal __rank-$k$ approximation__ to $A$ as \n\n$$\nA^{(k)} =\\mathop{\\arg\\min}\\limits_{\\{B~|~\\operatorname{Rank} B = k\\}} \\Vert A-B\\Vert_F.\n$$\n\nIn other words, $A^{(k)}$ is the closest rank-$k$ matrix to $A$.\n\n## Finding Rank-$k$ Approximations\n\nHow can we find the optimal rank-$k$ approximation to a matrix $A$?\n\n:::: {.fragment}\nThe __Singular Value Decomposition (SVD).__\n::::\n\n:::: {.fragment}\nWhy?\n::::\n\n:::: {.fragment}\nThe SVD  gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n::::\n\n---\n\nTo form the best rank-$k$ approximation to using the SVD you calculate\n\n$$ A^{(k)} = U'\\Sigma'(V')^T,$$\n\nwhere\n\n:::: {.incremental}\n* $U'$ are the $k$ leftmost columns of $U$, \n* $\\Sigma'$ is the $k\\times k$ upper left sub-matrix of $\\Sigma$, and \n* $V'$ is the $k$ leftmost columns of $V$.\n::::\n\n\n---\n\nFor a matrix $A$ of rank $n$, we can prove that\n\n$$\\Vert A-A^{(k)}\\Vert_F^2 = \\sum_{i=k+1}^n\\sigma^2_i.$$\n\nThis means that the distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^n\\sigma^2_i}$.\n\n# Low Rank Approximations in Practice \n\n## Models are simplifications\n\nOne way of thinking about modeling or clustering is that we are building a \n__simplification__ of the data. \n\nThat is, a model is a description of the data, that is simpler than the data.\n\nIn particular, instead of thinking of the data as thousands or millions of \nindividual data points, we might think of it in terms of a small number of \nclusters, or a parametric distribution, etc.\n\nFrom this simpler description, we hope to gain __insight.__\n\nThere is an interesting question here:  __why__ does this process often lead to insight?   \n\nThat is, why does it happen so often that a large dataset can be described in\nterms of a much simpler model?\n\n\n## William of Ockham\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n![](figs/L10-William-of-Ockham.png){width=80%}\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=5523066)\n:::\n::: {.column width=\"60%\"}\nWilliam of Ockham (c. 1300 AD) said:\n\n:::: {.fragment}\n\n> Non sunt multiplicanda entia sine necessitate\n::::\n\n:::: {.fragment}\nor, in other words:\n\n> Entities must not be multiplied beyond necessity.\n::::\n\n:::: {.fragment}\nby which he meant:\n\n> Among competing hypotheses, the one with the fewest assumptions should be selected.\n::::\n\n:::\n::::\n\n::: aside\nThis has come to be known as \"Occam's razor.\"\n:::\n\n\n## Occam's Razor\n\nWilliam was saying that it is more common for a set of observations to be determined by a simple process than a complex process.\n\n:::: {.fragment}\nIn other words, the world is full of simple (but often hidden) patterns.\n::::\n\n:::: {.fragment}\nFrom which one can justify the observation that *modeling works surprisingly often*.\n::::\n\n## Low Effective Rank of Data Matrices\n\nIn general, a data matrix $A\\in\\mathbb{R}^{m\\times n}$  is usually __full rank__, meaning that $\\operatorname{Rank}(A)\\equiv p = \\min(m, n)$.\n\n:::: {.fragment}\nHowever, it is possible to encounter data matrices that have __low effective rank__.\n::::\n\n:::: {.fragment}\nThis means that we can approximate $A$ by some $A^{(k)}$ for which $k \\ll p$.\n::::\n\n:::: {.fragment}\nFor any data matrix, we can judge when this is the case by looking at its singular values, because the singular values tell us the distance to the nearest rank-$k$ matrix.\n::::\n\n## Traffic Data\n\nLet's see how this theory can be used in practice  and investigate some real data.\n\nWe'll look at data traffic on the Abilene network:\n\n![](figs/L10-Abilene-map.png)\n\nSource: Internet2, circa 2005\n\n---\n\n::: {#d1210a9f .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq = '10min', periods = 1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nAtraf\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ATLA-ATLA</th>\n      <th>ATLA-CHIN</th>\n      <th>ATLA-DNVR</th>\n      <th>ATLA-HSTN</th>\n      <th>ATLA-IPLS</th>\n      <th>ATLA-KSCY</th>\n      <th>ATLA-LOSA</th>\n      <th>ATLA-NYCM</th>\n      <th>ATLA-SNVA</th>\n      <th>ATLA-STTL</th>\n      <th>...</th>\n      <th>WASH-CHIN</th>\n      <th>WASH-DNVR</th>\n      <th>WASH-HSTN</th>\n      <th>WASH-IPLS</th>\n      <th>WASH-KSCY</th>\n      <th>WASH-LOSA</th>\n      <th>WASH-NYCM</th>\n      <th>WASH-SNVA</th>\n      <th>WASH-STTL</th>\n      <th>WASH-WASH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2003-09-01 00:00:00</th>\n      <td>8466132.0</td>\n      <td>29346537.0</td>\n      <td>15792104.0</td>\n      <td>3646187.0</td>\n      <td>21756443.0</td>\n      <td>10792818.0</td>\n      <td>14220940.0</td>\n      <td>25014340.0</td>\n      <td>13677284.0</td>\n      <td>10591345.0</td>\n      <td>...</td>\n      <td>53296727.0</td>\n      <td>18724766.0</td>\n      <td>12238893.0</td>\n      <td>52782009.0</td>\n      <td>12836459.0</td>\n      <td>31460190.0</td>\n      <td>105796930.0</td>\n      <td>13756184.0</td>\n      <td>13582945.0</td>\n      <td>120384980.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:10:00</th>\n      <td>20524567.0</td>\n      <td>28726106.0</td>\n      <td>8030109.0</td>\n      <td>4175817.0</td>\n      <td>24497174.0</td>\n      <td>8623734.0</td>\n      <td>15695839.0</td>\n      <td>36788680.0</td>\n      <td>5607086.0</td>\n      <td>10714795.0</td>\n      <td>...</td>\n      <td>68413060.0</td>\n      <td>28522606.0</td>\n      <td>11377094.0</td>\n      <td>60006620.0</td>\n      <td>12556471.0</td>\n      <td>32450393.0</td>\n      <td>70665497.0</td>\n      <td>13968786.0</td>\n      <td>16144471.0</td>\n      <td>135679630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:20:00</th>\n      <td>12864863.0</td>\n      <td>27630217.0</td>\n      <td>7417228.0</td>\n      <td>5337471.0</td>\n      <td>23254392.0</td>\n      <td>7882377.0</td>\n      <td>16176022.0</td>\n      <td>31682355.0</td>\n      <td>6354657.0</td>\n      <td>12205515.0</td>\n      <td>...</td>\n      <td>67969461.0</td>\n      <td>37073856.0</td>\n      <td>15680615.0</td>\n      <td>61484233.0</td>\n      <td>16318506.0</td>\n      <td>33768245.0</td>\n      <td>71577084.0</td>\n      <td>13938533.0</td>\n      <td>14959708.0</td>\n      <td>126175780.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:30:00</th>\n      <td>10856263.0</td>\n      <td>32243146.0</td>\n      <td>7136130.0</td>\n      <td>3695059.0</td>\n      <td>28747761.0</td>\n      <td>9102603.0</td>\n      <td>16200072.0</td>\n      <td>27472465.0</td>\n      <td>9402609.0</td>\n      <td>10934084.0</td>\n      <td>...</td>\n      <td>66616097.0</td>\n      <td>43019246.0</td>\n      <td>12726958.0</td>\n      <td>64027333.0</td>\n      <td>16394673.0</td>\n      <td>33440318.0</td>\n      <td>79682647.0</td>\n      <td>16212806.0</td>\n      <td>16425845.0</td>\n      <td>112891500.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:40:00</th>\n      <td>10068533.0</td>\n      <td>30164311.0</td>\n      <td>8061482.0</td>\n      <td>2922271.0</td>\n      <td>35642229.0</td>\n      <td>9104036.0</td>\n      <td>12279530.0</td>\n      <td>29171205.0</td>\n      <td>7624924.0</td>\n      <td>11327807.0</td>\n      <td>...</td>\n      <td>66797282.0</td>\n      <td>40408580.0</td>\n      <td>11733121.0</td>\n      <td>54541962.0</td>\n      <td>16769259.0</td>\n      <td>33927515.0</td>\n      <td>81480788.0</td>\n      <td>16757707.0</td>\n      <td>15158825.0</td>\n      <td>123140310.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:10:00</th>\n      <td>8849096.0</td>\n      <td>33461807.0</td>\n      <td>5866138.0</td>\n      <td>3786793.0</td>\n      <td>19097140.0</td>\n      <td>10561532.0</td>\n      <td>26092040.0</td>\n      <td>28640962.0</td>\n      <td>8343867.0</td>\n      <td>8820650.0</td>\n      <td>...</td>\n      <td>65925313.0</td>\n      <td>21751316.0</td>\n      <td>11058944.0</td>\n      <td>58591021.0</td>\n      <td>17137907.0</td>\n      <td>24297674.0</td>\n      <td>83293655.0</td>\n      <td>17329425.0</td>\n      <td>20865535.0</td>\n      <td>123125390.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:20:00</th>\n      <td>9776675.0</td>\n      <td>31474607.0</td>\n      <td>5874654.0</td>\n      <td>11277465.0</td>\n      <td>14314837.0</td>\n      <td>9106198.0</td>\n      <td>26412752.0</td>\n      <td>26168288.0</td>\n      <td>8638782.0</td>\n      <td>9193717.0</td>\n      <td>...</td>\n      <td>70075490.0</td>\n      <td>29126443.0</td>\n      <td>12667321.0</td>\n      <td>54571764.0</td>\n      <td>15383038.0</td>\n      <td>25238842.0</td>\n      <td>70015955.0</td>\n      <td>16526455.0</td>\n      <td>16881206.0</td>\n      <td>142106800.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:30:00</th>\n      <td>9144621.0</td>\n      <td>32117262.0</td>\n      <td>5762691.0</td>\n      <td>7154577.0</td>\n      <td>17771350.0</td>\n      <td>10149256.0</td>\n      <td>29501669.0</td>\n      <td>25998158.0</td>\n      <td>11343171.0</td>\n      <td>9423042.0</td>\n      <td>...</td>\n      <td>68544458.0</td>\n      <td>27817836.0</td>\n      <td>15892668.0</td>\n      <td>50326213.0</td>\n      <td>12098328.0</td>\n      <td>27689197.0</td>\n      <td>73553203.0</td>\n      <td>18022288.0</td>\n      <td>18471915.0</td>\n      <td>127918530.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:40:00</th>\n      <td>8802106.0</td>\n      <td>29932510.0</td>\n      <td>5279285.0</td>\n      <td>5950898.0</td>\n      <td>20222187.0</td>\n      <td>10636832.0</td>\n      <td>19613671.0</td>\n      <td>26124024.0</td>\n      <td>8732768.0</td>\n      <td>8217873.0</td>\n      <td>...</td>\n      <td>65087776.0</td>\n      <td>28836922.0</td>\n      <td>11075541.0</td>\n      <td>52574692.0</td>\n      <td>11933512.0</td>\n      <td>31632344.0</td>\n      <td>81693475.0</td>\n      <td>16677568.0</td>\n      <td>16766967.0</td>\n      <td>138180630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:50:00</th>\n      <td>8716795.6</td>\n      <td>22660870.0</td>\n      <td>6240626.4</td>\n      <td>5657380.6</td>\n      <td>17406086.0</td>\n      <td>8808588.5</td>\n      <td>15962917.0</td>\n      <td>18367639.0</td>\n      <td>7767967.3</td>\n      <td>7470650.1</td>\n      <td>...</td>\n      <td>65599891.0</td>\n      <td>25862152.0</td>\n      <td>11673804.0</td>\n      <td>60086953.0</td>\n      <td>11851656.0</td>\n      <td>30979811.0</td>\n      <td>73577193.0</td>\n      <td>19167646.0</td>\n      <td>19402758.0</td>\n      <td>137288810.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1008 rows × 121 columns</p>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: {#b59702bf .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nAtraf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(1008, 121)\n```\n:::\n:::\n\n\nAs we would expect, our traffic matrix has rank 121:\n\n::: {#2f64edb9 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nnp.linalg.matrix_rank(Atraf)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nnp.int64(121)\n```\n:::\n:::\n\n\nHowever -- perhaps it has low __effective__ rank.\n\nThe `numpy` routine for computing the SVD is `np.linalg.svd`:\n\n::: {#7f95d304 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(Atraf)\n```\n:::\n\n\n---\n\nNow let's look at the singular values of `Atraf` to see if it can be usefully approximated as a low-rank matrix:\n\n::: {#1f4f1f78 .cell execution_count=7}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nplt.plot(range(1, 1+len(s)), s)\nplt.xlabel(r'$k$', size=20)\nplt.ylabel(r'$\\sigma_k$', size=20)\nplt.ylim(ymin=0)\nplt.xlim(xmin=-1)\nplt.title(r'Singular Values of $A$', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-7-output-1.png){width=453 height=324}\n:::\n:::\n\n\nThis classic, sharp-elbow tells us that a few singular values are very large, and most singular values are quite small.\n\n---\n\nZooming in for just small $k$ values, we can see that the elbow is around 4 - 6 singular values:\n\n::: {#12e3e42a .cell execution_count=8}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nplt.plot(range(1, 21), s[0:20]/Anorm, '.-')\nplt.xlim([0.5, 20])\nplt.ylim([0, 1])\nplt.xlabel(r'$k$', size=20)\nplt.xticks(range(1, 21))\nplt.ylabel(r'$\\sigma_k$', size=20);\nplt.title(r'Singular Values of $A$',size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-8-output-1.png){width=462 height=324}\n:::\n:::\n\n\nThis pattern of singular values suggests __low effective rank.__\n\n---\n\nLet's use the formula above to compute the relative error of a rank-$k$ approximation to $A$:\n\n::: {#70ae6ff6 .cell execution_count=9}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nerr = np.cumsum(s[::-1]**2)\nerr = np.sqrt(err[::-1])\nplt.plot(range(0, 20), err[:20]/Anorm, '.-')\nplt.xlim([0, 20])\nplt.ylim([0, 1])\nplt.xticks(range(1, 21))\nplt.xlabel(r'$k$', size = 16)\nplt.ylabel(r'relative F-norm error', size=16)\nplt.title(r'Relative Error of rank-$k$ approximation to $A$', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-9-output-1.png){width=492 height=315}\n:::\n:::\n\n\nRemarkably, we are down to 9% relative error using only a rank 20 approximation to $A$.\n\n---\n\nSo instead of storing \n\n* $mn =$ (1008 $\\cdot$ 121) = 121,968 values, \n\nwe only need to store \n\n* $k(m+n)$ = 20 $\\cdot$ (1008 + 121) = 22,580 values, \n\nwhich is an 81% reduction in size.\n\n## Low Effective Rank is Common\n\nIn practice __many__ datasets have low effective rank.   \n\nWe consider the following examples:\n\n:::: {.incremental}\n* Likes on Facebook,\n* Yelp reviews and Tweets (the site formerly known as twitter),\n* User preferences over time,\n* Images.\n::::\n\n## Likes on Facebook\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of likes:  Timebins $\\times$ Users\n2. Number of likes:  Users $\\times$ Page Categories\n3. Entropy of likes across categories:  Timebins $\\times$ Users\n::::\n\n:::: {.fragment}\n![](figs/L10-facebook.png){fig-align=\"center\"}\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n## Social Media Activity\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of Yelp reviews:  Timebins $\\times$ Users\n2. Number of Yelp reviews:  Users $\\times$ Yelp Categories\n3. Number of Tweets:  Users $\\times$ Topic Categories\n::::\n\n:::: {.fragment}\n![](figs/L10-yelp-twitter.png)\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n\n## Netflix\n\nExample: the Netflix prize worked with partially-observed matrices like this:\n\n$$\n\\begin{bmatrix}\n & & & \\vdots & & & \\\\\n & & 3 & 2 & & 1 &\\\\\n & 1 & & 1 & & & \\\\\n\\dots & & 2 & & 4 & & \\dots\\\\\n & 5 & 5 & & 4 & & \\\\\n & 1 & & & 1 & 5 & \\\\\n & & & \\vdots & & & \\\\\n\\end{bmatrix},\n$$\n\n:::: {.fragment}\nwhere the rows correspond to users, the columns to movies, and the entries are ratings.\n::::\n\n:::: {.fragment}\nAlthough the problem matrix was of size 500,000 $\\times$ 18,000, the winning approach modeled the matrix as having __rank 20 to 40.__\n\nSource: [Koren et al, IEEE Computer, 2009]\n::::\n\n\n## Images\n\nImage data often shows low effective rank.\n\nFor example, here is an original photo:\n\n::: {#bf814297 .cell execution_count=10}\n``` {.python .cell-code}\nboat = np.loadtxt('data/images/boat/boat.dat')\nimport matplotlib.cm as cm\nplt.figure()\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-10-output-1.png){width=389 height=389}\n:::\n:::\n\n\n---\n\nLet's look at the singular values.\n\n::: {#6e5bdbc8 .cell execution_count=11}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\nplt.plot(s)\nplt.xlabel('$k$', size=16)\nplt.ylabel(r'$\\sigma_k$', size=16)\nplt.title('Singular Values of Boat Image', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-11-output-1.png){width=841 height=462}\n:::\n:::\n\n\n---\n\nThis image is 512 $\\times$ 512. As a matrix, it has rank of 512.   \n\nBut its _effective_ rank is low.\n\nBased on the previous plot, its effective rank is perhaps 40.\n\nLet's find the closest rank-40 matrix and view it.\n\n::: {#a0794c8e .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\ns[40:] = 0\nboatApprox = u @ np.diag(s) @ vt\n```\n:::\n\n\n::: {#f7bc8961 .cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(9, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(boatApprox, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 40 Boat')\nplt.subplot(1, 2, 2)\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 512 Boat')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-13-output-1.png){width=689 height=344}\n:::\n:::\n\n\n## Interpretations of Low Effective Rank\n\nHow can we understand the low-effective-rank phenomenon in general?\n\n:::: {.fragment}\nThere are two helpful interpretations:\n\n:::: {.incremental}\n1. Common Patterns\n2. Latent Factors\n::::\n::::\n\n## Low Rank Implies Common Patterns\n\nThe first interpretation of low-rank behavior is in answering the question:\n\n:::: {.fragment}\n\"What is the strongest pattern in the data?\"\n::::\n\n:::: {.fragment}\nRemember that using the SVD we form the low-rank approximation as\n\n$$ A^{(k)} =  U'\\Sigma'(V')^T$$\n\nand\n\n:::: {.incremental}\n\n- $U'$ are the $k$ leftmost columns of $U$, \n- $\\Sigma'$ is the $k\\times k$ upper left submatrix of $\\Sigma$, and \n- $V'$ are the $k$ leftmost columns of $V$.\n::::\n::::\n \n:::: {.fragment}\nIn this interpretation, we think of each column of $A^{(k)}$ as a combination of the columns of $U'$.\n::::\n\n:::: {.fragment}\nHow can this be helpful? \n::::\n\n\n## Common Patterns: Traffic Example\n\nConsider the set of traffic traces. There are clearly some common patterns. How can we find them?\n\n::: {#624de08b .cell execution_count=14}\n``` {.python .cell-code}\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq='10min', periods=1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nplt.figure(figsize=(10, 8))\nfor i in range(1, 13):\n    ax = plt.subplot(4, 3, i)\n    Atraf.iloc[:, i-1].plot()\n    plt.title(odnames[i])\nplt.subplots_adjust(hspace=1)\nplt.suptitle('Twelve Example Traffic Traces', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-14-output-1.png){width=781 height=740}\n:::\n:::\n\n\n---\n\nLet's use as our example $\\mathbf{a}_1,$ the first column of $A$.\n\nThis happens to be the ATLA-CHIN flow.\n\nThe equation above tells us that\n\n$$\\mathbf{a}_1 \\approx v_{11}\\sigma_1\\mathbf{u}_1 + v_{12}\\sigma_2\\mathbf{u}_2 + \\dots + v_{1k}\\sigma_k\\mathbf{u}_k.$$\n\nIn other words, $\\mathbf{u}_1$ (the first column of $U$) is the \"strongest\" pattern occurring in $A$, and its strength is measured by $\\sigma_1$.\n\n---\n\nHere is a view of the first 2 columns of $U\\Sigma$ for the traffic matrix data.\n\nThese are the strongest patterns occurring across all of the 121 traces.\n\n::: {#8783db6e .cell execution_count=15}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(Atraf, full_matrices=False)\nuframe = pd.DataFrame(u @ np.diag(s), index=pd.date_range('9/1/2003', freq = '10min', periods = 1008))\nuframe[0].plot(color='r', label='Column 1')\nuframe[1].plot(label='Column 2')\nplt.legend(loc='best')\nplt.title('First Two Columns of $U$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-15-output-1.png){width=792 height=461}\n:::\n:::\n\n\n## Low Rank Defines Latent Factors\n\nThe next interpretation of low-rank behavior is that it exposes \"latent factors\" that describe the data.\n\n:::: {.fragment}\nIn this interpretation, we think of each element of $A^{(k)}=U'\\Sigma'(V')^T$ as the inner product of a row of $U'\\Sigma'$ and a column of $(V')^{T}$ (equivalently a row of $V'$).\n::::\n\n:::: {.fragment}\nLet's say we are working with a matrix of users and items.\n::::\n\n:::: {.fragment}\nIn particular, let the items be movies and matrix entries be ratings, as in the Netflix prize.\n::::\n\n## Latent Factors: Netflix example\n\nRecall the structure from earlier:\n\n$$\n\\begin{bmatrix}\n\\vdots & \\vdots &  & \\vdots \\\\\n\\mathbf{a}_{1} & \\mathbf{a}_{2} & \\cdots & \\mathbf{a}_{n} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\n\\end{bmatrix} \n\\approx\n\\underbrace{\n\\begin{bmatrix}\n\\vdots &  \\vdots  \\\\\n\\sigma_1 \\mathbf{u}_1 & \\sigma_k \\mathbf{u}_{k} \\\\\n\\vdots& \\vdots  \\\\\n\\end{bmatrix}\n}_{\\tilde{U}\\in\\mathbb{R}^{m\\times k}}\n\\underbrace{\n\\begin{bmatrix}\n\\cdots & \\mathbf{v}_{1}^{T} & \\cdots   \\\\\n\\cdots & \\mathbf{v}_{k}^{T} & \\cdots   \\\\\n\\end{bmatrix}\n}_{\\tilde{V}\\in\\mathbb{R}^{k\\times n}},\n$$\n\nwhere the rows of $A$ are the users and the columns are movie ratings.\n\nThen the rating that a user gives a movie is the inner product of a $k$-element vector that corresponds to the user, and a $k$-element vector that corresponds to the movie.\n\nIn other words we have:\n    \n$$ \na_{ij} = \\sum_{p=1}^{k} \\tilde{U}_{ip} \\tilde{V}_{pj}.\n$$\n\n---\n\nWe can think of user $i$'s preferences as being captured by row $i$ of $\\tilde{U}$, which is a point in $\\mathbb{R}^k$.  \n\n:::: {.fragment}\nWe have described everything we need to know to predict user $i$'s ratings via a $k$-element vector.\n::::\n\n:::: {.fragment}\nThe $k$-element vector is called a __latent factor.__\n::::\n\n:::: {.fragment}\nLikewise, we can think of column $j$ of $\\tilde{V}$ as a \"description\" of movie $j$ (another latent factor).\n::::\n\n:::: {.fragment}\nThe value in using latent factors comes from the summarization of user preferences, and the predictive power one obtains.\n::::\n\n:::: {.fragment}\nFor example, the winning entry in the Netflix prize competition modeled user preferences with 20 latent factors.\n::::\n\n:::: {.fragment}\nThe remarkable thing is that a person's preferences for all 18,000 movies can be reasonably well captured in a vector of dimension 20!\n::::\n\n---\n\nHere is a figure from the paper that described the winning strategy in the Netflix prize.\n\nIt shows a hypothetical __latent space__ in which each user, and each movie, is represented by a latent vector.\n\n![](figs/L10-Movie-Latent-Space.png)\n\nSource: Koren et al, IEEE Computer, 2009 \n\nIn practice, this is perhaps a 20- or 40-dimensional space.\n\n---\n\nHere are some representations of movies in that space (reduced to 2-D).\n\nNotice how the space seems to capture similarity among movies!\n\n![](figs/L10-Netflix-Latent-Factors.png)\n\nSource: Koren et al, IEEE Computer, 2009 \n\n## Summary\n\n:::: {.incremental}\n* When we are working with data matrices, it is valuable to consider the __effective rank__.\n* Many (many) datasets in real life show __low effective rank__.\n* This property can be explored precisely using the Singular Value Decomposition of the matrix.\n* When low effective rank is present\n    * the matrix can be compressed with only small loss of accuracy,\n    * we can extract the *strongest* patterns in the data,\n    * we can describe each data item in terms of the inner product of __latent factors__.\n::::\n\n",
>>>>>>> main
    "supporting": [
      "10-Low-Rank-and-SVD_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}