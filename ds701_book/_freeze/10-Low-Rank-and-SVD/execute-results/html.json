{
  "hash": "ba60792938dbf77313357febc07f3875",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Low Rank Approximation and the SVD\njupyter: python3\n---\n\n\nToday, we move on.   \n\nHowever, let's look back and try to put the modeling we've done into a larger context.\n\n## Models are simplifications\n\nOne way of thinking about modeling or clustering is that we are building a __simplification__ of the data. \n\nThat is, a model is a description of the data, that is simpler than the data.\n\nIn particular, instead of thinking of the data as thousands or millions of individual data points, we might think of it in terms of a small number of clusters, or a parametric distribution, etc, etc.\n\nFrom this simpler description, we hope to gain __insight.__\n\nThere is an interesting question here:  __why__ does this process often lead to insight?   \n\nThat is, why does it happen so often that a large dataset can be described in terms of a much simpler model?\n\nI don't know.\n\n<center>\n    \n<img src=\"figs/L10-William-of-Ockham.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nBy self-created (Moscarlop) - Own work, <a href=\"http://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=5523066\">Link</a>\n\nHowever, I think that William of Ockham (c. 1300 AD) was on the right track.\n\nHe said:\n\n> Non sunt multiplicanda entia sine necessitate\n\nor, in other words:\n\n> Entities must not be multiplied beyond necessity.\n\nby which he meant:\n\n> Among competing hypotheses, the one with the fewest assumptions should be selected.\n\nWhich has come to be known as \"Occam's razor.\"\n\nWilliam was saying that it is more common for a set of observations to be determined by a simple process than a complex process.\n\nIn other words, the world is full of simple (but often hidden) patterns.\n\nFrom which one can justify the observation that \"modeling works suprisingly often.\"\n\n## Data Matrices\n\nNow we'll consider a (seemingly) very different approximation of data, applicable to data when it is in matrix form.\n\n$${\\mbox{$m$ data objects}}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^{\\mbox{$n$ features}}$$\n\n<table>\n<tr><th>Data Type</th><th>Rows</th><th>Columns</th><th>Elements</th></tr>\n<tr><td>Network Traffic<td>Sources</td><td>Destinations</td><td>Number of Bytes</td></tr>\n<tr><td>Social Media</td><td>Users</td><td>time bins</td><td>Number of Posts/Tweets/Likes</td></tr>\n<tr><td>Web Browsing</td><td>Users</td><td>Content Categories</td><td>Visit Counts/Bytes Downloaded</td></tr>\n<tr><td>Web Browsing</td><td>Users</td><td>time bins</td><td>Visit Counts/Bytes Downloaded</td></tr>\n</table>\n\n## Matrix Rank\n\nLet's briefly review some definitions.\n\nWe'll consider an $m\\times n$ real matrix $A$.\n\nThe __rank__ of $A$ is the __dimension of its column space.__   \n\nThe dimension of a space is the smallest number of (linearly independent) vectors needed to span the space.\n\nSo the dimension of the column space of $A$ is the __smallest number of vectors that suffice to construct the columns of $A$.__\n\nThen the rank of $A$ is the size of the smallest set $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_p\\}$ such that every column $\\mathbf{a}_i$ can be expressed as:\n\n$$\\mathbf{a}_i = c_{i1}\\mathbf{u}_1 + c_{i2}\\mathbf{u}_2 + \\dots + c_{ip}\\mathbf{u}_p\\;\\;\\;\\;i=1,\\dots,n.$$\n\nThe largest value that a matrix rank can take is $\\min(m,n)$.\n\nHowever it can happen that the rank of a matrix is __less__ than $\\min(m,n)$.\n\nNow to store a matrix $A \\in \\mathbb{R}^{m\\times n}$ we need to store $m n$ values.\n\nHowever, if $A$ has rank $k$, it can be factored as $A = UV$,\n\nwhere $U \\in \\mathbb{R}^{m\\times k}$ and $V \\in \\mathbb{R}^{k \\times n}$.\n\nThis only requires $k(m+n)$ values, which could be much smaller than $mn$.\n\n$$ \\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c}a_{11}\\\\\\vdots\\\\a_{i1}\\\\\\vdots\\\\a_{m1}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1j}\\\\\\vdots\\\\a_{ij}\\\\\\vdots\\\\a_{mj}\\end{array}&\n\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n\\begin{array}{c}a_{1n}\\\\\\vdots\\\\a_{in}\\\\\\vdots\\\\a_{mn}\\end{array}\n\\end{array}\\right]}^\\mbox{$n$ features} =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\mathbf{u}_1&\\mathbf{u}_k\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\large k}\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\mathbf{v}_1&\\dots&\\dots\\\\\\dots&\\dots&\\mathbf{v}_k&\\dots&\\dots\\end{array}\\right]$$\n\n## Low Effective Rank\n\nIn many situations we may wish to __approximate__ a data matrix $A$ with a low-rank matrix $A^{(k)}.$\n\nTo talk about when one matrix \"approximates\" another, we need a norm for matrices.  \n\nWe will use the __Frobenius norm__ which is just the usual $\\ell_2$ norm, treating the matrix as a vector.\n\nThe definition of the Frobenius norm of $A$, denoted $\\Vert A\\Vert_F$, is:\n\n$$\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.$$\n\nTo quantify when one matrix is \"close\" to another, we use distance in Euclidean space:\n\n$$\\mbox{dist}(A,B) = \\Vert A-B\\Vert_F.$$\n\n(where the Euclidean space is the $mn$-dimensional space of $m\\times n$ matrices.)\n\nNow we can define the __rank-$k$ approximation__ to $A$:\n\nWhen $k < \\operatorname{Rank} A$, the rank-$k$ approximation to $A$ is the closest rank-$k$ matrix to $A$, i.e., \n\n$$A^{(k)} =\\arg \\min_{\\{B\\;|\\;\\operatorname{Rank} B = k\\}} \\Vert A-B\\Vert_F.$$\n\nThis can also be considered the best rank-$k$ approximation to $A$ in a least-squares sense.\n\nLet's say we have $A^{(k)}$, a rank-$k$ approximation to $A$.  \n\nBy definition, there is a set $\\mathcal{U}$ consisting of $k$ vectors such that each column of $A^{(k)}$ can be expressed as a linear combination of vectors in $\\mathcal{U}$.   \n\nLet us  call the matrix formed by those vectors $U$.\n\nSo \n\n$$A^{(k)} = UV^T$$\n\nfor some set of coefficients $V^T$ that describe the linear combinations of $U$ that yield the columns of $A^{(k)}$. \n\nSo $U$ is $m\\times k$ and $V$ is $n\\times k$.\n\nIf we approximate $A$ by $A^{(k)}$, then the error we incur is:\n\n$$\\Vert A-A^{(k)}\\Vert_F.$$\n\nHence, a rank-$k$ approximation $A^{(k)}$ is valuable if \n\n* $\\Vert A-A^{(k)}\\Vert_F$ is small compared to $\\Vert A\\Vert_F$, and \n* $k$ is small compared to $m$ and $n$.\n\nIn that case we have achieved a simplification of the data without a great loss in accuracy.\n\n## Finding Rank-$k$ Approximations\n\nThere is a celebrated method for finding the best rank-$k$ approximation to any matrix: the __Singular Value Decomposition (SVD).__\n\n> SVD is \"the Rolls-Royce and the Swiss Army Knife of Numerical Linear Algebra.”\n\nDianne O’Leary, MMDS ’06\n\nThe singular value decomposition of a rank-$r$ matrix $A$ has the form:\n\n$$A = U\\Sigma V^T$$\n\nwhere \n\n1. $U$ is $m\\times r$\n2. The columns of $U$ are mutually orthogonal and unit length, ie., $U^TU = I$.\n3. $V$ is $n\\times r$.\n4. The columns of $V$ are mutually orthogonal and unit length, ie., $V^TV = I$.\n5. The matrix $\\Sigma$ is an $r\\times r$ diagonal matrix, whose diagonal values are $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > 0$.\n\n    \n$$ \\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right] =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\mathbf{u}_1&\\mathbf{u}_r\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\large r}\n\\times\n\\left[\\begin{array}{cc}\\sigma_1&~\\\\~&\\sigma_r\\\\\\end{array}\\right]\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\mathbf{v}_1&\\dots&\\dots\\\\\\dots&\\dots&\\mathbf{v}_r&\\dots&\\dots\\end{array}\\right]$$\n\nNpw, the SVD is _incredibly useful_ for finding matrix approximations.\n\nIn particular, for an $m\\times n$ matrix $A$, the SVD does two things:\n\n1. It gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n2. It gives the __distance__ of the best approximation $A^{(k)}$ from $A$ for each $k$.\n\nIn terms of the singular value decomposition, \n\nThe best rank-$k$ approximation to $A$ is formed by taking \n\n   * $U' = $ the $k$ leftmost columns of $U$, \n   * $\\Sigma' = $ the $k\\times k$ upper left submatrix of $\\Sigma$, and \n   * $V'= $ the $k$ leftmost columns of $V$, and constructing \n\n$$ A^{(k)} = U'\\Sigma'(V')^T.$$\n\n\nFurthermore, the distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^r\\sigma^2_i}$.\n\nThat is, if you construct $A^{(k)}$ as shown above, then:\n\n$$\\Vert A-A^{(k)}\\Vert_F^2 = \\sum_{i=k+1}^r\\sigma^2_i$$\n\n## Low Effective Rank\n\nAlmost any data matrix $A$ that one encounters will usually be __full rank__,\n\nmeaning that $\\operatorname{Rank} A = \\min(m, n)$.\n\nHowever, it is often the case that data matrices have __low effective rank.__\n\nBy this we mean that one can usefully approximate $A$ by some $A^{(k)}$ for which $k \\ll \\min(m,n)$.\n\nFor any data matrix, we can judge when this is the case by looking at its singular values, because the singular values tell us the distance to the nearest rank-$k$ matrix.\n\n## Empirical Evidence\n\nLet's see how this theory can be used in practice, and investigate some real data.\n\nWe'll look at data traffic on the Abilene network:\n\n<img src='figs/L10-Abilene-map.png'>\n\nSource: Internet2, circa 2005\n\n::: {#a8ff7c77 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=2}\n``` {.python .cell-code}\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq = '10min', periods = 1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nAtraf\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ATLA-ATLA</th>\n      <th>ATLA-CHIN</th>\n      <th>ATLA-DNVR</th>\n      <th>ATLA-HSTN</th>\n      <th>ATLA-IPLS</th>\n      <th>ATLA-KSCY</th>\n      <th>ATLA-LOSA</th>\n      <th>ATLA-NYCM</th>\n      <th>ATLA-SNVA</th>\n      <th>ATLA-STTL</th>\n      <th>...</th>\n      <th>WASH-CHIN</th>\n      <th>WASH-DNVR</th>\n      <th>WASH-HSTN</th>\n      <th>WASH-IPLS</th>\n      <th>WASH-KSCY</th>\n      <th>WASH-LOSA</th>\n      <th>WASH-NYCM</th>\n      <th>WASH-SNVA</th>\n      <th>WASH-STTL</th>\n      <th>WASH-WASH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2003-09-01 00:00:00</th>\n      <td>8466132.0</td>\n      <td>29346537.0</td>\n      <td>15792104.0</td>\n      <td>3646187.0</td>\n      <td>21756443.0</td>\n      <td>10792818.0</td>\n      <td>14220940.0</td>\n      <td>25014340.0</td>\n      <td>13677284.0</td>\n      <td>10591345.0</td>\n      <td>...</td>\n      <td>53296727.0</td>\n      <td>18724766.0</td>\n      <td>12238893.0</td>\n      <td>52782009.0</td>\n      <td>12836459.0</td>\n      <td>31460190.0</td>\n      <td>105796930.0</td>\n      <td>13756184.0</td>\n      <td>13582945.0</td>\n      <td>120384980.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:10:00</th>\n      <td>20524567.0</td>\n      <td>28726106.0</td>\n      <td>8030109.0</td>\n      <td>4175817.0</td>\n      <td>24497174.0</td>\n      <td>8623734.0</td>\n      <td>15695839.0</td>\n      <td>36788680.0</td>\n      <td>5607086.0</td>\n      <td>10714795.0</td>\n      <td>...</td>\n      <td>68413060.0</td>\n      <td>28522606.0</td>\n      <td>11377094.0</td>\n      <td>60006620.0</td>\n      <td>12556471.0</td>\n      <td>32450393.0</td>\n      <td>70665497.0</td>\n      <td>13968786.0</td>\n      <td>16144471.0</td>\n      <td>135679630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:20:00</th>\n      <td>12864863.0</td>\n      <td>27630217.0</td>\n      <td>7417228.0</td>\n      <td>5337471.0</td>\n      <td>23254392.0</td>\n      <td>7882377.0</td>\n      <td>16176022.0</td>\n      <td>31682355.0</td>\n      <td>6354657.0</td>\n      <td>12205515.0</td>\n      <td>...</td>\n      <td>67969461.0</td>\n      <td>37073856.0</td>\n      <td>15680615.0</td>\n      <td>61484233.0</td>\n      <td>16318506.0</td>\n      <td>33768245.0</td>\n      <td>71577084.0</td>\n      <td>13938533.0</td>\n      <td>14959708.0</td>\n      <td>126175780.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:30:00</th>\n      <td>10856263.0</td>\n      <td>32243146.0</td>\n      <td>7136130.0</td>\n      <td>3695059.0</td>\n      <td>28747761.0</td>\n      <td>9102603.0</td>\n      <td>16200072.0</td>\n      <td>27472465.0</td>\n      <td>9402609.0</td>\n      <td>10934084.0</td>\n      <td>...</td>\n      <td>66616097.0</td>\n      <td>43019246.0</td>\n      <td>12726958.0</td>\n      <td>64027333.0</td>\n      <td>16394673.0</td>\n      <td>33440318.0</td>\n      <td>79682647.0</td>\n      <td>16212806.0</td>\n      <td>16425845.0</td>\n      <td>112891500.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:40:00</th>\n      <td>10068533.0</td>\n      <td>30164311.0</td>\n      <td>8061482.0</td>\n      <td>2922271.0</td>\n      <td>35642229.0</td>\n      <td>9104036.0</td>\n      <td>12279530.0</td>\n      <td>29171205.0</td>\n      <td>7624924.0</td>\n      <td>11327807.0</td>\n      <td>...</td>\n      <td>66797282.0</td>\n      <td>40408580.0</td>\n      <td>11733121.0</td>\n      <td>54541962.0</td>\n      <td>16769259.0</td>\n      <td>33927515.0</td>\n      <td>81480788.0</td>\n      <td>16757707.0</td>\n      <td>15158825.0</td>\n      <td>123140310.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:10:00</th>\n      <td>8849096.0</td>\n      <td>33461807.0</td>\n      <td>5866138.0</td>\n      <td>3786793.0</td>\n      <td>19097140.0</td>\n      <td>10561532.0</td>\n      <td>26092040.0</td>\n      <td>28640962.0</td>\n      <td>8343867.0</td>\n      <td>8820650.0</td>\n      <td>...</td>\n      <td>65925313.0</td>\n      <td>21751316.0</td>\n      <td>11058944.0</td>\n      <td>58591021.0</td>\n      <td>17137907.0</td>\n      <td>24297674.0</td>\n      <td>83293655.0</td>\n      <td>17329425.0</td>\n      <td>20865535.0</td>\n      <td>123125390.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:20:00</th>\n      <td>9776675.0</td>\n      <td>31474607.0</td>\n      <td>5874654.0</td>\n      <td>11277465.0</td>\n      <td>14314837.0</td>\n      <td>9106198.0</td>\n      <td>26412752.0</td>\n      <td>26168288.0</td>\n      <td>8638782.0</td>\n      <td>9193717.0</td>\n      <td>...</td>\n      <td>70075490.0</td>\n      <td>29126443.0</td>\n      <td>12667321.0</td>\n      <td>54571764.0</td>\n      <td>15383038.0</td>\n      <td>25238842.0</td>\n      <td>70015955.0</td>\n      <td>16526455.0</td>\n      <td>16881206.0</td>\n      <td>142106800.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:30:00</th>\n      <td>9144621.0</td>\n      <td>32117262.0</td>\n      <td>5762691.0</td>\n      <td>7154577.0</td>\n      <td>17771350.0</td>\n      <td>10149256.0</td>\n      <td>29501669.0</td>\n      <td>25998158.0</td>\n      <td>11343171.0</td>\n      <td>9423042.0</td>\n      <td>...</td>\n      <td>68544458.0</td>\n      <td>27817836.0</td>\n      <td>15892668.0</td>\n      <td>50326213.0</td>\n      <td>12098328.0</td>\n      <td>27689197.0</td>\n      <td>73553203.0</td>\n      <td>18022288.0</td>\n      <td>18471915.0</td>\n      <td>127918530.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:40:00</th>\n      <td>8802106.0</td>\n      <td>29932510.0</td>\n      <td>5279285.0</td>\n      <td>5950898.0</td>\n      <td>20222187.0</td>\n      <td>10636832.0</td>\n      <td>19613671.0</td>\n      <td>26124024.0</td>\n      <td>8732768.0</td>\n      <td>8217873.0</td>\n      <td>...</td>\n      <td>65087776.0</td>\n      <td>28836922.0</td>\n      <td>11075541.0</td>\n      <td>52574692.0</td>\n      <td>11933512.0</td>\n      <td>31632344.0</td>\n      <td>81693475.0</td>\n      <td>16677568.0</td>\n      <td>16766967.0</td>\n      <td>138180630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-07 23:50:00</th>\n      <td>8716795.6</td>\n      <td>22660870.0</td>\n      <td>6240626.4</td>\n      <td>5657380.6</td>\n      <td>17406086.0</td>\n      <td>8808588.5</td>\n      <td>15962917.0</td>\n      <td>18367639.0</td>\n      <td>7767967.3</td>\n      <td>7470650.1</td>\n      <td>...</td>\n      <td>65599891.0</td>\n      <td>25862152.0</td>\n      <td>11673804.0</td>\n      <td>60086953.0</td>\n      <td>11851656.0</td>\n      <td>30979811.0</td>\n      <td>73577193.0</td>\n      <td>19167646.0</td>\n      <td>19402758.0</td>\n      <td>137288810.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1008 rows × 121 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#56d0c960 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=3}\n``` {.python .cell-code}\nAtraf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(1008, 121)\n```\n:::\n:::\n\n\nAs we would expect, our traffic matrix has rank 121:\n\n::: {#21942d5f .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=4}\n``` {.python .cell-code}\nnp.linalg.matrix_rank(Atraf)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n121\n```\n:::\n:::\n\n\nHowever -- perhaps it has low __effective__ rank.\n\nThe `numpy` routine for computing SVD is `np.linalg.svd`:\n\n::: {#62e24701 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=5}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(Atraf)\n```\n:::\n\n\nNow let's look at the singular values of `Atraf` to see if it can be usefully approximated as a low-rank matrix:\n\n::: {#80207de0 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(6,4))\nplt.plot(range(1,1+len(s)),s)\nplt.xlabel(r'$k$',size=20)\nplt.ylabel(r'$\\sigma_k$',size=20)\nplt.ylim(ymin = 0)\nplt.xlim(xmin = -1)\nplt.title(r'Singular Values of $A$',size=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-7-output-1.png){width=527 height=398}\n:::\n:::\n\n\nThis classic, sharp-elbow tells us that a few singular values are very large, and most singular values are quite small.\n\nZooming in for just small $k$ values, we can see that the elbow is around 4 - 6 singular values:\n\n::: {#e96d8128 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\nfig = plt.figure(figsize = (6, 4))\nAnorm = np.linalg.norm(Atraf)\nplt.plot(range(1, 21), s[0:20]/Anorm, '.-')\nplt.xlim([0.5, 20])\nplt.ylim([0, 1])\nplt.xlabel(r'$k$', size=20)\nplt.xticks(range(1, 21))\nplt.ylabel(r'$\\sigma_k$', size=20);\nplt.title(r'Singular Values of $A$',size=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-8-output-1.png){width=536 height=398}\n:::\n:::\n\n\nThis pattern of singular values suggests __low effective rank.__\n\nLet's use the formula above to compute the relative error of a rank-$k$ approximation to $A$:\n\n::: {#92e122ce .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nfig = plt.figure(figsize = (6, 4))\nAnorm = np.linalg.norm(Atraf)\nerr = np.cumsum(s[::-1]**2)\nerr = np.sqrt(err[::-1])\nplt.plot(range(0, 20), err[:20]/Anorm, '.-')\nplt.xlim([0, 20])\nplt.ylim([0, 1])\nplt.xticks(range(1, 21))\nplt.xlabel(r'$k$', size = 16)\nplt.ylabel(r'relative F-norm error', size=16)\nplt.title(r'Relative Error of rank-$k$ approximation to $A$', size=16);\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-9-output-1.png){width=531 height=389}\n:::\n:::\n\n\nRemarkably, we are down to 9% relative error using only a rank 20 approximation to $A$.\n\nSo instead of storing \n\n* $mn =$ (1008 $\\cdot$ 121) = 121,968 values, \n\nwe only need to store \n\n* $k(m+n)$ = 20 $\\cdot$ (1008 + 121) = 22,580 values, \n\nwhich is a 81% reduction in size.\n\n## Low Effective Rank is Common\n\nIn practice __many__ datasets have low effective rank.   \n\nHere are some more examples.\n\n__Likes on Facebook.__\n\nHere, the matrices are \n\n1. Number of likes:  Timebins $\\times$ Users\n2. Number of likes:  Users $\\times$ Page Categories\n3. Entropy of likes across categories:  Timebins $\\times$ Users\n\n<center>\n<img width=650, src = figs/L10-facebook.png/>\n</center>\n\nSource: [Viswanath et al., Usenix Security, 2014]\n\n__Social Media Activity.__\n\nHere, the matrices are \n\n1. Number of Yelp reviews:  Timebins $\\times$ Users\n2. Number of Yelp reviews:  Users $\\times$ Yelp Categories\n3. Number of Tweets:  Users $\\times$ Topic Categories\n\n<center>\n<img width=650, src = figs/L10-yelp-twitter.png/>\n</center>\n\nSource: [Viswanath et al., Usenix Security, 2014]\n\n__User preferences over items.__\n\nExample: the Netflix prize worked with partially-observed matrices like this:\n\n$$\\left[\\begin{array}{ccccccc}\n  ~&~&~&\\vdots&~&~&~\\\\\n  &~&3&2&~&1&\\\\\n  &1&~&1&~&~&\\\\\n  \\dots&~&2&~&4&~&\\dots\\\\\n  &5&5&~&4&~&\\\\\n  &1&~&~&1&5&\\\\\n  ~&~&~&\\vdots&~&~&~\\\\\n\\end{array}\n\\right]\n$$\n\nWhere the rows correspond to users, the columns to movies, and the entries are ratings.\n\nAlthough the problem matrix was of size 500,000 $\\times$ 18,000, the winning approach modeled the matrix as having __rank 20 to 40.__\n\nSource: [Koren et al, IEEE Computer, 2009]\n\n__Images.__\n\nImage data often shows low effective rank.\n\nFor example, here is an original photo:\n\n::: {#b8884388 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nboat = np.loadtxt('data/images/boat/boat.dat')\nimport matplotlib.cm as cm\nplt.figure()\nplt.imshow(boat,cmap = cm.Greys_r)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-10-output-1.png){width=389 height=389}\n:::\n:::\n\n\nLet's look at its spectrum:\n\n::: {#b303e227 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' execution_count=10}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(boat, full_matrices = False)\nplt.plot(s)\nplt.xlabel('$k$', size = 16)\nplt.ylabel(r'$\\sigma_k$', size = 16)\nplt.title('Singular Values of Boat Image', size = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-11-output-1.png){width=618 height=462}\n:::\n:::\n\n\nThis image is 512 $\\times$ 512.  As a matrix, it has rank of 512.   \n\nBut its _effective_ rank is low.\n\nBased on the plot above, its effective rank is perhaps 40.\n\nLet's find the closest rank-40 matrix and view it.\n\n::: {#565e09d3 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=11}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(boat, full_matrices = False)\ns[40:] = 0\nboatApprox = u @ np.diag(s) @ vt\n```\n:::\n\n\n::: {#5ea2ec04 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\n#\nplt.figure(figsize=(9,6))\nplt.subplot(1,2,1)\nplt.imshow(boatApprox,cmap = cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 40 Boat')\nplt.subplot(1,2,2)\nplt.imshow(boat,cmap = cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 512 Boat');\n# plt.subplots_adjust(wspace=0.5)\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-13-output-1.png){width=689 height=344}\n:::\n:::\n\n\n## Interpretations of Low Effective Rank\n\nHow can we understand the low-effective-rank phenomenon in general?\n\nThere are two helpful interpretations:\n\n1. Common Patterns\n2. Latent Factors\n\n### Low Rank Implies Common Patterns\n\nThe first interpretation of low-rank behavior is in answering the question:\n\n\"What is the strongest pattern in the data?\"\n\nUsing the SVD we form the low-rank approximation as\n\n   * $U' = $ the $k$ leftmost columns of $U$, \n   * $\\Sigma' = $ the $k\\times k$ upper left submatrix of $\\Sigma$, and \n   * $V'= $ the $k$ leftmost columns of $V$, and constructing \n   \nwith $$ A \\approx U'\\Sigma'(V')^T $$\n\nIn this interpretation, we think of each column of $A$ as a combination of the columns of $U'$.\n\nHow can this be helpful? \n\nConsider the set of traffic traces.   There are clearly some common patterns.  How can we find them?\n\n::: {#46a3332a .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003',freq='10min',periods=1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X',sep='  ',header=None,names=odnames,engine='python')\nAtraf.index = dates\nplt.figure(figsize=(10,8))\nfor i in range(1,13):\n    ax = plt.subplot(4,3,i)\n    Atraf.iloc[:,i-1].plot()\n    plt.title(odnames[i])\nplt.subplots_adjust(hspace=1)\nplt.suptitle('Twelve Example Traffic Traces', size=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-14-output-1.png){width=781 height=740}\n:::\n:::\n\n\nLet's use as our example $\\mathbf{a}_1,$ the first column of $A$.\n\nThis happens to be the ATLA-CHIN flow.\n\nThe equation above tells us that\n\n$$\\mathbf{a}_1 \\approx v_{11}\\sigma_1\\mathbf{u}_1 + v_{12}\\sigma_2\\mathbf{u}_2 + \\dots + v_{1k}\\sigma_k\\mathbf{u}_k.$$\n\nIn other words, $\\mathbf{u}_1$ (the first column of $U$) is the \"strongest\" pattern occurring in $A$, and its strength is measured by $\\sigma_1$.\n\nHere is an view of the first 2 columns of $U\\Sigma$ for the traffic matrix data.\n\nThese are the strongest patterns occurring across all of the 121 traces.\n\n::: {#791bb7ae .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(Atraf, full_matrices = False)\nuframe = pd.DataFrame(u @ np.diag(s), index=pd.date_range('9/1/2003', freq = '10min', periods = 1008))\nuframe[0].plot(color = 'r', label = 'Column 1')\nuframe[1].plot(label = 'Column 2')\nplt.legend(loc = 'best')\nplt.title('First Two Columns of $U$');\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-html/cell-15-output-1.png){width=569 height=461}\n:::\n:::\n\n\n### Low Rank Defines Latent Factors\n\nThe next interpretation of low-rank behavior is that it exposes \"latent factors\" that describe the data.\n\nReturning to the low-rank decomposition:\n\n$$ A \\approx U'\\Sigma'(V')^T $$\n\nIn this interpretation, we think of each element of $A$ as the inner product of a row of $U'\\Sigma'$ and a row of $V'$.\n\nLet's say we are working with a matrix of users and items.\n\nIn particular, let the items be movies and matrix entries be ratings, as in the Netflix prize.\n\nRecall the structure from a previous slide:\n\n$$ \\mbox{users}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\mbox{movies}} =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\sigma_1\\mathbf{u}_1&\\sigma_k\\mathbf{u}_k\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\large k}\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\mathbf{v}_1&\\dots&\\dots\\\\\\dots&\\dots&\\mathbf{v}_k&\\dots&\\dots\\end{array}\\right]$$\n\nThen the rating that a user gives a movie is the inner product of a $k$ element vector that corresponds to the user, and a $k$ element vector that corresponds to the movie.\n\nIn other words:\n    \n$$ a_{ij} = \\mathbf{u}_i^T \\mathbf{v}_j$$\n\nWe can therefore think of user $i$'s preferences as being captured by $\\mathbf{u}_i$, ie., a point in $\\mathbb{R}^k$.  \n\nWe have described everything we need to know to predict user $i$'s ratings via a $k$-element vector.\n\nThe $k$-element vector is called a __latent factor.__\n\nLikewise, we can think of $\\mathbf{v}_j$ as a \"description\" of movie $j$ (another latent factor).\n\nThe value in using latent factors comes from the summarization of user preferences, and the predictive power one obtains.\n\nFor example, the winning entry in the Netflix prize competition modeled user preferences with a 20-element latent factor.\n\nThe remarkable thing is that a person's preferences for all 18,000 movies can be reasonably well captured in a 20-element vector!\n\nHere is a figure from the paper that described the winning strategy in the Netflix prize.\n\nIt shows a hypothetical <font color = 'blue'>latent space</font> in which each user, and each movie, is represented by a latent vector.\n\n<center>\n    \n<img src=\"figs/L10-Movie-Latent-Space.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nSource: Koren et al, IEEE Computer, 2009 \n\nIn practice, this is perhaps a 20- or 40-dimensional space.\n\nHere are some representations of movies in that space (reduced to 2-D).\n\nNotice how the space seems to capture similarity among movies!\n\n<center>\n    \n<img src=\"figs/L10-Netflix-Latent-Factors.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nSource: Koren et al, IEEE Computer, 2009 \n\n## Summary\n\n* When we are working with data matrices, it is valuable to consider the __effective rank__\n* Many (many) datasets in real life show __low effective rank__.\n* This property can be explored precisely using the Singular Value Decomposition of the matrix.\n* When low effective rank is present,\n    * the matrix can be compressed with only small loss of accuracy\n    * we can extract the \"strongest\" patterns in the data\n    * we can describe each data item in terms of the inner product of __latent factors.__\n\n",
    "supporting": [
      "10-Low-Rank-and-SVD_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}