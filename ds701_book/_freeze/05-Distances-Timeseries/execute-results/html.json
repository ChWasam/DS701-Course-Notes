{
  "hash": "09978b1450a00918236e7d8cc625e7d3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Distances and Timeseries\njupyter: python3\n---\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)\n\nToday we will start building some tools for making comparisons of data objects, with particular attention to timeseries.\n\nWorking with data, we can encounter a wide variety of different data objects:\n\n::: {.incremental}\n* Records of users\n* Graphs\n* Images\n* Videos\n* Text (webpages, books)\n* Strings (DNA sequences)\n* Timeseries\n:::\n\n::: {.fragment}\nHow can we compare them?\n:::\n\n## Feature space representation\n\nUsually a data object consists of a set of attributes.\n\nThese are also commonly called __features.__\n\n* (\"J. Smith\", 25, \\$ 200,000)\n* (\"M. Jones\", 47, \\$ 45,000)\n\nIf all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.\n \n* `(25, USD 200000)` $\\rightarrow \\begin{bmatrix}25\\\\200000\\end{bmatrix}$.\n\nLikewise If all features are binary then we can think of each data object as a binary vector in vector space.\n\nThe space is called __feature space.__\n\n::: {.content-hidden when-profile=\"web\"}\n## One-hot encoding\n:::\nVector spaces are such a useful tool that we often use them even for non-numeric data.\n\nFor example, consider a categorical variable that can be only one of \"house\", \"tree\", or \"moon\". For such a variable, we can use a __one-hot__ encoding.  \n\n::: {.content-hidden when-profile=\"slides\"}\nWe would encode as follows:\n:::\n\n::: {.incremental}\n* `house`: $[1, 0, 0]$\n* `tree`:  $[0, 1, 0]$\n* `moon`:  $[0, 0, 1]$\n:::\n\n::: {.fragment}\nSo an encoding of `(25, USD 200000, 'house')` could be: \n$$\\begin{bmatrix}25\\\\200000\\\\1\\\\0\\\\0\\end{bmatrix}.$$\n::: \n\n::: {.content-hidden when-profile=\"web\"}\n## Encodings\n:::\n\nWe will see many other encodings that take non-numeric data and encode into vectors or matrices.\n\nFor example, there are vector or matrix encodings for:\n\n::: {.incremental}\n* Graphs\n* Images\n* Text \n:::\n\n## Metrics\n\nA metric is a function $d(x, y)$ that satisfies the following properties.\n\n::: {.incremental}\n* $d(i, j) = 0 \\leftrightarrow  i == j \\quad\\quad$ (identity of indiscernables)\n* $d(i, j) = d(j, i) \\qquad\\qquad~~$ (symmetry)\n* $d(i, j)\\leq d(i, h) + d(h, j) ~~$ (triangle inequality)\n:::\n\n::: {.fragment}\nWe can use a metric to determine how __similar__ or __dissimilar__ two objects are.\n\nA dissimilarity function takes two objects as input, and returns a large value when the two objects are not very similar.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Triangle inequality\n:::\n![](figs/TriangleInequality.png)\n    \n[Source](https://commons.wikimedia.org/w/index.php?curid=26047092)\n\n::: {.content-hidden when-profile=\"web\"}\n## Distances\n:::\n\nA metric is also commonly called a __distance__.\n\nSometimes we will use \"distance\" informally, i.e, to refer to a dissimilarity function even if we are not sure it is a metric.   \n\nWe'll try to say \"dissimilarity\" in those cases though.\n\n::: {.fragment}\nWhy is it important or valuable for a dissimilarity to be a metric?\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Distances continued\n:::\n\n::: {.incremental}\n- The additional constraints allow us to reason about and more easily visualize the data.\n- The main way this happens is through the triangle inequality.   \n- The triangle inequality basically says, if two objects are \"close\" to another object, then they are \"close\" to each other.  \n- This is not always the case for real data, but when it is true, it can really help.\n- Definitions of distance or dissimilarity functions are usually\ndiferent for real, boolean, categorical, and ordinal\nvariables.\n- Weights may be associated with diferent variables\nbased on applications and data semantics.\n:::\n\n## Matrix representation\n\nVery often we will manage data conveniently in matrix form.\n\nThe standard way of doing this is:\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{i1} \\\\ \\vdots \\\\ x_{m1} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1j} \\\\ \\vdots \\\\ x_{ij} \\\\ \\vdots \\\\ x_{mj} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1n} \\\\ \\vdots \\\\ x_{in} \\\\ \\vdots \\\\ x_{mn} \\end{array}\n\\end{array}\\right]}^{\\mbox{$n$ features}} \n$$\n\nWhere we typically use symbols $m$ for number of rows (objects) and $n$ for number of columns (features).\n\n::: {.content-hidden when-profile=\"web\"}\n## Matrix represntation of distances\n:::\n\nWhen we are working with distances, the matrix representation is:\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} 0  \\\\  d(1,2) \\\\ d(1,3) \\\\ \\vdots \\\\ d(1,m)  \\end{array} &\n\\begin{array}{c} \\; \\\\  0      \\\\ d(2,3) \\\\ \\vdots \\\\ d(2,m)  \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ 0      \\\\ \\vdots \\\\ \\cdots   \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\[-1pt] \\ddots \\\\[-1pt] \\cdots   \\end{array}  &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\;     \\\\[4pt] d(m, n) \\end{array} &\n\\end{array}\\right]}^{\\mbox{$n$ data features}}\n$$\n\n## Norms\n\nAssume some function $p(\\mathbf{v})$ which measures the \"size\" of the vector $\\mathbf{v}$.\n\n$p()$ is called a __norm__ if:\n\n::: {.incremental}\n* $p(a\\mathbf{v}) = |a|\\; p(\\mathbf{v})\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ (absolute scaling)\n* $p(\\mathbf{u} + \\mathbf{v}) \\leq p(\\mathbf{u}) + p(\\mathbf{v})\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$  (subadditivity)\n* $p(\\mathbf{v}) = 0 \\leftrightarrow \\mathbf{v}$ is the zero vector $\\;\\;\\;\\;$(separates points)\n:::\n\n::: {.fragment}\nNorms are important for this reason, among others:\n:::\n\n::: {.fragment}\n__Every norm defines a corresponding metric.__\n:::\n\n::: {.fragment}\nThat is, if $p()$ is a norm, then $d(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}-\\mathbf{y})$ is a metric.\n:::\n\n## Useful Norms\n\nA general class of norms are called __$\\ell_p$__ norms, where $p \\geq 1.$\n\n$$ \\Vert \\mathbf{x} \\Vert_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}} $$ \n\nNotice that we use this notation for the $p$-norm of a vector $\\mathbf{x}$:\n\n$$ \\Vert \\mathbf{x} \\Vert_p $$\n\nThe corresponding distance that an $\\ell_p$ norm defines is called the _Minkowski distance._\n\n$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_p = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}} $$\n\n::: {.content-hidden when-profile=\"web\"}\n## $\\ell_2$ norm\n:::\nA special -- but very important -- case is the $\\ell_2$ norm.\n\n$$ \\Vert \\mathbf{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2} $$\n\nWe've already mentioned it: it is the __Euclidean__ norm.\n\nThe distance defined by the $\\ell_2$ norm is the same as the Euclidean distance between the vectors.\n\n$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2  = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2} $$\n\n::: {.content-hidden when-profile=\"web\"}\n## $\\ell_1$ norm\n:::\n\nAnother important special case is the $\\ell_1$ norm.\n\n$$ \\Vert \\mathbf{x} \\Vert_1 = \\sum_{i=1}^d |x_i| $$\n\nThis defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:\n\n$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 = \\sum_{i=1} |x_i - y_i| $$\n\n![](figs/L05-manhattan-distance.png){fig-align=\"center\"}\n    \n::: {.content-hidden when-profile=\"web\"}\n## $\\ell_\\infty$ norm\n:::\n\nIf we take the limit of the $\\ell_p$ norm as $p$ gets large we get the $\\ell_\\infty$ norm.  \n\nThe value of the $\\ell_\\infty$ norm is simply the __largest element__ in a vector.\n\n::: {.fragment}\nWhat is the metric that this norm induces?\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## $\\ell_0$ norm\n:::\n\nAnother related idea is the $\\ell_0$ \"norm,\" which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.\n\nNote that this is __not__ a norm, but it gets called that anyway.   \n\nThis \"norm\" simply counts the number of __nonzero__ elements in a vector.\n\nThis is called the vector's __sparsity.__\n\n::: {.content-hidden when-profile=\"web\"}\n## Visualizing norms\n:::\n\nHere is the notion of a \"circle\" under each of three norms.\n\nThat is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.\n\n![](figs/L5-Vector-Norms.png){fig-align=\"center\"}\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=678101)\n\n\n## Similarity and Dissimilarity\n\nFrom linear algebra, we learn that the inner product of two vectors can be used to compute the __cosine of the angle__ between them:\n\n$$ \\cos(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert} $$\n\nNote that this value is __large__ when $\\mathbf{x} \\approx \\mathbf{y}.$  So it is a __similarity__ function.\n\nWe often find that we have a similarity function $s$ and need to convert it to a dissimilarity function $d$.   \n\nTwo straightforward ways of doing that are:\n\n$$d(x,y) = 1\\,/\\,s(x,y)$$\n\nor \n\n$$d(x,y) = k - s(x,y)$$\n\n...for some properly chosen $k$.\n\nFor cosine similarity, one often uses:\n    \n$$ d(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos(\\mathbf{x}, \\mathbf{y})$$\n\nNote however that this is __not a metric!__\n\nHowever if we recover the actual angle beween $\\mathbf{x}$ and $\\mathbf{y}$, that is a metric.\n\n## Bit vectors and Sets\n\nWhen working with bit vectors, the $\\ell_1$ metric is commonly used and is called the __Hamming__ distance.\n\n![](figs/L5-hamming-1.png){fig-align=\"center\"}\n\nThis has a natural interpretation: \"how well do the two vectors match?\"\n\nOr: \"What is the smallest number of bit flips that will convert one vector into the other?\"\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance\n:::\n\n![](figs/L5-hamming-2.png){fig-align=\"center\"}\n\nIn other cases, the Hamming distance is not a very appropriate metric.\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance with sets\n:::\n\nConsider the case in which the bit vector is being used to represent a set.\n\nIn that case, Hamming distance measures the __size of the set difference.__\n\nFor example, consider two documents.  We will use bit vectors to represent the sets of words in each document.\n\n* Case 1: both documents are large, almost identical, but differ in 10 words.\n* Case 2: both documents are small, disjoint, have 5 words each.\n\nThe situation can be represented as this:\n\n![](figs/L5-jaccard-1.png){fig-align=\"center\"}\n\nWhat matters is not just the size of the set difference, but the size of the intersection as well.\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard similarity\n:::\n\nThis leads to the _Jaccard_ similarity:\n\n$$J_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}$$\n\nThis takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$\n\nIn fact, this is a __metric!__:\n\n$$J_{Dist}(\\mathbf{x}, \\mathbf{y}) = 1- \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}$$\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard similarity continued\n:::\n\nConsider our two cases:\n    \nCase 1: (very large almost identical documents)\n\n![](figs/L5-jaccard-2.png){fig-align=\"center\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is almost 1.\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard similarity continued\n:::\n\nCase 2: (small disjoint documents)\n\n![](figs/L5-jaccard-3.png){fig-align=\"center\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is 0.\n\n## Time Series\n\nA time series is a sequence of real numbers, representing the measurements of a real variable at (equal) time intervals.\n\n::: {.incremental}\n* Stock prices\n* Volume of sales over time\n* Daily temperature readings\n:::\n\n::: {.fragment}\nA time series database is a large collection of time series.\n:::\n\n## Similarity of Time Series\n\nHow should we measure the \"similarity\" of two timeseries?\n\nWe will assume they are the same length.\n\nExamples:\n\n::: {.incremental}\n* Find companies with similar stock price movements over a time interval\n* Find similar DNA sequences\n* Find users with similar credit usage patterns\n:::\n\n::: {.fragment}\nTwo Problems:\n\n::: {.incremental}\n1. Defining a meaningful similarity (or distance) function.\n2. Finding an efficient algorithm to compute it.\n:::\n:::\n\n## Norm-based Similarity Measures\n\nWe could just view each sequence as a vector.\n\nThen we could use a $p$-norm, e.g., $\\ell_1, \\ell_2,$ or $\\ell_p$ to measure similarity.\n\n::: {.fragment}\n\nAdvantages:\n\n::: {.incremental}    \n1. Easy to compute - linear in the length of the time series (O(n)).\n2. It is a metric.\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Norm-based Similarity Measures continued\n:::\n\nDisadvantage:\n\n::: {.fragment}\n1. May not be __meaningful!__\n\n![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n\nWe may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of timeseries.\n:::\n\n::: {.fragment}\nHowever, according to Euclidean distance: \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts2} \\Vert_2 = 26.9,$$\n\nwhile \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts3} \\Vert_2 = 23.2.$$\n:::\n\n::: {.fragment}\nNot good!\n:::\n\n## Feature Engineering\n\nIn general, there may be different aspects of a timeseries that are important in different settings.\n\n::: {.fragment}\nThe first step therefore is to ask yourself \"what is important about timeseries in my application?\"\n:::\n\n::: {.fragment}\nThis is an example of __feature engineering.__\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Feature engineering continued\n:::\n\nIn other words, feature engineering is the art of computing some derived measure from your data object that makes its important properties usable in a subsequent step.\n\n::: {.fragment}\nA reasonable approach may then to be:\n\n::: {.incremental}    \n* extract the relevant features\n* use a simple method (eg, a norm) to define similarity over those features.\n:::\n:::\n\n::: {.fragment}\nIn the case above, one might think of using \n\n::: {.incremental}\n* Fourier coefficients (to capture periodicity)\n* Histograms\n* Or something else!\n:::\n:::\n\n## Dynamic Time Warping\n\nOne case that arises often is something like the following:  \"bump hunting\"\n\n![](figs/L5-DTW-1.png){fig-align=\"center\"} \n\nBoth timeseries have the same key characteristics: four bumps.\n\nBut a one-one match (ala Euclidean distance) will not detect the similarity.\n\n(Be sure to think about why Euclidean distance will fail here.)\n\nA solution to this is called __dynamic time warping.__\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic time warping continued\n:::\n\nThe basic idea is to allow acceleration or deceleration of signals along the time dimension.\n\n::: {.fragmen}\nClassic applications:\n\n::: {.incremental}\n* Speech recognition\n* Handwriting recognition\n:::\n:::\n\n::: {.fragment}\nSpecifically: \n\n::: {.incremental}\n* Consider $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_n$.\n* We are allowed to extend each sequence by repeating elements to form $X'$ and $Y'$.\n* We then calculate, eg, Euclidean distance between the extended sequnces $X'$ and $Y'$\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic time warping continued\n:::\n\nThere is a simple way to visualize this algorithm.\n\nConsider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure).\n\n![](figs/L5-DTW-2.png){fig-align=\"center\"}\n\n$M$ measures the amount of error we get if we match $x_i$ with $y_j$. \n\nSo we seek a __path through $M$ that minimizes the total error.__\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic time warping continued\n:::\n\nWe need to start in the lower left and work our way up via a continuous path.\n\n::: {.fragment}\nBasic restrictions on path:\n    \n::: {.incremental}\n* Montonicity\n  * path should not go down or to the left\n* Continuity\n  * No elements may be skipped in sequence\n:::\n:::\n\n::: {.fragment}\nThis can be solved via dynamic programming.  However, the algorithm is still quadratic in $n$.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic time warping continued\n:::\n\nHence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.\n\nThe basic algorithm looks like this:\n\n```\nD[0, 0] = 0\nfor i in range(n):\n  for j in range(m):\n    D[i,j] = M[i,j] + \n             min( D[i-1, j],    # insertion\n                  D[i, j-1],    # deletion\n                  D[i-1, j-1] ) # match\n```\n\nUnfortunately, the algorithm is still quadratic in $n$ -- it is $\\mathcal{O}(nm)$.\n\nHence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.\n\nThis is implemented by not allowing the path to pass through locations where $|i - j| > w$.\n\nThen the algorithm is $\\mathcal{O}(nw)$.\n\n## From Timeseries to Strings\n\nA closely related idea concerns strings.\n\nThe key point is that, like timeseries, strings are __sequences__.\n\nGiven two strings, one way to define a 'distance' between them is:\n\n* the minimum number of __edit operations__ that are needed to transform one string into the other.\n\nEdit operations are insertion, deletion, and substitution of single characters.\n\nThis is called __edit distance__ or __Levenshtein distance.__\n\n::: {.content-hidden when-profile=\"web\"}\n## Comparing strings\n:::\n\nFor example, given strings:\n\n``s = VIVALASVEGAS``\n    \nand\n\n``t = VIVADAVIS``\n\n::: {.fragment}\n\nwe would like to \n\n::: {.incremental}\n* compute the edit distance, and\n* obtain the optimal __alignment__\n:::\n:::\n\n::: {.fragment}\n![](figs/viva-las-vegas.png){fig-align=\"center\"}\n\n[Source](http://medicalbioinformaticsgroup.de/downloads/lectures/Algorithmen_und_Datenstrukturen/WS15-16/aldabi_ws15-16_woche6.pdf)\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Comparing strings continued\n:::\n\nA dynamic programming algorithm can also be used to find this distance, \n\nand it is __very similar to dynamic time-warping.__\n\nIn bioinformatics this algorithm is called __\"Smith-Waterman\" sequence alignment.__\n\n",
    "supporting": [
      "05-Distances-Timeseries_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}