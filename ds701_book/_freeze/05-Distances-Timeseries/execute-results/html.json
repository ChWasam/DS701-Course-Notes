{
  "hash": "ed73fc330393ddcd44eff21d0b8abbb3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Distances and Time Series\njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)\n\nWe will start building some tools for making comparisons of data objects with particular attention to time series.\n\nWorking with data, we can encounter a wide variety of different data objects\n\n::: {.incremental}\n* records of users,\n* images,\n* videos,\n* text (webpages, books),\n* strings (DNA sequences), and\n* time series.\n:::\n\n::: {.fragment}\nHow can we compare them?\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Lecture Overview\n\nWe cover the following topics in today's lecture\n\n:::: {.incremental}\n- feature space and matrix representations of data,\n- metrics, norms, similarity, and dissimilarity,\n- bit vectors, sets, and time series.\n::::\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Time Series Data\nSome examples of time series data\n\n:::: {.incremental}\n- stock prices,\n- weather data,\n- electricity consumption,\n- website traffic,\n- retail sales, and\n- various economic indicators.\n::::\n:::\n\n## Feature space representation\n\nUsually a data object consists of a set of attributes.\n\nThese are also commonly called __features.__\n\n* (\"J. Smith\", 25, \\$ 200,000)\n* (\"M. Jones\", 47, \\$ 45,000)\n\nIf all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.\n \n* `(25, USD 200000)` $\\rightarrow \\begin{bmatrix}25\\\\200000\\end{bmatrix}$.\n\nLikewise If all features are binary then we can think of each data object as a binary vector in vector space.\n\nThe space is called __feature space.__\n\n::: {.content-hidden when-profile=\"web\"}\n## One-hot encoding\n:::\nVector spaces are such a useful tool that we often use them even for non-numeric data.\n\nFor example, consider a categorical variable that can be only one of \"house\", \"tree\", or \"moon\". For such a variable, we can use a __one-hot__ encoding.  \n\n::: {.content-hidden when-profile=\"slides\"}\nWe would encode as follows:\n:::\n\n::: {.incremental}\n* `house`: $[1, 0, 0]$\n* `tree`:  $[0, 1, 0]$\n* `moon`:  $[0, 0, 1]$\n:::\n\n::: {.fragment}\nSo an encoding of `(25, USD 200000, 'house')` could be: \n$$\\begin{bmatrix}25\\\\200000\\\\1\\\\0\\\\0\\end{bmatrix}.$$\n::: \n\n::: {.content-hidden when-profile=\"web\"}\n## Encodings\n:::\n\nWe will see many other encodings that take non-numeric data and encode them into vectors or matrices.\n\nFor example, there are vector or matrix encodings for\n\n::: {.incremental}\n* graphs,\n* images, and\n* text.\n:::\n\n## Matrix representation of data\n\nWe generally store data in a matrix form as\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{i1} \\\\ \\vdots \\\\ x_{m1} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1j} \\\\ \\vdots \\\\ x_{ij} \\\\ \\vdots \\\\ x_{mj} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1n} \\\\ \\vdots \\\\ x_{in} \\\\ \\vdots \\\\ x_{mn} \\end{array}\n\\end{array}\\right]}^{\\mbox{$n$ features}} \n$$\n\nThe number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.\n\n## Metrics\n\nA metric is a function $d(x, y)$ that satisfies the following properties.\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n::: {.incremental}\n* $d(x, x) = 0$\n* $d(x, y) > 0 \\hspace{1cm} \\forall x\\neq y$ (positivity)\n* $d(x, y) = d(y, x)$ (symmetry)\n* $d(x, y)\\leq d(x, z) + d(z, y)$ (triangle inequality)\n:::\n:::\n::: {.column width=\"30%\"}\n:::: {.fragment}\n![](figs/Lec03-TriangleInequality.png){fig-align=\"center\"}\n::::\n:::\n::::\n\n::: {.fragment}\nWe can use a metric to determine how __similar__ or __dissimilar__ two objects are.\n\nA metric is a measure of the dissimilarity between two objects. The larger the\nmeasure, the more dissimilar the objects are.\n\nIf the objects are vectors, then the metric is also commonly called a __distance__.\n:::\n\n::: {.content-visible when-profile=\"web\"}\nSometimes we will use \"distance\" informally, i.e., to refer to a similarity or\ndissimilarity function even if we are not sure it is a metric.   \n\nWe'll try to say \"dissimilarity\" in those cases though.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Distance Matrices\n:::\n\nThe distance matrix is defined as\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} 0  \\\\  d(x_1, x_2) \\\\ d(x_1,x_3) \\\\ \\vdots \\\\ d(x_1,x_m)  \\end{array} &\n\\begin{array}{c} \\; \\\\  0      \\\\ d(x_2,x_3) \\\\ \\vdots \\\\ d(x_2,x_m)  \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ 0      \\\\ \\vdots \\\\ \\cdots   \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\ddots \\\\ d(x_{m-1},x_m)   \\end{array}  &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\;     \\\\[6pt] 0 \\end{array} &\n\\end{array}\\right]}^{\\mbox{$m$ data objects}},\n$$\n\nwhere $x_i$ denotes the $i$-th column of the data matrix $X$.\n\n## Norms\n\nLet $\\mathbf{u}, \\mathbf{v}\\in\\mathbb{R}^{n}$ and $a\\in\\mathbb{R}$. The vector function $p(\\mathbf{v})$ is called a __norm__ if\n\n:::: {.fragment}\n\n::: {.incremental}\n* $p(a\\mathbf{v}) = |a|p(\\mathbf{v})$,\n* $p(\\mathbf{u} + \\mathbf{v}) \\leq p(\\mathbf{u}) + p(\\mathbf{v})$,\n* $p(\\mathbf{v}) = 0$ if and only if $\\mathbf{v} = 0$.\n:::\n\n::: {.fragment}\n__Every norm defines a corresponding metric.__ \n::::\n\n::: {.fragment}\nIn particular If $p()$ is a norm, then $d(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}-\\mathbf{y})$ is a metric.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_p$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_p$ norm\n:::\n\nA general class of norms are called __$\\ell_p$__ norms, where $p \\geq 1.$\n\n$$\\Vert \\mathbf{x} \\Vert_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}}.$$ \n\nThe corresponding distance that an $\\ell_p$ norm defines is called the _Minkowski distance._\n\n$$\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_p = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}}.$$\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_2$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_2$ norm\n:::\n\nA special -- but very important -- case is the $\\ell_2$ norm.\n\n$$\n\\Vert \\mathbf{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2}.\n$$\n\nWe've already mentioned it: it is the __Euclidean__ norm.\n\nThe distance defined by the $\\ell_2$ norm is the same as the Euclidean distance between two vectors $\\mathbf{x}, \\mathbf{y}$.\n\n$$ \n\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2  = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\n$$\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_1$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_1$ norm\n:::\n\nAnother important special case is the $\\ell_1$ norm.\n\n$$ \\Vert \\mathbf{x} \\Vert_1 = \\sum_{i=1}^d |x_i|.$$\n\nThis defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:\n\n$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 = \\sum_{i=1} |x_i - y_i|.$$\n\n![](figs/L05-manhattan-distance.png){fig-align=\"center\"}\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_\\infty$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_\\infty$ norm\n:::\n\nIf we take the limit of the $\\ell_p$ norm as $p$ gets large we get the $\\ell_\\infty$ norm.  \n\nWe have that\n\n$$\n\\Vert \\mathbf{x} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} \\vert .\n$$\n\n::: {.fragment}\nWhat is the metric that this norm induces?\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## $\\ell_0$ norm\n\nAnother related idea is the $\\ell_0$ \"norm,\" which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.\n\nNote that this is __not__ a norm, but it gets called that anyway.   \n\nThis \"norm\" simply counts the number of __nonzero__ elements in a vector.\n\nThis is called the vector's __sparsity.__\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Visualizing norms\n:::\n\nHere is the notion of a \"circle\" under each of three norms.\n\nThat is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/L5-Vector-Norms.png){fig-align=\"center\"}\n:::\n::: {.column width=\"50%\"}\n\n<br><br>\n$|x_1| + |x_2| = 1$\n\n<br><br>\n$\\sqrt{x_1^2 + x_2^2} = 1$\n\n<br><br>\n$\\max(|x_1|, |x_2|) = 1$\n:::\n::::\n\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=678101)\n\n\n::: {.content-visible when-profile=\"slides\"}\n## What norm should I use?\n\nThe choice of norm depends on the characteristics of your data and the problem you're trying to solve.\n\n:::: {.columns}\n::: {.column width=\"33%\"}\n__$\\ell_1$ norm__\n\n- Use when your data is sparse.\n- Robust to outliers.\n:::\n::: {.column width=\"33%\"}\n__$\\ell_2$ norm__\n\n- Use when measuring distances in Euclidean space.\n- Smooth and differentiable.\n:::\n::: {.column width=\"33%\"}\n__$\\ell_\\infty$ norm__\n\n- Use when you need uniform bounds.\n- Maximum deviation.\n:::\n::::\n\n:::\n\n\n## Similarity and Dissimilarity Functions\n\n:::: {.columns}\n::: {.column width=\"44%\"}\nSimilarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- cosine similarity,\n- Jaccard similarity _(intersection over union)_.\n::::\n::::\n:::\n::: {.column width=\"55%\"}\nDissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- Manhattan distance,\n- Hamming distance.\n::::\n::::\n:::\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Similarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Similarity\n:::\n\nWe know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them\n\n$$ \\cos(\\theta) = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert} \\equiv \\cos(\\mathbf{x}, \\mathbf{y})  .$$\n\nThis value is close to 1 when $\\mathbf{x} \\approx \\mathbf{y}$. We can use this formula to define a __similarity__ function called the __cosine similarity__ $\\cos(\\mathbf{x}, \\mathbf{y})$.\n\n::: {.content-visible when-profile=\"web\"}\n### Dissimilarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Dissimilarity\n:::\n\n:::: {.fragment}\nGiven a similarity function $s(\\mathbf{x}, \\mathbf{y})$, how could we convert it to a dissimilarity function $d(\\mathbf{x}, \\mathbf{y})$?\n::::\n\n:::: {.fragment}\nTwo straightforward ways of doing that are:\n\n$$d(\\mathbf{x},\\mathbf{y}) = 1\\,/\\,s(\\mathbf{x},\\mathbf{y})$$\n\nor \n\n$$d(\\mathbf{x},\\mathbf{y}) = k - s(\\mathbf{x},\\mathbf{y})$$\n\nfor some properly chosen $k$.\n::::\n\n:::: {.fragment}\nFor cosine similarity, one often uses:\n    \n$$ d(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos(\\mathbf{x}, \\mathbf{y})$$\n::::\n\n::: {.content-visible when-profile=\"web\"}\nNote however that this is __not a metric!__\n:::\n\n\n## Bit vectors and Sets\n\nWhen working with bit vectors, the $\\ell_1$ metric is commonly used and is called the __Hamming__ distance.\n\n![](figs/L5-hamming-1.png){fig-align=\"center\"}\n\nThis has a natural interpretation: \"how well do the two vectors match?\"\n\nOr: \"What is the smallest number of bit flips that will convert one vector into the other?\"\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance\n:::\n\n![](figs/L5-hamming-2.png){fig-align=\"center\"}\n\n::: {.content-visible when-profile=\"web\"}\nIn other cases, the Hamming distance is not a very appropriate metric.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance with sets\n:::\n\nConsider the case in which the bit vector is being used to represent a set.\n\nIn that case, Hamming distance measures the __size of the set difference.__\n\nFor example, consider two documents. We will use bit vectors to represent the sets of words in each document.\n\n:::: {.incremental}\n* Case 1: both documents are large, almost identical, but differ in 10 words.\n* Case 2: both documents are small, disjoint, have 5 words each.\n::::\n\n:::: {.fragment}\nWhat matters is not just the size of the set difference, but the size of the intersection.\n::::\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard similarity\n:::\nThis leads to the _Jaccard_ similarity:\n\n$$\nJ_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n$$\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThis takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$\n\nIn fact, this is a __metric!__\n\n$$\nJ_{Dist}(\\mathbf{x}, \\mathbf{y}) = 1- \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n$$\n:::\n::: {.column width=\"40%\"}\n![](figs/L5-jaccard-1.png){fig-align=\"center\"}\n:::\n::::\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard Similarity Example 1\n:::\n    \n::: {.content-visible when-profile=\"web\"}\nLet's revisit the previously introduces cases comparing documents.\n:::\n\nCase 1: Very large almost identical documents.\n\n![](figs/L5-jaccard-2.png){fig-align=\"center\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is almost 1.\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard Similarity Example 2\n:::\n\nCase 2: Very small disjoint documents.\n\n![](figs/L5-jaccard-3.png){fig-align=\"center\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is 0.\n\n## Time Series\n\nA time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.\n\nSome examples are\n\n::: {.incremental}\n* stock prices,\n* the volume of sales over time, and\n* daily temperature readings.\n:::\n\n::: {.fragment}\nA time series database is a large collection of time series.\n:::\n\n## Similarity of Time Series\n\nSuppose we wish to compare the following time series.\n\n::: {.incremental}\n* Stock price movements for companies over a time interval.\n* The motion data of two people walking.\n* Credit usage patterns for bank clients.\n:::\n\n:::: {.fragment}\nHow should we measure the \"similarity\" of these time series?\n::::\n\n::: {.fragment}\nThere are two problems to address.\n\n::: {.incremental}\n1. Defining a meaningful similarity (or distance) function.\n2. Finding an efficient algorithm to compute it.\n:::\n:::\n\n## Norm-based Similarity Measures\n\nWe could just view each sequence as a vector.\n\nThen we could use a $p$-norm, e.g., $\\ell_1, \\ell_2,$ or $\\ell_p$ to measure similarity.\n\n::: {.fragment}\n\n__Advantages__\n\n::: {.incremental}    \n1. Easy to compute - linear in the length of the time series (O(n)).\n2. It is a metric.\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Beware of Norm-based Similarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n:::: {.columns}\n::: {.column width=\"40%\"}\n__Disadvantage__\n\n1. May not be __meaningful!__\n\nWe may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n:::\n::: {.column width=\"60%\"}\n![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n:::\n::::\n:::\n\n::: {.content-visible when-profile=\"web\"}\n__Disadvantage__\n1. May not be __meaningful!__\n\n![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n\nWe may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n:::\n\n::: {.fragment}\nHowever, according to Euclidean distance: \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts2} \\Vert_2 = 26.9,$$\n\nwhile \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts3} \\Vert_2 = 23.2.$$\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Feature Engineering\n\nIn general, there may be different aspects of a time series that are important in different settings.\n\n::: {.fragment}\nThe first step therefore is to ask yourself \"what is important about time series in my application?\"\n:::\n\n::: {.fragment}\nThis is an example of __feature engineering.__\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Feature Engineering\n:::\n\nFeature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.\n\n::: {.fragment}\nA reasonable approach is to\n\n::: {.incremental}    \n* extract the relevant features,\n* use a simple method (e.g., a norm) to define similarity over those features.\n:::\n:::\n\n::: {.fragment}\nIn the case above, one might think of using \n\n::: {.incremental}\n* Fourier coefficients (to capture periodicity),\n* histograms,\n* or something else!\n:::\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Dynamic Time Warping\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Bump Hunting\n:::\n\nOne case that arises often is something like the following:  \"bump hunting\"\n\n![](figs/L5-DTW-1.png){fig-align=\"center\"} \n\nBoth time series have the same key characteristics: four bumps.\n\nBut a one-to-one match (ala Euclidean distance) will not detect the similarity.\n\n::: {.content-visible when-profile=\"web\"}\n(Be sure to think about why Euclidean distance will fail here.)\n:::\n\nA solution to this is called __dynamic time warping.__\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic Time Warping\n:::\n\nThe basic idea is to allow acceleration or deceleration of signals along the time dimension.\n\n::: {.fragment}\n__Classic applications__\n\n::: {.incremental}\n* speech recognition\n* handwriting recognition\n:::\n:::\n\n::: {.fragment}\nSpecifically\n\n::: {.incremental}\n* Consider $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_m$.\n* We are allowed to modify each sequence by inserting, deleting, or matching elements to form $X'$ and $Y'$.\n* We then calculate, e.g., Euclidean distance between the extended sequences $X'$ and $Y'$.\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Visualizing DTW\n:::\n\nThere is a simple way to visualize this algorithm.\n\nConsider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure).\n\n![](figs/L5-DTW-2.png){fig-align=\"center\"}\n\n$M$ measures the amount of error we get if we match $x_i$ with $y_j$. \n\nSo we seek a __path through $M$ that minimizes the total error.__\n\n::: {.content-hidden when-profile=\"web\"}\n## DTW restrictions\n:::\n::: {.content-visible when-profile=\"web\"}\nWe need to start in the lower left and work our way up via a continuous path.\n:::\n\n::: {.fragment}\nThe basic restrictions on path are:\n    \n::: {.incremental}\n* Monotonicity\n  * The path should not go down or to the left.\n* Continuity\n  * No elements may be skipped in a sequence.\n:::\n:::\n\n::: {.fragment}\nThis can be solved via dynamic programming. However, the algorithm is still quadratic in $n$.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Improving DTW \n:::\n\nTo reduce the computational complexity, we can put a restriction on the amount that the path can deviate from the diagonal.\n\nThe basic algorithm looks like this:\n\n```\nD[0, 0] = 0\nfor i in range(n):\n  for j in range(m):\n    D[i,j] = M[i,j] + \n             min( D[i-1, j],    # insertion\n                  D[i, j-1],    # deletion\n                  D[i-1, j-1] ) # match\n```\n\nUnfortunately, the algorithm is still quadratic in $n$ -- it is $\\mathcal{O}(nm)$.\n\nHence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.\n\nThis is implemented by not allowing the path to pass through locations where $|i - j| > w$.\n\nThen the algorithm is $\\mathcal{O}(nw)$.\n\n::: {.content-hidden when-profile=\"slides\"}\n## From Time series to Strings\n\nA closely related idea concerns strings.\n\nThe key point is that, like time series, strings are __sequences__.\n\nGiven two strings, one way to define a 'distance' between them is:\n\n* the minimum number of __edit operations__ that are needed to transform one string into the other.\n\nEdit operations are insertion, deletion, and substitution of single characters.\n\nThis is called __edit distance__ or __Levenshtein distance.__\n\nFor example, given strings:\n\n``s = VIVALASVEGAS``\n    \nand\n\n``t = VIVADAVIS``\n\n\nwe would like to \n\n* compute the edit distance, and\n* obtain the optimal __alignment__.\n\n\n![](figs/viva-las-vegas.png){fig-align=\"center\"}\n\n[Source](http://medicalbioinformaticsgroup.de/downloads/lectures/Algorithmen_und_Datenstrukturen/WS15-16/aldabi_ws15-16_woche6.pdf)\n\nA dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__\n\nIn bioinformatics this algorithm is called __\"Smith-Waterman\" sequence alignment.__\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Recap\n\nWe covered the following topics\n\n:::: {.incremental}\n- reviewed representations of data,\n- discussed metrics and norms,\n- discussed similarity and dissimilarity functions,\n- introduced time series, \n- feature engineering, and\n- dynamic time warping.\n:::: \n:::\n\n",
    "supporting": [
      "05-Distances-Timeseries_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}