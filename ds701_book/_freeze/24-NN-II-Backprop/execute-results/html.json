{
  "hash": "5338a347212e5867a26466a723482a79",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NN II -- Compute Graph, Backprop and Training'\njupyter: python3\n---\n\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/24-NN-II-Backprop.ipynb)\n\n\n\nIn this lecture we'll gradually build out a light weight neural network training framework reminiscent of PyTorch.\n\nWe build:\n\n* A simple neural network engine we call `Value` class that wraps numbers and math operators and includes useful attributes and methods for implementing _forward pass_ and _back propagation_. _(~63 lines of code)_\n\n::: {.content-visible when-profile=\"slides\"}\n## Introduction\n:::\n\nWe'll provide (and explain)\n\n* A simple _compute graph_ visualization function. _(34 lines of code)_\n* A small set of helper functions that easily define a neuron, layer and multi-layer perceptron (MLP). _(84 lines of code)_\n\nWith that we can implement a neural network training loop, and see how similar it is to a PyTorch implementation.\n\n\nThe code is based on Andrej Karpathy's [micrograd](https://github.com/karpathy/micrograd).\n\n# Artificial Neuron\n\n## Artificial Neuron\n\nRecall we looked at an artificial neuron in the\n[Neural Networks I](23-NN-I-Gradient-Descent.qmd) lecture.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/NN-figs/neuron_model.jpeg){width=\"100%\" .lightbox}\n\nFrom [cs231n](https://cs231n.github.io/neural-networks-1/)\n:::\n::: {.column width=\"50%\"}\nThe  common artifical neuron\n\n* collects one or more inputs, \n* each multiplied by a unique weight\n* sums the weighted inputs\n* adds a bias\n* then applies a nonlinear activation function\n:::\n::::\n\n\n## Activation Functions\n\nThe activation function is typically some nonlinear function that compresses the\ninput in some way. Historically, it's been the sigmoid and $\\tanh()$ functions. \n\n<br><br>\n\n::: {.callout-important}\nWithout a nonlinear activation function, a neural network collapses to just a\nlinear model and loses all its expressive power. To dive deeper, see the Universal\nApproximation Theorem in Chapter 3 of \n[Understanding Deep Learning](https://udlbook.github.io).\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Activation Functions\n:::\n\nThe **sigmoid function** is\n\n$$\n\\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}.\n$$\n\n::: {#503ee428 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-3-output-1.png){width=514 height=376 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Activation Functions\n:::\n\nThe hyperbolic tangent, $\\tanh$, is basically the sigmoid function shifted and scaled to a range of [-1,1].\nThe **tanh** function is\n\n$$\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\n$$\n\n::: {#170b57f9 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-4-output-1.png){width=534 height=376 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Activation Functions\n:::\n\nA more common activation function these days and that is more efficient to implement is the _Rectified Linear Unit_ or **ReLU**.\n\n$$\n\\textrm{ReLU}(x) = \\mathrm{max}(0, x)\n$$\n\n::: {#b8ca4930 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-5-output-1.png){width=427 height=302 fig-align='center'}\n:::\n:::\n\n\nThere are many other variations. See for example [PyTorch Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n\n## Relating Back to an Earlier Lecture\n\n__Question__\n\nWhat does \n\n$$\n\\mathrm{sigmoid}\\left(\\sum_i w_i x_i + b\\right) \\hspace{10pt} \\textrm{where} \\hspace{10pt} \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n$$\n\nremind you of?\n\n:::: {.fragment}\n__Answer__\n\nHow about the Logistic Regression model?\n\nIn fact, just like in Logistic Regression, we use the sigmoid function on the last layer of a neural network that is\ndoing binary classification to output the probabilities.\n\n> So Logistic Regression is similar to one neuron with a sigmoid activation\n> function.\n::::\n\n# Fully Connected Network (FCN)\n\n## Fully Connected Network (FCN)\n\n![FCN from UDL.](figs/NN-figs/L24-fcn-dag.png){width=\"75%\" fig-align=\"center\"}\n\n* **Layer**: Multiple artificial neurons can be acting on the same inputs.\n* **Network**: More than one __layer__ cascaded to produce one or more outputs.\n\n::: {.content-visible when-profile=\"slides\"}\n## Fully Connected Network (FCN)\n![FCN from UDL.](figs/NN-figs/L24-fcn-dag.png){width=\"75%\" fig-align=\"center\"}\n:::\n\nIn above: a network with\n\n* _3 inputs_, \n* three hidden layers of 4, 2 and 3 neurons, respectively, followed by \n* one layer that produces two continuous value outputs. \n\n> This is a **regression model** that is predicting two continuous valued\n> outputs.\n\n## FCN Matrix Equations\n\nWe can express the matrix equations for a $K$ layer network as\n\n$$\n\\begin{aligned}\n\\mathbf{h}_1 &= \\mathbf{a}[\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}] \\\\\n\\mathbf{h}_2 &= \\mathbf{a}[\\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{h}_1] \\\\\n\\vdots \\\\\n\\mathbf{h}_K &= \\mathbf{a}[\\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1} \\mathbf{h}_{K-1}] \\\\\n\\mathbf{\\hat{y}} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{h}_K \\\\\n\\end{aligned}\n$$\n\nwhere $\\mathbf{a}[\\cdot]$ is the element-wise activation function.\n\n::: {.content-visible when-profile=\"slides\"}\n## FCN Matrix Equations\n:::\n\nAnd we can write the square loss function in vector notation as\n\n$$\nL = \\sum_{i=1}^N \\ell_i = \\sum_{i=1}^N \\|\\mathbf{y}_i - \\mathbf{\\hat{y}}_i\\|_2^2\n$$\n\nwhich is calculated for each training example $i$ and then accumulated.\n\nSo what we want to do is:\n\n* find the partial derivatives of $L$ with respect to each parameter in the network\n* update the parameters with the negative of its partial derivatives\n\n::: {.content-visible when-profile=\"slides\"}\n## FCN Matrix Equations\n:::\n\nIn other words, we want to compute\n\n$$\n\\frac{\\partial L}{\\partial \\boldsymbol{\\beta}_k}, \\frac{\\partial L}{\\partial \\boldsymbol{\\Omega}_k}, \\frac{\\partial L}{\\partial \\boldsymbol{\\beta}_{k-1}}, \\frac{\\partial L}{\\partial \\boldsymbol{\\Omega}_{k-1}}, \\ldots, \\frac{\\partial L}{\\partial \\boldsymbol{\\beta}_0}, \\frac{\\partial L}{\\partial \\boldsymbol{\\Omega}_0}\n$$\n\nIn the notation above, $\\mathbf{\\beta}_k$ and $\\mathbf{\\Omega}_k$ are the bias\nvectors and weight matrices of the $k$-th layer. \n\nJust as with partial derivatives with respect to scalars, there are well-defined\nrules for how to compute the partial derivatives of vector and matrix expressions\nand, in fact, PyTorch and TensorFlow implement these calculations.\n\n::: {.content-visible when-profile=\"slides\"}\n## FCN Matrix Equations\n:::\n\nAnd then update the parameters with the negative of the partial derivatives.\n\n$$\n\\boldsymbol{\\beta}_k \\leftarrow \\boldsymbol{\\beta}_k - \\frac{\\partial L}{\\partial \\boldsymbol{\\beta}_k}\n$$\n\n$$\n\\boldsymbol{\\Omega}_k \\leftarrow \\boldsymbol{\\Omega}_k - \\frac{\\partial L}{\\partial \\boldsymbol{\\Omega}_k}\n$$\n\n# Computation Graph\n\n## Computation Graph\n\nThe way we are going to differentiate more complex functions is to first build a\n\"computation graph\" to apply our operations on.\n\nWe'll see that we can \"backpropagate\" through the graph to calculate the gradients\nof the loss function with respect to the parameters.\n\nIt's a scalable approach employed by TensorFlow and PyTorch, and in fact we'll\nfollow PyTorch interface definition.\n\n## Building the `Value` Class\n\nTo do that we will \n\n* build a data wrapper as a `class` called `Value` and \n* gradually build in on all the functionality we need to define a Multi-Layer Neural Network\n(a.k.a. Multi-Layer Perceptron) \n* and train it.\n\nThis is similar to how PyTorch defines its `Tensor` class.\n\n::: {.content-visible when-profile=\"slides\"}\n## Building the `Value` Class\n:::\nFirst, the class has only a simple initialization method and a representation method.\n\n::: {#103db52d .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\n# Value version 1\nclass Value:\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n```\n:::\n\n\nWhich we can instantiate and evaluate as follows.\n\n::: {#905c4e88 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0)\na\n```\n\n::: {.cell-output .cell-output-display execution_count=8634}\n```\nValue(data=4.0)\n```\n:::\n:::\n\n\nIf you are not familiar with [python classes](https://docs.python.org/3/tutorial/classes.html), there are a few things to note here.\n\n1. The property `self` is just a pointer to the object itself.\n2. The `__init__` method is called when you initialize a class object\n3. The `__repr__` method is how you represent the class object\n\n## Implementing Addition\n\nSo the Value object doesn't do much yet except for taking a value and printing it. We'd also like to do things like addition and other operations with them, but...\n\n::: {#17171c5c .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0)\nb = Value(-3.0)\n\ntry:\n    a+b \nexcept Exception as e:\n    print(\"Uh oh!\", e)\nelse:\n    print(\"It worked!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUh oh! unsupported operand type(s) for +: 'Value' and 'Value'\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Implementing Addition\n:::\n\nWhen python tries to add two objects `a` and `b`, internally it will call\n`a.__add__(b)`. So we have to add the `__add__()` method.\n\n::: {#81593ec4 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n# Value version 2\nclass Value:\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data)\n        return out\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Implementing Addition\n:::\n\nNow we try adding two `Value` objects again.\n\n::: {#afa5440f .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0)\nb = Value(-3.0)\n\ntry:\n    a+b\nexcept Exception as e:\n    print(\"Uh oh!\", e)\nelse:\n    print(\"It worked!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIt worked!\n```\n:::\n:::\n\n\nWhich, as mentioned is equivalent to calling the `__add__` method on `a`.\n\n::: {#79a75ad7 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\na.__add__(b)\n```\n\n::: {.cell-output .cell-output-display execution_count=8638}\n```\nValue(data=1.0)\n```\n:::\n:::\n\n\n## Implementing More Operations\n\nSimilarly we can support for multiplication and a ReLU function.\n\n::: {#a3cc09e1 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\n\n# Value version 3\nclass Value:\n\n    def __init__(self, data):\n        self.data = data\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other): # self + other\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data)\n        return out\n    \n    def __mul__(self, other): # self * other\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data)\n        return out\n    \n    def relu(self):\n        out = Value(np.maximum(0, self.data))\n        return out\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Implementing More Operations\n:::\n\nAnd now we can use the additional operations.\n\n::: {#ed8ee28c .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0)\nb = Value(-3.0)\nc = Value(8.0)\n\nd = a*b+c\nd\n```\n\n::: {.cell-output .cell-output-display execution_count=8640}\n```\nValue(data=-4.0)\n```\n:::\n:::\n\n\n::: {#7d04ca6d .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\nd.relu()\n```\n\n::: {.cell-output .cell-output-display execution_count=8641}\n```\nValue(data=0.0)\n```\n:::\n:::\n\n\nBy the way, internally, python will call `__mul__` on `a`, then `__add__` on the temporary product object.\n\n::: {#b945f5fc .cell execution_count=14}\n``` {.python .cell-code code-fold=\"false\"}\n(a.__mul__(b)).__add__(c)\n```\n\n::: {.cell-output .cell-output-display execution_count=8642}\n```\nValue(data=-4.0)\n```\n:::\n:::\n\n\n## Child Nodes\n\nIn order to calculate the gradients, we will need to capture the computation graphs. \n\nTo do that, we'll need to store pointers to the operands of each operation as a\ntuple of child nodes in the initializer.\n\n::: {#51d6f57d .cell execution_count=15}\n``` {.python .cell-code code-fold=\"false\"}\n# Value version 4\nclass Value:\n                        #    vvvvvvvvvvvv\n    def __init__(self, data, _children=()):\n        self.data = data\n        self._prev = set(_children)\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other)) # store tuple of children\n        return out                        # ^^^^^^^^^^^^^\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other)) # store tuple of children\n        return out                        # ^^^^^^^^^^^^^\n    \n    def relu(self):\n        out = Value(np.maximum(0, self.data), (self,))\n        return out                         #  ^^^^^^^\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Child Nodes\n:::\n\nLet's instanstiate a few `Value` objects and do some operations with them.\n\n::: {#11299f90 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0)\nb = Value(-3.0)\nc = Value(8.0)\n\nd = a*b\ne = d + c\n```\n:::\n\n\nWe can now see the children of the operands that produced the output value by printing the `_prev` value. The name `_prev` might not be intuitive yet, but it will make more sense when we view these operations as a graph.\n\n::: {#3cd58b90 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\nd._prev\n```\n\n::: {.cell-output .cell-output-display execution_count=8645}\n```\n{Value(data=-3.0), Value(data=4.0)}\n```\n:::\n:::\n\n\n::: {#e364348a .cell execution_count=18}\n``` {.python .cell-code code-fold=\"false\"}\ne._prev\n```\n\n::: {.cell-output .cell-output-display execution_count=8646}\n```\n{Value(data=-12.0), Value(data=8.0)}\n```\n:::\n:::\n\n\n## Child Operations\n\nNow we've recorded pointers to the child nodes. It would be helpful to also record the operator used.\n\nWe'll also add labels for convenience.\n\n::: {#54dd6bc2 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"false\"}\n# Value version 5\nclass Value:\n                                    #     vvvvvvv  vvvvvvvv\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self._prev = set(_children)\n        self._op = _op # store the operation that created this node\n        self.label = label # label for the node\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)   \n        out = Value(self.data + other.data, (self, other), '+') # store operator used\n        return out                                      #  ^^^\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*') # store operator used\n        return out                                      #  ^^^\n    \n    def relu(self):                                 #  vvvvvv\n        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n\n        return out\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Child Operations\n:::\n\nLet's instanstiate a few `Value` objects and do some operations with them.\n\n::: {#c05377bc .cell execution_count=20}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(8.0, label='c')\n\nd = a*b ; d.label = 'd'\ne = d + c ; e.label = 'e'\n```\n:::\n\n\nWe can now inspect the parent operands and the operations used to create the\ncurrent node value.\n\n::: {#85fab67f .cell execution_count=21}\n``` {.python .cell-code code-fold=\"false\"}\nd._prev, d._op, d.label\n```\n\n::: {.cell-output .cell-output-display execution_count=8649}\n```\n({Value(data=-3.0), Value(data=4.0)}, '*', 'd')\n```\n:::\n:::\n\n\n::: {#ae35ec89 .cell execution_count=22}\n``` {.python .cell-code code-fold=\"false\"}\ne._prev, e._op, e.label\n```\n\n::: {.cell-output .cell-output-display execution_count=8650}\n```\n({Value(data=-12.0), Value(data=8.0)}, '+', 'e')\n```\n:::\n:::\n\n\n## The Compute Graph\n\nWe now have enough information stored about the compute graph to visualize it.\n\nThese are two functions to walk the graph and build sets of all nodes and edges (`trace`) and then draw them as a\ndirected graph (`draw_dot`).\n\n::: {#636dfac1 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code for `trace()` and `draw_dot()`\"}\n# draw_dot version 1\nfrom graphviz import Digraph\n\ndef trace(root):\n  # builds a set of all nodes and set of all edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n\n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f }\" % (n.label, n.data), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this no de to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot\n```\n:::\n\n\n::: {#c016e73c .cell execution_count=24}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(8.0, label='c')\n\nd = a*b ; d.label = 'd'\ne = d + c ; e.label = 'e'\n\ndraw_dot(e)\n```\n\n::: {.cell-output .cell-output-display execution_count=8652}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-25-output-1.svg){}\n:::\n:::\n\n\nNote that every value object becomes a node in the graph. The operators are also\nrepresented as a kind of fake node so they can be visualized too.\n\n::: {#c5b0a3f6 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"false\"}\nnodes, edges = trace(e)\nprint(\"Nodes: \", nodes)\nprint(\"Edges: \", edges)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNodes:  {Value(data=-12.0), Value(data=4.0), Value(data=-3.0), Value(data=-4.0), Value(data=8.0)}\nEdges:  {(Value(data=8.0), Value(data=-4.0)), (Value(data=-3.0), Value(data=-12.0)), (Value(data=4.0), Value(data=-12.0)), (Value(data=-12.0), Value(data=-4.0))}\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## The Compute Graph\n:::\n\nLets add one more operation, or stage in the compute graph.\n\n::: {#424a2124 .cell execution_count=26}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(4.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(8.0, label='c')\n\nd = a*b; d.label = 'd'\ne = d + c; e.label = 'e'\nf = Value(2.0, label='f')\n\nL = e*f; L.label = 'L'\n\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8654}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-27-output-1.svg){}\n:::\n:::\n\n\n## Recap\n\nSo far we've:\n\n* built a `Value` class and associated data structures,\n* captured a computational graph and\n* calculated the output based on the inputs and operations. \n\nWe'll call this the __forward pass__.\n\nBut now, we're interested in calculating the gradients with respect to some of\nthe parameters with respect to $L$. \n\nSo next we'll update our `Value` class to capture the partial derivative at each\nnode relative to `L`.\n\n# Calculating Gradients\n\n## Calculating Gradients\n\nWe now add a gradient member variable, `grad`, to our class to store the partial\nderivative of the node with respect to the output node.\n\n::: {#2dd088f3 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"false\" code-line-numbers=\"true\"}\n# Value version 6\nclass Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # default to 0  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n        self._prev = set(_children)\n        self._op = _op # store the operation that created this node\n        self.label = label # label for the node\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)   \n        out = Value(self.data + other.data, (self, other), '+')\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)   \n        out = Value(self.data * other.data, (self, other), '*')\n        return out\n    \n    def relu(self):         \n        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n\n        return out\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Calculating Gradients\n:::\n\nAnd update `draw_dot()` to show `grad` in the node info.\n\n::: {#8f555957 .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code for `draw_dot()`\"}\n# draw_dot version 2\nfrom graphviz import Digraph\n\ndef trace(root):\n  # builds a set of all nodes and set of all edges in a graph\n  nodes, edges = set(), set()\n  def build(v):\n    if v not in nodes:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child, v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n \n  nodes, edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name = uid + n._op, label = n._op)\n      # and connect this node to it\n      dot.edge(uid + n._op, uid)\n\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n  return dot        \n```\n:::\n\n\nAnd reinitialize and redraw...\n\n::: {#abb845ce .cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\"}\na = Value(4.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(8.0, label='c')\n\nd = a*b; d.label = 'd'\ne = d + c; e.label = 'e'\nf = Value(2.0, label='f')\n\nL = e*f; L.label = 'L'\n\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8657}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-30-output-1.svg){}\n:::\n:::\n\n\nWe have have placeholders for the gradients, but they are currently all zero.\n\n## Manual Gradient Calculation\n\nBefore we start implementing backpropagation, it is helpful to manually calculate some gradients to better understand the procedure.\n\nFor the node $L$, we trivially calculate $\\frac{dL}{dL} = 1$. \n\nWe can easily prove this from the limit ratio perspective, \n\n$$\n\\frac{\\partial L}{\\partial L} = \\lim_{h \\rightarrow 0} \\frac{ (L+h) - L }{h} = \\frac{h}{h} = 1\n$$\n\nSo let's set the gradient of the last node `L` to 1.\n\n::: {#c934c169 .cell execution_count=30}\n``` {.python .cell-code code-fold=\"false\"}\nL.grad = 1.0\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Manual Gradient Calculation\n:::\n\nIf we go backwards a step in the graph, we see that $L=e*f$, so we calculate\n\n$$\n\\frac{\\partial{L}}{\\partial{e}} = \\frac{\\partial}{\\partial{e}} (e\\times f) = f\n$$\n\nand\n\n$$\n\\frac{\\partial{L}}{\\partial{f}} = \\frac{\\partial}{\\partial{f}} (e\\times f) = e.\n$$\n\nSo we just assign the gradient value to the value of the other operand node.\n\n::: {#1c2dbff9 .cell execution_count=31}\n``` {.python .cell-code code-fold=\"false\"}\ne.grad = f.data\nf.grad = e.data\n```\n:::\n\n\n::: {.callout-tip}\nFor **products**, the partial derivative w.r.t. to one operand is simply the _other operand_.\n:::\n\n::: {.callout-important}\nWe needed the node values, e.g. `e.data` and `f.data`, in order to update the\ngradients. Remember, we calculate all the node values in the **forward pass**.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Manual Gradient Calculation\n:::\n\nAnd we can redraw the graph above again.\n\n::: {#3c304045 .cell execution_count=32}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8660}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-33-output-1.svg){}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Manual Gradient Calculation\n:::\n\nHow do the parameters $e$ and $f$ influence $L$? Here's a function to wiggle them\nand see the effect on $L$.\n\n::: {#ddf657aa .cell execution_count=33}\n``` {.python .cell-code code-fold=\"false\" code-line-numbers=\"true\"}\n# Try uncommenting `e += h` or `f += h` and calling `wiggle()` then `wiggle(1.0)`\n# to see the influence of e or f on L\ndef wiggle(h = 0.0):\n    a = Value(4.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(8.0, label='c')\n\n    d = a*b; d.label = 'd'\n    e = d + c; e.label = 'e'\n    # e += h\n    f = Value(2.0, label='f')\n    f += h\n\n    L = e*f; L.label = 'L'\n    print(L)\n```\n:::\n\n\nLet's invoke it with no change $0$ and then a change of $1$.\n\n::: {#9fd06404 .cell execution_count=34}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(0.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=-8.0)\n```\n:::\n:::\n\n\n::: {#cb22815e .cell execution_count=35}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=-12.0)\n```\n:::\n:::\n\n\n__Question__\n\nWhat does $L$ change by if we change $f$ by 1?\n\nHow much would $L$ change if we changed $e$ by 1?\n\nCommentline 12 and uncomment line 10 to check your answer.\n\n## Propagating Back\n\n::: {#fa6aaccb .cell execution_count=36}\n``` {.python .cell-code}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8664}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-37-output-1.svg){}\n:::\n:::\n\n\nNow we want to calculate\n\n$$\n\\frac{\\partial{L}}{\\partial{c}}\n$$\n\nor put another way, we want to know how much $L$ wiggles if we wiggle $c$, or how $c$ influences $L$.\n\n::: {.content-visible when-profile=\"slides\"}\n## Propagating Back\n\n::: {#da84916c .cell execution_count=37}\n``` {.python .cell-code}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8665}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-38-output-1.svg){}\n:::\n:::\n\n\n:::\n\nLooking at the graph again we see that $c$ influences $e$ and $e$ influences $L$, so we should be able see the ripple effect of $c$ on $L$.\n\n$$\nc \\rightarrow e \\rightarrow L \n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Propagating Back\n\n::: {#d07d2da8 .cell execution_count=38}\n``` {.python .cell-code}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8666}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-39-output-1.svg){}\n:::\n:::\n\n\n:::\n\nSo $e = c + d$, and so we calculate\n\n$$ \n\\frac{\\partial{e}}{\\partial{c}} = \\frac{\\partial{}}{\\partial{c}} (d + c) = 1\n$$\n\n::: {.callout-tip}\nFor **additions**, the partial derivative w.r.t. to one operand is 1.\n:::\n\n## Question\n\n$$ \nc \\rightarrow e \\rightarrow L \n$$\n\nSo now we know $\\partial{L}/\\partial{e}$ and we also know $\\partial{e}/\\partial{c}$,\n\nHow do we get $\\partial{L}/\\partial{c}$?\n\n## The Chain Rule\n\nTo paraphrase from the Wikipedia page on\n[Chain rule](https://en.wikipedia.org/wiki/Chain_rule), if a variable $L$ depends\non the variable $e$, which itself depends on the variable $c$, then $L$ depends\non $c$ as well, via the intermediate variable $e$. \n\nIn this case, the chain rule is expressed as\n\n$$\n\\frac{\\partial L}{\\partial c} \n= \\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial c},\n$$\n\nand\n\n$$\n\\left.\\frac{\\partial L}{\\partial c}\\right|_{c} \n= \\left.\\frac{\\partial L}{\\partial e}\\right|_{e(c)}\\cdot \\left.\n  \\frac{\\partial e}{\\partial c}\\right|_{c} ,\n$$\n\nfor indicating at which points the derivatives have to be evaluated.\n\n> We evaluate the derivatives at the specific values of the variables that we\n> calculated in the forward pass.\n\n::: {.content-visible when-profile=\"slides\"}\n## The Chain Rule\n:::\n\nNow since we've established that\n\n$$ \\frac{\\partial{e}}{\\partial{c}} = 1$$\n\nthen\n\n$$\n\\frac{dL}{dc} = \\frac{dL}{de} \\cdot \\frac{\\partial{e}}{\\partial{c}} = \\frac{dL}{de} \\cdot 1.\n$$\n\nSo in the case of an operand in an addition operation, we just copy the gradient of the parent node.\n\nOr put another way:\n\n::: {.callout-important}\nWith the **addition operator**, we just _route the parent gradient to the child_.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Chain Rule\n:::\n\nSo let's assign the gradient of `e` to `d` and `c`.\n\n::: {#e0db4348 .cell execution_count=39}\n``` {.python .cell-code code-fold=\"false\"}\nd.grad = e.grad\nc.grad = e.grad\n```\n:::\n\n\nAnd redraw the graph.\n\n::: {#fc044e03 .cell execution_count=40}\n\n::: {.cell-output .cell-output-display execution_count=8668}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-41-output-1.svg){}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## The Chain Rule\n:::\n\nHow does $c$ and $d$ influence $L$?\n\n::: {#569ab83c .cell execution_count=41}\n``` {.python .cell-code code-fold=\"false\"}\n# Try uncommenting `c += h` or `d += h` and calling `wiggle()` then `wiggle(1.0)`\n# to see the influence of c or d on L\ndef wiggle(h = 0.0):\n    a = Value(4.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(8.0, label='c')\n    c += h\n\n    d = a*b; d.label = 'd'\n    # d += h\n\n    e = d + c; e.label = 'e'\n    f = Value(2.0, label='f')\n\n    L = e*f; L.label = 'L'\n    print(L)\n```\n:::\n\n\nLet's look at the effect `c` has on `L`.\n\n::: {#f068981f .cell execution_count=42}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(0.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=-8.0)\n```\n:::\n:::\n\n\n::: {#5930281c .cell execution_count=43}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=-6.0)\n```\n:::\n:::\n\n\nChange the code to see the effect of `d` on `L`.\n\n## Propagating Back Again\n\n::: {#ec96fa6e .cell execution_count=44}\n\n::: {.cell-output .cell-output-display execution_count=8672}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-45-output-1.svg){}\n:::\n:::\n\n\nNow we want to calculate\n$\\frac{\\partial{L}}{\\partial{b}}$ and\n$\\frac{\\partial{L}}{\\partial{a}}$\nbut we have\n$\\frac{\\partial{L}}{\\partial{d}}$\nand we know that\n\n$$\n\\frac{\\partial{d}}{\\partial{b}} = \\frac{\\partial{}}{\\partial{b}}(a\\cdot b) = a\n$$\n\nso again from the chain rule\n\n$$\n\\frac{\\partial{L}}{\\partial{b}} \n  = \\frac{\\partial{L}}{\\partial{d}} \\cdot \\frac{\\partial{d}}{\\partial{b}}\n  = \\frac{\\partial{L}}{\\partial{d}} \\cdot a.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Propagating Back Again\n\n::: {#4b8afd6a .cell execution_count=45}\n\n::: {.cell-output .cell-output-display execution_count=8673}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-46-output-1.svg){}\n:::\n:::\n\n\n:::\n\nIf we expanded all the partial derivatives, we would get\n\n$$\n\\frac{\\partial{L}}{\\partial{b}} \n  = \\left(\\frac{\\partial{L}}{\\partial{e}} \\cdot \\frac{\\partial{e}}{\\partial{d}}\\right) \\cdot \\frac{\\partial{d}}{\\partial{b}}\n$$\n\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Propagating Back Again\n:::\n\nLet's assign the gradient values to `b` and `a`.\n\n::: {#2075a397 .cell execution_count=46}\n``` {.python .cell-code code-fold=\"false\"}\nb.grad = a.data * d.grad\na.grad = b.data * d.grad\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8674}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-47-output-1.svg){}\n:::\n:::\n\n\nWe've traversed all the way back to the inputs and calculated all the partial derivatives!!\n\n::: {.content-visible when-profile=\"slides\"}\n## Propagating Back Again\n:::\n\nAnd now we can see the effects of wigling `b` and `a`.\n\n::: {#c955bd5a .cell execution_count=47}\n``` {.python .cell-code code-fold=\"false\"}\n# Try uncommenting `a += h` or `b += h` and calling `wiggle()` then `wiggle(1.0)`\n# to see the influence of a or b on L\ndef wiggle(h = 0.0):\n    a = Value(4.0, label='a')\n    # a += h\n    b = Value(-3.0, label='b')\n    b += h\n    c = Value(8.0, label='c')\n\n    d = a*b; d.label = 'd'\n\n    e = d + c; e.label = 'e'\n    f = Value(2.0, label='f')\n\n    L = e*f; L.label = 'L'\n    print(L)\n```\n:::\n\n\nLet's look at the effect `b` has on `L` if we wiggle it by 0 and 1.\n\n::: {#af1ab2f2 .cell execution_count=48}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(0.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=-8.0)\n```\n:::\n:::\n\n\n::: {#87962e11 .cell execution_count=49}\n``` {.python .cell-code code-fold=\"false\"}\nwiggle(1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue(data=0.0)\n```\n:::\n:::\n\n\nChange the code to see the effect of `a` on `L`.\n\n## Recap\n\nAs you saw, we recursively went backwards through the computation graph and\napplied the local gradients to the gradients calculated so far to get the partial\ngradients. \n\n> Put another way, we propagated this calculation backwards through the graph.\n\nOf course, in practice, we will only need the gradients on the parameters, not\nthe inputs, so we won't bother calculating them on inputs.\n\n::: {.callout-note}\nPyTorch let's you control this by setting the `requires_grad` flag on Tensors.\n:::\n\n_That is the essence of Back Propagation._\n\n# Model Optimization\n\n## A Step in Optimization\n\nLet's take a look at the graph again. \n\n::: {#ae17470d .cell execution_count=50}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(L)\n```\n\n::: {.cell-output .cell-output-display execution_count=8678}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-51-output-1.svg){}\n:::\n:::\n\n\nAssume we want the value of L to _decrease_. \n\nWe are free to change the values of the leaf nodes -- all the other nodes are\nderived from children and leaf nodes.\n\nThe leaf nodes are $a, b, c$ and $f$.\n\n> Again, in practice we would only update the parameter leaf nodes, not the input\n> leaf node, but we'll ignore that distinction temporarily for this example.\n\n::: {.content-visible when-profile=\"slides\"}\n## A Step in Optimization\n:::\n\nLet's check the current value of L.\n\n::: {#ec0d6ef8 .cell execution_count=51}\n``` {.python .cell-code code-fold=\"false\"}\n# remind ourselves what L is\nprint(L.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-8.0\n```\n:::\n:::\n\n\nAs we showed before, we want to nudge each of those leaf nodes by the negative\nof the gradient, multiplied by a step size, $\\eta$.\n\n$$\nw_{n+1} = w_n - \\eta * \\frac{\\partial{L}}{\\partial{w_n}}\n$$\n\nwhere $n$ is the step number.\n\n::: {#5094efdb .cell execution_count=52}\n``` {.python .cell-code code-fold=\"false\"}\n# nudge all the leaf nodes along the negative direction of the gradient\nstep_size = 0.01    # also called eta above\n\na.data -= step_size * a.grad\nb.data -= step_size * b.grad\nc.data -= step_size * c.grad\nf.data -= step_size * f.grad\n\n# recompute the forward pass\nd = a*b\ne = d + c\nL = e*f\n\nprint(L.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-9.230591999999998\n```\n:::\n:::\n\n\nAnd indeed, the value of L has decreased.\n\n## A Single Neuron\n\nLet's now programmatically define a single neuron with\n\n* two inputs\n* two weights (1 for each input)\n* a bias\n* the ReLU activation function\n\nRecall the neuron figure above.\n\n::: {#b4831a45 .cell execution_count=53}\n``` {.python .cell-code code-fold=\"false\"}\n# inputs x0, x1\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n# weights w1, w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n# bias of the neuron\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\no = n.relu(); o.label = 'o'\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## A Single Neuron\n:::\n\nWe can draw this compute graph:\n\n::: {#4d1ae1a3 .cell execution_count=54}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=8682}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-55-output-1.svg){}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## A Single Neuron\n:::\n\nThe only new operation we've added is the ReLU, so let's take a quick look at how we \ndifferentiate the ReLU. \n\nLike before, for the output node, o:\n\n$$\n\\frac{\\partial o}{\\partial o} = 1\n$$\n\n::: {#9efdaed5 .cell execution_count=55}\n``` {.python .cell-code code-fold=\"false\"}\no.grad = 1.0\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## A Single Neuron\n:::\n\n<!-- Credit: _Understanding Deep Learning_, Figure 7.6 -->\nReLU is technically not differentiable at 0, but practically we implement the\nderivative as 0 when $\\le 0$ and 1 when $1 > 0$\n\n![](figs/NN-figs/Train2ReLUDeriv.svg){width=\"50%\" fig-align=\"center\"}\n\n::: {#d46bb06b .cell execution_count=56}\n``` {.python .cell-code code-fold=\"false\"}\nn.grad = (o.data > 0) * o.grad  # = 0 when o.data <= 0; = o.grad when o.data > 0\n```\n:::\n\n\n## Coding Backpropagation\n\nNow we'll update our `Value` class once more to support the backward pass.\n\nThere's a\n\n* private `_backward()` function _in each operator_ that implements the local\nstep of the chain rule, and\n* a `backward()` function in the class that topologically sorts the graph and calls the operator `_backward()` function starting at the end of the graph and going _backward_.\n\n::: {#72a01c72 .cell execution_count=57}\n``` {.python .cell-code code-fold=\"false\"}\n# version 7\nclass Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # default to 0, no impact on the output\n        self._backward = lambda: None  # by default backward doesn't do anything\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n        # out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Coding Backpropagation\n:::\n\nWe redefined the class so we have to reinitialize the objects and run the operations again.\n\nThis constitutes the _forward pass_.\n\n::: {#0cb1f421 .cell execution_count=58}\n``` {.python .cell-code code-fold=\"false\"}\n# inputs x0, x1\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\n\n# weights w1, w2\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\n\n# bias of the neuron\n#b = Value(6.7, label='b')\nb = Value(6.8813735870195432, label='b')\n\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\n\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1w1 + x2w2'\nn = x1w1x2w2 + b; n.label = 'n'\no = n.relu(); o.label = 'o'\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Coding Backpropagation\n:::\n\n\nSo we've filled the data values for all the nodes, but haven't calculated the gradients.\n\n::: {#c052092a .cell execution_count=59}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=8687}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-60-output-1.svg){}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Coding Backpropagation\n:::\n\n\nNow, all we have to do is call the `backward()` method of the last node...\n\n::: {#d92e4717 .cell execution_count=60}\n``` {.python .cell-code code-fold=\"false\"}\no.backward()\n```\n:::\n\n\nAnd voila! We have all the gradients!\n\n::: {#c3cf88c4 .cell execution_count=61}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(o)\n```\n\n::: {.cell-output .cell-output-display execution_count=8689}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-62-output-1.svg){}\n:::\n:::\n\n\n## Accumulating the Gradients\n\nYou might have noticed that we are accumulating the gradients.\n\nThis is because we calculate the gradient for each sample in a batch and then\nsum them up to calculate the average gradient for the batch.\n\nThe risk now is that if you don't zero the gradients for the next iteration, \nyou will have incorrect gradients. \n\n::: {.callout-warning}\nAlways remember to zero the gradient after each iteration where you update the parameters!\n:::\n\n::: {.content-visible when-profile=\"web\"}\n\n## Accumulating the Gradients -- Special Cases\n\nThere are also special cases to handle like where a `Value` object is on both\nsides of the operand like\n\n::: {#0b71e5fd .cell execution_count=62}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(3.0, label='a')\nb = a + a ; b.label = 'b'\nb.backward()\n\na\n```\n\n::: {.cell-output .cell-output-display execution_count=8690}\n```\nValue(data=3.0, grad=2.0)\n```\n:::\n:::\n\n\nIf we didn't have the accumulation, then `a.grad = 1` instead.\n\nOr the other case where a node goes to different operations.\n\n::: {#dd30bf1a .cell execution_count=63}\n``` {.python .cell-code code-fold=\"false\"}\na = Value(-2.0, label='a')\nb = Value(3.0, label='b')\nd = a * b  ; d.label = 'd'\ne = a + b   ; e.label = 'e'\n  \ndraw_dot(f) \n```\n\n::: {.cell-output .cell-output-display execution_count=8691}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-64-output-1.svg){}\n:::\n:::\n\n\n::: {.callout-note}\nWe can verify that the gradients are correct analytically.\n\nTo find the partial derivative $\\frac{\\partial f}{\\partial a}$, we first need to define $f$ in terms of $a$ and $b$.\n\nGiven:\n$$\n\\begin{aligned}\nd &= a \\times b \\\\\ne &= a + b      \\\\\nf &= d \\times e\n\\end{aligned}\n$$\n\nThen $f$ can be expanded as:\n$$\n\\begin{aligned}\nf &= (a \\times b) \\times (a + b) \\\\\nf &= a^2 \\times b + a \\times b^2\n\\end{aligned}\n$$\n\nNext, we find the partial derivative of $f$ with respect to $a$:\n$$\n\\frac{\\partial f}{\\partial a} = 2a \\times b + b^2\n$$\n\nFinally, we plug in the given values $a = -2.0$ and $b = 3.0$:\n$$\n\\begin{aligned}\n\\frac{\\partial f}{\\partial a} &= 2(-2.0) \\times 3.0 + 3.0^2 \\\\\n\\frac{\\partial f}{\\partial a} &= -12.0 + 9.0                 \\\\\n\\frac{\\partial f}{\\partial a} &= -3.0\n\\end{aligned}\n$$\n\nSo the partial derivative $\\frac{\\partial f}{\\partial a}$ for the value $a = -2.0$ is $-3.0$.\n:::\n:::\n\n\n## Enhancements to `Value` Class\n\nThere are still some useful operations that `Value` doesn't support, so to be more\ncomplete we have the final version of the `Value` class below.\n\nWe added:\n\n* `__radd__` for when the `Value` object is the right operand of an add\n* `__rmul__` for when the `Value` object is the right operand of a product \n* `__pow__` to support the ** operator\n* plus some others you can see below\n\n::: {#67393f9b .cell execution_count=64}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"The Value Class with additional operators\"}\n# version 8\nclass Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0 # default to 0, no impact on the output\n        self._backward = lambda: None  # by default backward doesn't do anything\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        \n        return out\n\n    def __pow__(self, other):\n        \"\"\"Adding support for ** operator, which we'll need for the \n        squared loss function\"\"\"\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(np.maximum(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __neg__(self): # -self\n        return self * -1\n\n    def __radd__(self, other): # other + self\n        return self + other\n\n    def __sub__(self, other): # self - other\n        return self + (-other)\n\n    def __rsub__(self, other): # other - self\n        return other + (-self)\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def __truediv__(self, other): # self / other\n        return self * other**-1\n\n    def __rtruediv__(self, other): # other / self\n        return other * self**-1\n    \n    def backward(self):\n\n        # topological order all of the children in the graph\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # go one variable at a time and apply the chain rule to get its gradient\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object for display\"\"\"\n        return f\"Value(data={self.data}, grad={self.grad})\"\n```\n:::\n\n\n## Comparing to PyTorch\n\nWe're using a class implementation that resembles the PyTorch implementation, and in fact we can compare our implementation with PyTorch.\n\n::: {#9182fdd6 .cell execution_count=65}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\n```\n:::\n\n\n::: {#78c7eb87 .cell execution_count=66}\n``` {.python .cell-code code-fold=\"false\"}\nx1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\nx2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\nw1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\nw2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\nb = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\nn = x1*w1 + x2*w2 + b\no = torch.relu(n)\n\nprint(o.data.item())\no.backward()\n\nprint('---')\nprint('x2.grad', x2.grad.item())\nprint('w2.grad', w2.grad.item())\nprint('x1.grad', x1.grad.item())\nprint('w1.grad', w1.grad.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.881373405456543\n---\nx2.grad 1.0\nw2.grad 0.0\nx1.grad -3.0\nw1.grad 2.0\n```\n:::\n:::\n\n\nBy default, tensors don't store gradients and so won't support backprop, so we explicitly set `requires_grad = True`.\n\n# Building a Neural Network\n\n## Neural Network Modules\n\nNow we'll define some classes which help us build out a small neural network.\n\n__Module__ -- A base class\n\n__Neuron__ -- Implement a single linear or nonlinear neuron with `nin` inputs.\n\n__Layer__ -- Implement a layer of network consisting of `nout` neurons, each taking `nin` inputs\n\n__MLP__ -- A _Multi-Layer Perceptron_ that implements `len(nouts)` layers of neurons.\n\nEach class can calculate a forward pass and enumerate all its parameters.\n\n::: {#3afff30a .cell execution_count=67}\n``` {.python .cell-code code-fold=\"true\"}\nimport random\n# we assume that Value class is already defined\n\nclass Module:\n    \"\"\"Define a Neural Network Module base class \"\"\"\n\n    def zero_grad(self):\n        \"\"\"When we run in a training loop, we'll need to zero out all the gradients\n        since they are defined to accumulate in the backwards passes.\"\"\"\n        for p in self.parameters():\n            p.grad = 0\n\n    def parameters(self):\n        return []\n\nclass Neuron(Module):\n    \"\"\"Define a Neuron as a subclass of Module\"\"\"\n\n    def __init__(self, nin, nonlin=True):\n        \"\"\"Randomly initialize a set of weights, one for each input, and initialize the bias to zero.\"\"\"\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(0.0)\n        self.nonlin = nonlin\n\n    def __call__(self, x):\n        \"\"\"Implement the forward pass of the neuron\"\"\"\n        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n        return act.relu() if self.nonlin else act\n\n    def parameters(self):\n        return self.w + [self.b]\n\n    def __repr__(self):\n        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n\nclass Layer(Module):\n    \"\"\"Define a Layer of Network as a subclass of Module\"\"\"\n\n    def __init__(self, nin, nout, **kwargs):\n        \"\"\"Initialize nout Neurons, each with nin inputs\"\"\"\n        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n\n    def __call__(self, x):\n        \"\"\"Forward pass each neuron in the layer\"\"\"\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n    def __repr__(self):\n        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n\nclass MLP(Module):\n    \"\"\"Define a Multi-Layer Perceptron\"\"\"\n\n    def __init__(self, nin: int, nouts: list):\n        \"\"\"\n        Initialize the Multi-Layer Perceptron, by initializing each layer\n        then initializing each neuron of each layer.\n\n        Parameters:\n            nin: Number of inputs (int)\n            nouts: A list of the number of neurons in each layer\n        \"\"\"\n        sz = [nin] + nouts\n        # self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n        # Create a list of layer objects for this MLP. All but the last layer\n        # have ReLU activations. The last layer is linear.\n        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        \"\"\"Forward pass through the MLP\"\"\"\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        \"\"\"Recursively retrieve the parameters of the MLP\"\"\"\n        return [p for layer in self.layers for p in layer.parameters()]\n\n    def __repr__(self):\n        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n```\n:::\n\n\nFeel free to uncomment lines of code in the following cell to see the docstrings for each class.\n\n::: {#c4bb3f44 .cell execution_count=68}\n``` {.python .cell-code code-fold=\"false\"}\n# help(Module)\n# Module.__doc__\n# help(Neuron)\n# help(Layer)\n# help(MLP)\n```\n:::\n\n\n## Initialize and Evaluate a Neuron\n\n::: {#519e951d .cell execution_count=69}\n``` {.python .cell-code code-fold=\"false\"}\n# 2 inputs\nx = [2.0, 3.0]\n\n# initialize neuron with 2 inputs\nn = Neuron(2, nonlin=False)\n\n# evaluate our neuron with our 2 inputs\nn(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=8697}\n```\nValue(data=-1.367255056222844, grad=0.0)\n```\n:::\n:::\n\n\n::: {#a09159ca .cell execution_count=70}\n``` {.python .cell-code code-fold=\"false\"}\nn\n```\n\n::: {.cell-output .cell-output-display execution_count=8698}\n```\nLinearNeuron(2)\n```\n:::\n:::\n\n\n::: {#190aa65d .cell execution_count=71}\n``` {.python .cell-code code-fold=\"false\"}\n# list the 2 weights and the bias\nn.parameters()\n```\n\n::: {.cell-output .cell-output-display execution_count=8699}\n```\n[Value(data=-0.5624379253246228, grad=0.0),\n Value(data=-0.08079306852453283, grad=0.0),\n Value(data=0.0, grad=0.0)]\n```\n:::\n:::\n\n\n## Initialize and Evaluate a Layer\n\n::: {#f1b5ac0d .cell execution_count=72}\n``` {.python .cell-code code-fold=\"false\"}\n# same 2 inputs again\nx = [2.0, 3.0]\n\n# Now initialize a layer of 3 neurons, each with 2 inputs\nl = Layer(2, 3, nonlin=False)\n\n# Evaluate our layer of neurons with the 2 inputs\nl(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=8700}\n```\n[Value(data=-3.7119353100426045, grad=0.0),\n Value(data=1.689037838564892, grad=0.0),\n Value(data=-1.3153849529019115, grad=0.0)]\n```\n:::\n:::\n\n\n::: {#054b73e6 .cell execution_count=73}\n``` {.python .cell-code code-fold=\"false\"}\nl\n```\n\n::: {.cell-output .cell-output-display execution_count=8701}\n```\nLayer of [LinearNeuron(2), LinearNeuron(2), LinearNeuron(2)]\n```\n:::\n:::\n\n\n::: {#94b228bb .cell execution_count=74}\n``` {.python .cell-code code-fold=\"false\"}\nl.parameters()\n```\n\n::: {.cell-output .cell-output-display execution_count=8702}\n```\n[Value(data=-0.42043677081902886, grad=0.0),\n Value(data=-0.9570205894681822, grad=0.0),\n Value(data=0.0, grad=0.0),\n Value(data=0.6751559513251457, grad=0.0),\n Value(data=0.11290864530486688, grad=0.0),\n Value(data=0.0, grad=0.0),\n Value(data=0.28458872586489115, grad=0.0),\n Value(data=-0.6281874682105646, grad=0.0),\n Value(data=0.0, grad=0.0)]\n```\n:::\n:::\n\n\n## Initialize and Evaluate an MLP\n\nWe'll instantiate an MLP like the picture below.\n\n![](figs/NN-figs/neural_net2.jpeg){width=\"25%\" fig-align=\"center\"}\n\n::: {#ef96d1e3 .cell execution_count=75}\n``` {.python .cell-code code-fold=\"false\"}\nx = [2.0, 3.0, -1.0]\nm = MLP(3, [4, 4, 1])\nm(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=8703}\n```\nValue(data=0.163738954227571, grad=0.0)\n```\n:::\n:::\n\n\n::: {#f8110453 .cell execution_count=76}\n``` {.python .cell-code code-fold=\"false\"}\nm\n```\n\n::: {.cell-output .cell-output-display execution_count=8704}\n```\nMLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)], Layer of [ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4)], Layer of [LinearNeuron(4)]]\n```\n:::\n:::\n\n\n::: {#58449f46 .cell execution_count=77}\n``` {.python .cell-code code-fold=\"false\"}\n# m.parameters()\n```\n:::\n\n\n::: {#d6142b6d .cell execution_count=78}\n``` {.python .cell-code code-fold=\"false\"}\ndraw_dot(m(x))\n```\n\n::: {.cell-output .cell-output-display execution_count=8706}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-79-output-1.svg){}\n:::\n:::\n\n\n# Training Loop\n\n## Training Loop\n\nSo after manually iterating, we put it all together in a training loop. \n\nLet's define inputs, targets and initialize the MLP.\n\n::: {#dd8187ac .cell execution_count=79}\n``` {.python .cell-code code-fold=\"false\"}\n# Define 4 different sets of inputs\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\n# For each input set, we have a desired target value -- binary classification\n# ys = [1.0, -1.0, -1.0, 1.0]\nys = [1.0, 0.0, 0.0, 1.0]\n\n# Manually seed the Random Number Generator for Reproducibility\n# You can comment the next line out see the variability\nrandom.seed(1) \n\n# Initialize an MLP with random weights\nm = MLP(3, [4, 4, 1])\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Training Loop\n:::\n\nNow we can run the training loop.\n\n::: {#c209ef61 .cell execution_count=80}\n``` {.python .cell-code code-fold=\"false\"}\nlosses = []\nniters = 100\nstep_size = 0.01\n\nfor k in range(niters):\n\n    # Training Step 1: forward pass\n    ypred = [m(x) for x in xs]\n    \n    # Training Step 2: Calculate the loss\n    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n    losses.append(loss.data)\n\n    # Training Step 3: Zero the gradients and run the backward pass\n    m.zero_grad()\n    loss.backward()\n\n    # Training Step 4: Update parameters\n    for p in m.parameters():\n        p.data += -step_size * p.grad\n\n    # print(k, loss.data)\n\nprint(\"Final Loss: \", loss.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal Loss:  0.007386422402034709\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Training Loop\n:::\n\nLet's plot the loss per iteration.\n\n::: {#251ca888 .cell execution_count=81}\n``` {.python .cell-code code-fold=\"false\"}\nplt.plot(losses)\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Loss Per Iteration\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8709}\n```\nText(0.5, 1.0, 'Loss Per Iteration')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-82-output-2.png){width=799 height=449}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Training Loop\n:::\n\nLet's look at the ground truth and predictions.\n\n::: {#5c7b5d5b .cell execution_count=82}\n``` {.python .cell-code code-fold=\"false\"}\nprint(type(ypred))\nprint(ypred[0].data)\n\nlist(zip(ys, [y.data for y in ypred]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'list'>\n1.0120099058492713\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8710}\n```\n[(1.0, np.float64(1.0120099058492713)),\n (0.0, np.float64(0.07244688900469765)),\n (0.0, np.float64(-0.035390875110888426)),\n (1.0, np.float64(0.9727765028706291))]\n```\n:::\n:::\n\n\n## Build and Train the MLP in PyTorch\n\nDefine the MLP in PyTorch.\n\n::: {#ce16d336 .cell execution_count=83}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nfrom torch import nn\nfrom torch.optim import SGD\n\n# Manually seed the Random Number Generator for Reproducibility\n# You can comment the next line out see the variability\ntorch.manual_seed(99)\n\n# Step 1: Define the MLP model\nclass ptMLP(nn.Module):\n    def __init__(self):\n        super(ptMLP, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(3, 4),\n            nn.ReLU(),\n            nn.Linear(4, 4),\n            nn.ReLU(),\n            nn.Linear(4, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodel = ptMLP()\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nptMLP(\n  (layers): Sequential(\n    (0): Linear(in_features=3, out_features=4, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=4, out_features=4, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=4, out_features=1, bias=True)\n  )\n)\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Build and Train the MLP in PyTorch\n:::\n\nDefine a loss function and an optimizer.\n\n::: {#538f169f .cell execution_count=84}\n``` {.python .cell-code code-fold=\"false\"}\n# Step 2: Define a loss function and an optimizer\ncriterion = nn.MSELoss(reduction='sum')\noptimizer = SGD(model.parameters(), lr=0.01)\n```\n:::\n\n\nCreate the tiny dataset.\n\n::: {#60440e38 .cell execution_count=85}\n``` {.python .cell-code code-fold=\"false\"}\n# Step 3: Create a tiny dataset\nxs = [\n    [2.0, 3.0, -1.0],\n    [3.0, -1.0, 0.5],\n    [0.5, 1.0, 1.0],\n    [1.0, 1.0, -1.0]\n]\n\n# we had to transpose ys for torch.tensor\nys_transpose = [[1.0], \n      [0.0], \n      [0.0], \n      [1.0]]\n\ninputs = torch.tensor(xs)\noutputs = torch.tensor(ys_transpose)\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Build and Train the MLP in PyTorch\n:::\n\nNow run the training loop.\n\n::: {#61d0b294 .cell execution_count=86}\n``` {.python .cell-code code-fold=\"false\"}\n# Step 4: Write the training loop\nlosses = []\nniters = 100\n\nfor epoch in range(niters):\n\n    # Training Step 1: Forward pass\n    predictions = model(inputs)\n\n    # Training Step 2: Calculate the loss\n    loss = criterion(predictions, outputs)\n\n    # Training Step 3: Zero the gradient and run backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Training Step 4: Update parameters\n    optimizer.step()\n\n    losses.append(loss.item())\n    # print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n\nprint(f'Final Loss: {loss.item()}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal Loss: 0.06534025073051453\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Build and Train the MLP in PyTorch\n:::\n\nLet's plot the loss per iteration.\n\n::: {#f011a8aa .cell execution_count=87}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(losses)\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Loss Per Iteration\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8715}\n```\nText(0.5, 1.0, 'Loss Per Iteration')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](24-NN-II-Backprop_files/figure-revealjs/cell-88-output-2.png){width=812 height=449}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Build and Train the MLP in PyTorch\n:::\n\nLet's look at the predictions.\n\n::: {#a2161e28 .cell execution_count=88}\n``` {.python .cell-code}\nlist(zip(ys, [y.item() for y in predictions]))\n```\n\n::: {.cell-output .cell-output-display execution_count=8716}\n```\n[(1.0, 1.130600094795227),\n (0.0, 0.03639908879995346),\n (0.0, 0.017568498849868774),\n (1.0, 0.7840131521224976)]\n```\n:::\n:::\n\n\n## To Dig a Little Deeper\n\n[PyTorch Quick Start Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\n[TensorFlow Playground](https://playground.tensorflow.org/#activation=relu&batchSize=30&dataset=gauss&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=4,4&seed=0.75152&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n\n## Summary\n\nSo today we...\n\n* got a glimpse of the wide applications of neural networks\n* revisited loss functions\n* developed the notion of gradient descent first by intuition, then in the univariate case, then the multivariate case\n* defined artificial neurons\n* implemented a computation graph and visualization\n* implemented the chain rule as backpropagation on the computation graph\n* defined Neuron, Layer and MLP modules which completes are homegrown Neural Network Framework\n* then trained a small MLP on a tiny dataset\n* finally implemented the same in PyTorch\n\n",
    "supporting": [
      "24-NN-II-Backprop_files"
    ],
    "filters": [],
    "includes": {}
  }
}