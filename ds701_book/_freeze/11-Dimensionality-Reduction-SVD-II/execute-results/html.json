{
  "hash": "d79e97a91e21ca3c9b04193f010ca030",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Dimensionality Reduction and PCA -- SVD II\njupyter: python3\n---\n\n\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/11-Dimensionality-Reduction-SVD-II.ipynb)\n\nIn the last section we learned about the SVD as a tool for constructing low-rank matrices.\n\nToday we'll look at it as a way to transform our data objects.\n\n\n::: {.content-visible when-profile=\"slides\"}\n## SVD\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$, recall that the SVD is\n\n$$\nA = U\\Sigma V^T,\n$$\n\nwhere $U\\in\\mathbb{R}^{m\\times m}$ is an orthogonal matrix, $\\Sigma\\in\\mathbb{R}^{m\\times n}$ is a rectangular diagonal matrix, and $V\\in\\mathbb{R}^{n\\times n}$ is an orthogonal matrix.\n\nNotice that $U$ contains a row for each data object.   \n\n::: {.content-visible when-profile=\"slides\"}\n## Dimensionality reduction\n:::\n\nWe can select the $k$ largest singular values and form the matrix:\n\n$$\nA^{(k)} =\n\\begin{bmatrix}\n\\vdots &  \\vdots  \\\\\n\\mathbf{u}_1 & \\mathbf{u}_{k} \\\\\n\\vdots& \\vdots  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{1} & &    \\\\\n & \\ddots &    \\\\\n&  & \\sigma_{k}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\cdots & \\mathbf{v}_1 & \\cdots   \\\\\n\\cdots & \\mathbf{v}_k & \\cdots   \\\\\n\\end{bmatrix}.\n$$\n\n:::: {.fragment}\nBy forming $A^{(k)}$ we have transformed the data objects that live in an $n$ dimensional space to a $k$ dimensional space, where $k$ is (probably much) smaller than $n$.\n::::\n\n:::: {.fragment}\nThis is an example of __dimensionality reduction.__\n::::\n\n:::: {.fragment}\nWhen we take our data to be the rows of $U\\Sigma$ instead of the rows of $A$, we are __reducing the dimension__ of our data from $n$ dimensions to $k$ dimensions.\n::::\n\n:::: {.fragment}\nWe will see today that the SVD is the __optimal__ transformation of the data into $k$ dimensions. It is optimal in the sense that it captures the maximum variance in the data.\n::::\n\n::: {.content-visible when-profile=\"web\"}\nAn excellent reference for the following section is at this [link](https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/).\n\nSome figures and discusion are taken from there.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Dimensionality reduction continued\n:::\n\nIn order to better understand how the SVD provides us with this optimal transformation, let's consider the following criterion for a $k$-dimensional transformation:\n\n:::: {.fragment}\n__Find the $k$-dimensional hyperplane that is \"closest\" to the points.__\n::::\n\n:::: {.fragment}\nMore precisely:\n\nGiven n points in $\\mathbb{R}^n$, find the hyperplane (affine space) of dimension $k$ with the property that the squared distance of the points to their orthogonal projection onto the hyperplane is minimized.\n::::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Dimensionality reduction continued\n:::\n\n![](figs/pca_figure1.jpeg){fig-align=\"center\" width=\"60%\"}\n\n[Source](https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/)\n\n::: {.content-visible when-profile=\"web\"}\nThis sounds like an appealing criterion.   \n\nBut it also turns out to have a strong statistical guarantee.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Dimensionality reduction continued\n\n__Find the $k$-dimensional hyperplane that is \"closest\" to the points.__\n:::\n\n:::: {.fragment}\nIn fact, this criterion is a transformation that captures the maximum __variance__ in the data.\n::::\n\n:::: {.fragment}\nThat is, the resulting $k$-dimensional dataset is the one with maximum variance.\n\n::::\n\n:::: {.fragment}\nLet's see why this is the case.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Centroid\n:::\n\nFirst, let's recall the idea of a dataset __centroid.__\n\n:::: {.fragment}\nGiven a $m\\times n$ data matrix $X$ with observations on the rows (as always) define\n\n$$\\bar{\\mathbf{x}}^T = \\frac{1}{m}\\mathbf{1}^TX.$$\n::::\n\n:::: {.fragment}\nThe vector $\\bar{\\mathbf{x}}^T$ is the centroid vector (mean vector) where the $i$-th entry is the average over the $i$-th row.\n::::\n\n:::: {.fragment}\nIt is the \"center of mass\" of the dataset.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Sample variance\n:::\n\nNext, recall the sample variance of a dataset is:\n    \n$$ \\operatorname{Var}(X) = \\frac{1}{m}\\sum_{j=1}^{m}\\Vert \\mathbf{x}_j^T - \\bar{\\mathbf{x}}^T\\Vert^2 $$\n\nwhere $\\mathbf{x}_j^T$ is row $j$ of $X$.\n\n:::: {.fragment}\nThe sample variance of the set of points is the average squared distance from each point to the centroid. \n::::\n\n:::: {.fragment}\nIf we move the points (translate each point by some constant amount), the sample variance does not change.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Sample variance continued\n:::\n\nSo, let's move the points to be __centered__ on the __origin.__\n\n$$ \\tilde{X} = X - \\mathbf{1}\\overline{\\mathbf{x}}^T $$\n\n:::: {.fragment}\nThe sample variance of the new points $\\tilde{X}$ is the same as the old points $X$, but the centroid of the new point set is the origin.\n::::\n\n:::: {.fragment}\nNow that the mean of the points is the zero vector, we can reason geometrically.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Centered data\n:::\n\nHere is a picture to show why the distance-minimizing subspace is variance-maximizing.\n\nIn this figure,   \n\n:::: {.columns}\n::: {.column width=\"40%\"}\n* the red point is one example point from $\\tilde{X}$, \n* the green point is the origin / centroid, and\n* the blue point is the $k$-dimensional projection of the red point.\n:::\n::: {.column width=\"60%\"}\n![](figs/pca_figure5.jpeg){fig-align=\"center\" width=\"75%\"}\n    \n[Source](https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/)\n:::\n::::\n\n::: {.content-visible when-profile=\"web\"}\nThe length of the black line is fixed -- it is the distance of the original point from the origin.\n\nSo the squared length of the black line is this point's contribution to the sample variance.\n\nNow, regardless of how we shift the green line, a right triangle is formed because the projection is orthogonal. \n\nSo -- by virtue of the Pythagorean Theorem, the __blue line squared__ plus the __red line squared__ equals the __black line squared.__\n\nWhich means that when we shift the subspace (green line) so as to __minimize the squared distances to all the example points__ $\\tilde{X}$ (blue lines) we automatically __maximize the squared distance of all the resulting blue points to the origin__ (red lines).\n\nAnd the squared distance of the blue point from the origin (red dashed line) is its contribution to the new $k$-dimensional sample variance.\n\nIn other words, the __distance-minimizing__ projection is the __variance-maximizing__ projection!\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Centering data example\n:::\n\nFor example, here is a dataset $X$ in $\\mathbb{R}^2$.\n\n::: {#abb7f74f .cell execution_count=1}\n``` {.python .cell-code}\ndef centerAxes(ax):\n    ax.spines['left'].set_position('zero')\n    ax.spines['right'].set_color('none')\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['top'].set_color('none')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n    bounds = np.array([ax.axes.get_xlim(), ax.axes.get_ylim()])\n    ax.plot(bounds[0][0], bounds[1][0], '')\n    ax.plot(bounds[0][1], bounds[1][1], '')\n```\n:::\n\n\n::: {#6c10a9c4 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nn_samples = 500\nC = np.array([[0.1, 0.6], [2., .6]])\nX = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])\nax = plt.figure(figsize = (6, 6)).add_subplot()\nplt.xlim([-12, 12])\nplt.ylim([-7, 7])\ncenterAxes(ax)\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], s=10, alpha=1)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-3-output-1.png){width=466 height=463 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Centering data example continued\n:::\n\nBy taking \n$$\n\\tilde{X} = X - \\mathbf{1}\\bar{\\mathbf{x}}^T \n$$\n\n::: {#caa2255c .cell execution_count=3}\n``` {.python .cell-code}\nXc = X - np.mean(X, axis=0)\nax = plt.figure(figsize=(6, 5)).add_subplot()\nplt.xlim([-12, 12])\nplt.ylim([-7, 7])\ncenterAxes(ax)\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)\nplt.scatter(Xc[:, 0], Xc[:, 1], s=10, alpha=0.8, color='r')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-4-output-1.png){width=466 height=389 fig-align='center'}\n:::\n:::\n\n\nwe translate each point so that the new mean is the origin.\n\n::: {.content-visible when-profile=\"slides\"}\n## Centering data example continued\n:::\n\nThe last step is to find the $k$-dimensional subspace that minimizes the distance between the data (red points) and their projection on the subspace.\n\n::: {.content-visible when-profile=\"web\"}\nRemember that the $\\ell_2$ norm of a vector difference is Euclidean distance.\n\n:::\n\n:::: {.fragment}\nIn other words, what rank $k$ matrix $X^{(k)} \\in \\mathbb{R}^{m\\times k}$ is closest to $\\tilde{X}$?\n::::\n\n:::: {.fragment}\nWe seek\n\n$$X^{(k)} =\\mathop{\\arg\\min}\\limits_{\\{B~|~\\operatorname{Rank} B = k\\}} \\Vert \\tilde{X}-B\\Vert_F.$$\n::::\n\n:::: {.fragment}\nWe know how to find this matrix -- as we showed in the last lecture, we obtain it via the SVD!\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Centering data example continued\n:::\n\nSo for this case, let's construct the best 1-D approximation of the mean-centered data.\n\n::: {#c29f6482 .cell execution_count=4}\n``` {.python .cell-code}\nXc = X - np.mean(X, axis=0)\nu, s, vt = np.linalg.svd(Xc, full_matrices=False)\nscopy = s.copy()\nscopy[1] = 0.\nreducedX = u @ np.diag(scopy) @ vt\n```\n:::\n\n\n::: {#e9433171 .cell execution_count=5}\n``` {.python .cell-code}\nax = plt.figure(figsize=(6, 5)).add_subplot()\ncenterAxes(ax)\nplt.axis('equal')\nplt.scatter(Xc[:,0], Xc[:,1], color='r')\nplt.scatter(reducedX[:,0], reducedX[:,1])\nendpoints = np.array([[-10], [10]]) @ vt[[0], :]\nplt.plot(endpoints[:, 0], endpoints[:, 1], 'g-')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-6-output-1.png){width=482 height=389 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Principal component analysis (PCA)\n:::\n\nThis method is called __Principal Component Analysis.__\n\n:::: {.fragment}\nIn summary, PCA consists of:\n\n:::: {.incremental}\n1. Mean center the data, and\n2. Reduce the dimension of the mean-centered data via SVD.\n::::\n::::\n\n:::: {.fragment}\nThis is equivalent to projecting the data onto the hyperplane that captures the maximum variance in the data.\n::::\n\n:::: {.fragment}\nIt winds up constructing the __best low dimensional linear approximation of the data.__\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## PCA continued\n:::\n\nWhat are \"principal components\"?\n\n:::: {.fragment}\nThese are nothing more than the columns of $U$ (or the rows of $V^T$).  Because they capture the direction of maximum variation, they are called \"principal\" components.\n::::\n\n## Uses of PCA/SVD\n\nThere are many uses of PCA (and SVD).\n\nWe'll cover two use cases:\n\n:::: {.incremental}\n1. Visualization\n2. Denoising\n::::\n\n:::: {.fragment}\nAs already mentioned, SVD is also useful for data compression -- we won't discuss it in detail, but it is the principle behind audio and video compression (MP3s, HDTV, etc).\n\nSVD is also useful for anomaly detection, though we won't cover it here.\n::::\n\n## Visualization and Denoising -- Extended Example\n\nWe will study both visualization and denoising in the context of text processing.\n\n:::: {.fragment}\nAs we have seen, a common way to work with documents is using the bag-of-words model (perhaps considering n-grams), which results in a term-document matrix.\n::::\n\n:::: {.fragment}\nEntries in the matrix are generally TF-IDF scores.\n::::\n\n:::: {.fragment}\nOften, terms are correlated -- they appear together in combinations that suggest a certain \"concept\".\n::::\n\n:::: {.fragment}\nThat is, term-document matrices often show low effective rank -- many columns can be approximated as combinations of other columns.\n::::\n\n:::: {.fragment}\nWhen PCA is used for dimensionality reduction of documents, it tends to to extract these \"concept\" vectors.\n::::\n\n:::: {.fragment}\nThe application of PCA to term-document matrices is called __Latent Semantic Analysis (LSA).__\n::::\n\n::: {.content-visible when-profile=\"web\"}\nAmong other benefits, LSA can improve the performance of clustering of documents.\n\nThis happens because the important concepts are captured in the most significant principal components.\n:::\n\n## Data: 20 Newsgroups\n\n::: {#feafa975 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_20newsgroups\n\ncategories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset='train', categories=categories)\n\nprint(news_data.target_names)\nprint(news_data.target)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['comp.os.ms-windows.misc', 'rec.sport.baseball', 'sci.space']\n[2 0 0 ... 2 1 2]\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n### Basic Clustering\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Basic Clustering\n:::\n\nTo get started, let's compute tf-idf scores.\n\nNotice that we will let the tokenizer compute $n$-grams for $n=1$ and $n=2$.  \n\nAn $n$-gram is a set of $n$ consecutive terms.\n\nWe'll compute a document-term matrix `dtm`.\n\n::: {#7d89d4e3 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(stop_words='english', min_df=4, max_df=0.8)\ndtm = vectorizer.fit_transform(news_data.data)\n\nprint(\"Type of dtm: \", type(dtm))\nprint(\"Shape of dtm: \", dtm.shape)\nterms = vectorizer.get_feature_names_out()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nType of dtm:  <class 'scipy.sparse._csr.csr_matrix'>\nShape of dtm:  (1781, 9409)\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Basic Clustering continued\n:::\n\nAs a comparison case, let's first cluster the documents using the raw tf-idf scores.\n\nThis is without any use of PCA, and so includes lots of noisy or meaningless terms.\n\n::: {#bf6658e6 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.cluster import KMeans\nk = 3\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10, random_state = 0)\nkmeans.fit_predict(dtm)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Basic Clustering continued\n:::\nLet's evaluate the clusters.  We'll assume that the newgroup the article came from is the 'ground truth.'\n\n::: {#0fa90b3c .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\nimport sklearn.metrics as metrics\nri = metrics.adjusted_rand_score(labels, news_data.target)\nss = metrics.silhouette_score(dtm, kmeans.labels_, metric='euclidean')\n\nprint('Rand Index is {}'.format(ri))\nprint('Silhouette Score is {}'.format(ss))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRand Index is 0.8602675268919155\nSilhouette Score is 0.009524355852520665\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n### Improvement: Stemming\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Improvement: Stemming\n:::\n\nOne source of noise that we can eliminate (before we use LSA) comes from word endings.\n\n:::: {.fragment}\nFor example: a Google search on 'run' will return web pages on 'running.'\n::::\n\n:::: {.fragment}\nThis is useful, because the difference between 'run' and 'running' in practice is not enough to matter.\n::::\n\n:::: {.fragment}\nThe usual solution taken is to simply 'chop off' the part of the word that indicates a variation from the base word.\n::::\n\n::: {.content-visible when-profile=\"web\"}\n(For those of you who studied Latin or Greek, this will sound familiar -- we are removing the 'inflection.')\n:::\n\n:::: {.fragment}\nThe process is called 'stemming.'\n::::\n\n:::: {.fragment}\nA very good stemmer is the \"Snowball\" stemmer.\n::::\n\n::: {.content-visible when-profile=\"web\"}\nYou can read more at http://www.nltk.org and http://www.nltk.org/howto/stem.html.\n\nInstallation Note: From a cell you need to call `nltk.download()` and select the appropriate packages from the interface that appears. In particular you need to download: `stopwords` from _corpora_ and `punkt` and `snowball_data` from _models._\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Stemming continued\n:::\nLet's stem the data using the Snowball stemmer:\n\n::: {#a95505aa .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('snowball_data')\nnltk.download('wordnet')\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nstemmed_data = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n         for sent in sent_tokenize(message)\n        for word in word_tokenize(sent))\n        for message in news_data.data]\n\ndtm = vectorizer.fit_transform(stemmed_data)\nterms = vectorizer.get_feature_names_out()\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Clustering with stemming\n:::\nAnd now let's see how well we can cluster on the stemmed data.\n\n::: {#0ba74b7a .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.cluster import KMeans\nimport sklearn.metrics as metrics\n\nk = 3\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10,random_state=0)\nkmeans.fit_predict(dtm)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n\nri = metrics.adjusted_rand_score(labels, news_data.target)\nss = metrics.silhouette_score(dtm, kmeans.labels_, metric='euclidean')\nprint('Rand Index is {}'.format(ri))\nprint('Silhouette Score is {}'.format(ss))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRand Index is 0.9078408032531082\nSilhouette Score is 0.010845942480799687\n```\n:::\n:::\n\n\nSo the Rand Index went from 0.86 to 0.90 as a result of stemming.\n\n## Demonstrating PCA\n\n::: {.content-visible when-profile=\"web\"}\nOK.  Now, let's apply PCA.\n:::\n\nOur data matrix `dtm` is in sparse form.\n\n:::: {.fragment}\nFirst, we mean center the data. Note that when `dtm` is mean centered it is no longer sparse.\n::::\n\n:::: {.fragment}\nThen we use PCA to reduce the dimension of the mean-centered data.\n::::\n\n:::: {.fragment}\n\n::: {#2ae79417 .cell execution_count=12}\n``` {.python .cell-code}\ndtm_dense = dtm.todense()\ncentered_dtm = dtm_dense - np.mean(dtm_dense, axis=0)\nu, s, vt = np.linalg.svd(centered_dtm)\n```\n:::\n\n\n::::\n\n:::: {.fragment}\nNote that if you have sparse data, you may want to use `scipy.sparse.linalg.svds()` and for large data it may be advantageous to use `sklearn.decomposition.TruncatedSVD()`.\n::::\n\n:::: {.fragment}\nThe principal components (rows of $V^T$) encode the extracted concepts.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Demonstrating PCA continued\n:::\n\nEach LSA __concept__ is a linear combination of words.\n\n::: {#1b5bbc46 .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\npd.DataFrame(vt, columns=vectorizer.get_feature_names_out())\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>0005</th>\n      <th>0062</th>\n      <th>0096b0f0</th>\n      <th>00bjgood</th>\n      <th>00mbstultz</th>\n      <th>01</th>\n      <th>0114</th>\n      <th>01wb</th>\n      <th>...</th>\n      <th>zri</th>\n      <th>zrlk</th>\n      <th>zs</th>\n      <th>zt</th>\n      <th>zu</th>\n      <th>zv</th>\n      <th>zw</th>\n      <th>zx</th>\n      <th>zy</th>\n      <th>zz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007831</td>\n      <td>0.012323</td>\n      <td>0.000581</td>\n      <td>0.005558</td>\n      <td>0.001032</td>\n      <td>0.002075</td>\n      <td>0.002008</td>\n      <td>0.005575</td>\n      <td>0.001247</td>\n      <td>0.000813</td>\n      <td>...</td>\n      <td>-0.000028</td>\n      <td>-0.000025</td>\n      <td>-0.000200</td>\n      <td>-0.000025</td>\n      <td>-0.000128</td>\n      <td>-0.000207</td>\n      <td>-0.000087</td>\n      <td>-0.000150</td>\n      <td>-0.000113</td>\n      <td>0.000534</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.005990</td>\n      <td>0.009540</td>\n      <td>0.002089</td>\n      <td>-0.010679</td>\n      <td>-0.001646</td>\n      <td>-0.003477</td>\n      <td>-0.002687</td>\n      <td>0.002143</td>\n      <td>-0.003394</td>\n      <td>0.002458</td>\n      <td>...</td>\n      <td>-0.000015</td>\n      <td>-0.000013</td>\n      <td>-0.000054</td>\n      <td>-0.000013</td>\n      <td>-0.000042</td>\n      <td>-0.000100</td>\n      <td>-0.000026</td>\n      <td>-0.000064</td>\n      <td>-0.000040</td>\n      <td>-0.001041</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.012630</td>\n      <td>-0.011904</td>\n      <td>-0.002443</td>\n      <td>0.001438</td>\n      <td>0.000439</td>\n      <td>0.000044</td>\n      <td>0.000349</td>\n      <td>-0.006817</td>\n      <td>0.000692</td>\n      <td>-0.001124</td>\n      <td>...</td>\n      <td>-0.000095</td>\n      <td>-0.000086</td>\n      <td>-0.000289</td>\n      <td>-0.000087</td>\n      <td>-0.000252</td>\n      <td>-0.000576</td>\n      <td>-0.000134</td>\n      <td>-0.000293</td>\n      <td>-0.000204</td>\n      <td>-0.000013</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.013576</td>\n      <td>0.017639</td>\n      <td>0.003552</td>\n      <td>0.001148</td>\n      <td>0.003354</td>\n      <td>-0.000410</td>\n      <td>0.000622</td>\n      <td>0.011649</td>\n      <td>0.002237</td>\n      <td>0.001969</td>\n      <td>...</td>\n      <td>0.000205</td>\n      <td>0.000186</td>\n      <td>0.000486</td>\n      <td>0.000172</td>\n      <td>0.000464</td>\n      <td>0.001142</td>\n      <td>0.000220</td>\n      <td>0.000508</td>\n      <td>0.000352</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.002254</td>\n      <td>-0.004619</td>\n      <td>-0.005458</td>\n      <td>-0.001938</td>\n      <td>-0.000251</td>\n      <td>0.000689</td>\n      <td>0.000043</td>\n      <td>-0.002620</td>\n      <td>-0.000533</td>\n      <td>0.001434</td>\n      <td>...</td>\n      <td>-0.000310</td>\n      <td>-0.000283</td>\n      <td>-0.000775</td>\n      <td>-0.000252</td>\n      <td>-0.000698</td>\n      <td>-0.001714</td>\n      <td>-0.000331</td>\n      <td>-0.000728</td>\n      <td>-0.000529</td>\n      <td>-0.000961</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8048</th>\n      <td>0.000047</td>\n      <td>-0.000085</td>\n      <td>-0.000199</td>\n      <td>0.000033</td>\n      <td>-0.002095</td>\n      <td>-0.000735</td>\n      <td>-0.000557</td>\n      <td>0.000497</td>\n      <td>-0.000407</td>\n      <td>-0.000198</td>\n      <td>...</td>\n      <td>-0.001449</td>\n      <td>-0.002008</td>\n      <td>0.000131</td>\n      <td>-0.000929</td>\n      <td>0.000642</td>\n      <td>0.976202</td>\n      <td>-0.000030</td>\n      <td>0.000284</td>\n      <td>-0.000041</td>\n      <td>0.000658</td>\n    </tr>\n    <tr>\n      <th>8049</th>\n      <td>0.000137</td>\n      <td>0.000250</td>\n      <td>-0.000049</td>\n      <td>-0.000149</td>\n      <td>0.001291</td>\n      <td>-0.000104</td>\n      <td>-0.000018</td>\n      <td>-0.000710</td>\n      <td>-0.000040</td>\n      <td>-0.000350</td>\n      <td>...</td>\n      <td>0.000114</td>\n      <td>0.000102</td>\n      <td>-0.000523</td>\n      <td>-0.000170</td>\n      <td>-0.001086</td>\n      <td>-0.000133</td>\n      <td>0.999364</td>\n      <td>-0.000521</td>\n      <td>-0.000747</td>\n      <td>-0.000892</td>\n    </tr>\n    <tr>\n      <th>8050</th>\n      <td>-0.000184</td>\n      <td>0.000979</td>\n      <td>-0.000018</td>\n      <td>-0.000616</td>\n      <td>-0.000292</td>\n      <td>-0.000500</td>\n      <td>-0.000221</td>\n      <td>0.000033</td>\n      <td>0.000120</td>\n      <td>0.000318</td>\n      <td>...</td>\n      <td>0.000246</td>\n      <td>0.000210</td>\n      <td>-0.001593</td>\n      <td>-0.000179</td>\n      <td>-0.000888</td>\n      <td>0.000154</td>\n      <td>-0.000516</td>\n      <td>0.996858</td>\n      <td>-0.001115</td>\n      <td>0.000817</td>\n    </tr>\n    <tr>\n      <th>8051</th>\n      <td>0.000020</td>\n      <td>0.000596</td>\n      <td>-0.000162</td>\n      <td>-0.000330</td>\n      <td>0.000522</td>\n      <td>-0.000119</td>\n      <td>-0.000112</td>\n      <td>-0.000707</td>\n      <td>-0.000050</td>\n      <td>0.000093</td>\n      <td>...</td>\n      <td>0.000201</td>\n      <td>0.000184</td>\n      <td>-0.000607</td>\n      <td>-0.000288</td>\n      <td>-0.001271</td>\n      <td>-0.000157</td>\n      <td>-0.000769</td>\n      <td>-0.001136</td>\n      <td>0.998706</td>\n      <td>-0.000867</td>\n    </tr>\n    <tr>\n      <th>8052</th>\n      <td>0.000100</td>\n      <td>0.000260</td>\n      <td>0.000651</td>\n      <td>-0.000180</td>\n      <td>-0.003344</td>\n      <td>0.002136</td>\n      <td>0.001585</td>\n      <td>-0.001141</td>\n      <td>0.000223</td>\n      <td>0.000034</td>\n      <td>...</td>\n      <td>-0.000003</td>\n      <td>0.000017</td>\n      <td>0.000071</td>\n      <td>-0.001073</td>\n      <td>-0.002255</td>\n      <td>0.000697</td>\n      <td>-0.000814</td>\n      <td>0.000833</td>\n      <td>-0.000757</td>\n      <td>0.978108</td>\n    </tr>\n  </tbody>\n</table>\n<p>8053 rows × 8053 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#47105240 .cell execution_count=14}\n``` {.python .cell-code}\nnames = np.array(vectorizer.get_feature_names_out())\nfor cl in range(3):\n    print(f'\\nPrincipal Component {cl}:')\n    idx = np.array(np.argsort(vt[cl]))[0][-10:]\n    for i in idx[::-1]:\n        print(f'{names[i]:12s} {vt[cl, i]:0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPrincipal Component 0:\nyear         0.140\ngame         0.111\nhenri        0.108\nteam         0.107\nspace        0.106\nnasa         0.091\ntoronto      0.086\nalaska       0.083\nplayer       0.079\nhit          0.077\n\nPrincipal Component 1:\nspace        0.260\nnasa         0.218\nhenri        0.184\ngov          0.135\norbit        0.134\nalaska       0.129\naccess       0.129\ntoronto      0.118\nlaunch       0.109\ndigex        0.102\n\nPrincipal Component 2:\nhenri        0.458\ntoronto      0.364\nzoo          0.228\nedu          0.201\nspencer      0.194\nzoolog       0.184\nalaska       0.123\nwork         0.112\numd          0.096\nutzoo        0.092\n```\n:::\n:::\n\n\nThe rows of $U$ correspond to documents, which are linear combinations of __concepts__.\n\n## Denoising\n\nIn order to improve our clustering accuracy, we will __exclude__ the less significant concepts from the documents' feature vectors.\n\nThat is, we will choose the leftmost $k$ columns of $U$ and the topmost $k$ rows of $V^T$.  \n\nThe reduced set of columns of $U$ are our new document encodings, and it is those that we will cluster.\n\n::: {.content-visible when-profile=\"slides\"}\n## Denoising continued\n:::\n\n::: {#0261d89b .cell execution_count=15}\n``` {.python .cell-code}\nplt.xlim([0, 50])\nplt.xlabel('Number of Principal Components (Rank $k$)')\nplt.ylabel('Singular Values')\nplt.plot(range(1, len(s)+1), s)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-16-output-1.png){width=585 height=431}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Denoising continued\n:::\n\n::: {#45061197 .cell execution_count=16}\n``` {.python .cell-code}\nri = []\nss = []\nmax = len(u)\nfor k in range(1, 50):\n    vectorsk = np.asarray(u[:, :k] @ np.diag(s[:k]))\n    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n    kmeans.fit_predict(vectorsk)\n    labelsk = kmeans.labels_\n    ri.append(metrics.adjusted_rand_score(labelsk, news_data.target))\n    ss.append(metrics.silhouette_score(vectorsk, kmeans.labels_, metric='euclidean'))\n\nplt.plot(range(1, 50), ri)\nplt.ylabel('Rand Score', size=20)\nplt.xlabel('No of Prin Comps', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-17-output-1.png){width=602 height=442}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Denoising continued\n:::\n\n::: {#1b861324 .cell execution_count=17}\n``` {.python .cell-code}\nplt.plot(range(1, 50), ss)\nplt.ylabel('Silhouette Score', size=20)\nplt.xlabel('No of Prin Comps', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-18-output-1.png){width=602 height=442}\n:::\n:::\n\n\nNote that we can get good accuracy and coherent clusters with just __two__ principal components.\n\n## Visualization\n\nThat's a good thing, because it means that we can also __visualize__ the data well with the help of PCA.\n\nRecall that the challenge of visualization is that the data lives in a high dimensional space.  \n\nWe can only look at 2 (or maybe 3) dimensions at a time, so it's not clear __which__ dimensions to look at.\n\nThe idea behind using PCA for visualization is that since low-numbered principal components capture most of the __variance__ in the data, these are the \"directions\" from which it is most useful to inspect the data.\n\nWe saw that the first two principal components were particularly large -- let's start by using them for visualization.\n\n::: {.content-visible when-profile=\"slides\"}\n## Visualization continued\n:::\n\n::: {#ba8fc7ba .cell execution_count=18}\n``` {.python .cell-code}\nimport seaborn as sns\nXk = u @ np.diag(s)\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 1,figsize=(5, 3))\n    cmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\n    for i, label in enumerate(set(news_data.target)):\n        point_indices = np.where(news_data.target == label)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D', label=news_data.target_names[i])\n        plt.legend(loc='best')\nsns.despine()\nplt.title('Ground Truth Labels', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-19-output-1.png){width=433 height=292 fig-align='center'}\n:::\n:::\n\n\nPoints in this plot have been labelled with their \"true\" (aka \"ground truth\") cluster labels.\n\nNotice how clearly the clusters separate and how coherently they present themselves. This is an excellent visualization that is provided by PCA.\n\nSince this visualization is so clear, we can use it to examine the results of our various clustering methods and get some insight into how they differ.\n\n::: {#d5d5da8e .cell execution_count=19}\n``` {.python .cell-code}\nk = 3\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10, random_state=0)\nkmeans.fit_predict(dtm)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n    cmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\n    for i in range(k):\n        point_indices = np.where(labels == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D', label=news_data.target_names[i])\nsns.despine()\nplt.title('Clusters On Full Dataset, Dimension = {}\\nRand Score = {:0.3f}'.format(dtm.shape[1], metrics.adjusted_rand_score(labels,news_data.target)), size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-20-output-1.png){width=607 height=322 fig-align='center'}\n:::\n:::\n\n\n::: {#a0675e1e .cell execution_count=20}\n``` {.python .cell-code}\nk = 3\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10,random_state=0)\nkmeans.fit_predict(np.asarray(Xk[:, :2]))\ncentroids = kmeans.cluster_centers_\nXklabels = kmeans.labels_\nerror = kmeans.inertia_\n\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n    cmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\n    for i, label in enumerate(set(news_data.target)):\n        point_indices = np.where(Xklabels == label)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D')\nsns.despine()\nplt.title('Clusters On PCA-reduced Dataset, Dimension = 2\\nRand Score = {:0.3f}'.format(metrics.adjusted_rand_score(Xklabels, news_data.target)), size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-21-output-1.png){width=682 height=322 fig-align='center'}\n:::\n:::\n\n\n::: {#b4d2dab6 .cell execution_count=21}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 3))\nplt.subplot(121)\ncmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\nfor i in range(k):\n        point_indices = np.where(labels == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D')\nsns.despine()\nplt.title('Dimension = {}\\nRand Score = {:0.3f}'.format(dtm.shape[1], metrics.adjusted_rand_score(labels, news_data.target)), size=14)\nplt.subplot(122)\ncmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\nfor i in range(k):\n        point_indices = np.where(Xklabels == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D')\nsns.despine()\nplt.title('Dimension = 2\\n Rand Score = {:0.3f}'.format(metrics.adjusted_rand_score(Xklabels, news_data.target)), size=14)\nplt.subplots_adjust(hspace=1.2)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-22-output-1.png){width=445 height=307 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Visualization continued\n:::\nWhat happens if we misjudge the number of clusters?  Let's form 6 clusters.\n\n::: {#c95296fb .cell execution_count=22}\n``` {.python .cell-code}\nk = 6\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10, random_state=0)\nkmeans.fit_predict(dtm)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n    cmap = sns.hls_palette(n_colors=k, h=0.35, l=0.4, s=0.9)\n    for i in range(k):\n        point_indices = np.where(labels == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D')\n    sns.despine()\nplt.title('Clusters On Full Dataset, Dimension = {}'.format(dtm.shape[1]),size=20)\n    \nplt.title(f'K means with six clusters on full dataset; Rand Score {metrics.adjusted_rand_score(labels,news_data.target):0.2f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-23-output-1.png){width=485 height=283 fig-align='center'}\n:::\n:::\n\n\n::: {#2fea6fe1 .cell execution_count=23}\n``` {.python .cell-code}\nk = 6\nnpc = 5\nkmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10,random_state=0)\nkmeans.fit_predict(np.asarray(Xk[:, :npc]))\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n    cmap = sns.hls_palette(n_colors=k, h=0.35, l=0.4, s=0.9)\n    for i in range(k):\n        point_indices = np.where(labels == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(np.ravel(Xk[point_indices, 0]), np.ravel(Xk[point_indices, 1]), s=20, alpha=0.5, color=cmap[i], marker='D')\n    sns.despine()\n\nplt.title(f'K means with six clusters on {npc} principal components; Rand Score {metrics.adjusted_rand_score(labels,news_data.target):0.2f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-24-output-1.png){width=583 height=283 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Visualization continued\n:::\nWhat about the other principal components?   Are they useful for visualization?\n\nA common approach is to look at all pairs of (low-numbered) principal components, to look for additional structure in the data.\n\n::: {#ed0417c5 .cell execution_count=24}\n``` {.python .cell-code}\nk = 5\nXk = u[:, :k] @ np.diag(s[:k])\nX_df = pd.DataFrame(Xk)\ng = sns.PairGrid(X_df)\ndef pltColor(x, y, color):\n    cmap = sns.hls_palette(n_colors=3, h=0.35, l=0.4, s=0.9)\n    for i in range(3):\n        point_indices = np.where(news_data.target == i)[0]\n        point_indices = point_indices.tolist()\n        plt.scatter(x[point_indices], y[point_indices], color=cmap[i], s=3)\nsns.despine()\ng.map(pltColor)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-html/cell-25-output-1.png){width=1182 height=1177 fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [
      "11-Dimensionality-Reduction-SVD-II_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}