{
  "hash": "09798c07e1360751091d1cab1903ada0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Logistic Regression\njupyter: python3\n---\n\n\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/18-Regression-II-Logistic.ipynb)\n\n\nSo far we have seen linear regression: a continuous valued observation is estimated as linear (or affine) function of the independent variables.\n\nToday we will look at the following situation.\n\nImagine that you are observing a binary variable -- a 0/1 value.\n\nThat is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n\nYou believe that there is some __probability__ of observing a 1, and that probability is a function of certain independent variables.\n\nSo the key properties of a problem that make it appropriate for logistic regression are:\n    \n* What you can observe is a __categorical__ variable\n* What you want to estimate is a __probability__ of seeing a particular value of the categorical variable.\n\n## What is the probability I will be admitted to Grad School?\n\n\n\n\n\n\n```{note}\nThe following example is based on http://www.ats.ucla.edu/stat/r/dae/logit.htm.\n```\n\n\n\n\n\n\nA researcher is interested in how variables, such as _GRE_ (Graduate Record Exam scores), _GPA_ (grade point average) and prestige of the undergraduate institution affect admission into graduate school. \n\nThe response variable, admit/don't admit, is a binary variable.\n\nThere are three predictor variables: __gre,__ __gpa__ and __rank.__ \n\n* We will treat the variables _gre_ and _gpa_ as continuous. \n* The variable _rank_ takes on the values 1 through 4. \n    * Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. \n\n::: {#354dad56 .cell hide_input='false' slideshow='{\"slide_type\":\"fragment\"}' execution_count=2}\n``` {.python .cell-code}\n# data source: http://www.ats.ucla.edu/stat/data/binary.csv\ndf = pd.read_csv('data/ats-admissions.csv') \ndf.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>admit</th>\n      <th>gre</th>\n      <th>gpa</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>380</td>\n      <td>3.61</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>660</td>\n      <td>3.67</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>800</td>\n      <td>4.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>640</td>\n      <td>3.19</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>520</td>\n      <td>2.93</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>760</td>\n      <td>3.00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>560</td>\n      <td>2.98</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>400</td>\n      <td>3.08</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>540</td>\n      <td>3.39</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>700</td>\n      <td>3.92</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#9fa566cf .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=3}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>admit</th>\n      <th>gre</th>\n      <th>gpa</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>400.000000</td>\n      <td>400.000000</td>\n      <td>400.000000</td>\n      <td>400.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.317500</td>\n      <td>587.700000</td>\n      <td>3.389900</td>\n      <td>2.48500</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.466087</td>\n      <td>115.516536</td>\n      <td>0.380567</td>\n      <td>0.94446</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>220.000000</td>\n      <td>2.260000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>520.000000</td>\n      <td>3.130000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>580.000000</td>\n      <td>3.395000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>660.000000</td>\n      <td>3.670000</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>800.000000</td>\n      <td>4.000000</td>\n      <td>4.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#35b5e5a1 .cell hide_input='false' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\ndf.hist(figsize = (10, 7));\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-5-output-1.png){width=798 height=579}\n:::\n:::\n\n\nLet's look at how each independent variable affects admission probability.\n\nFirst, __rank__:\n\n::: {#e316f412 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nax = df.groupby('rank').mean()['admit'].plot(marker = 'o',\n                                       fontsize = 12)\nax.set_ylabel('P[admit]', fontsize = 16)\nax.set_xlabel('Rank', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-6-output-1.png){width=611 height=440}\n:::\n:::\n\n\nNext, __GRE__:\n\n::: {#cd2fc9b8 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nax = df.groupby('gre').mean()['admit'].plot(marker = 'o',\n                                       fontsize = 12)\nax.set_ylabel('P[admit]', fontsize = 16)\nax.set_xlabel('GRE', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-7-output-1.png){width=601 height=440}\n:::\n:::\n\n\nFinally, __GPA__ (for this visualization, we aggregate GPA into 10 bins):\n\n::: {#46f60ca3 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\nbins = np.linspace(df.gpa.min(), df.gpa.max(), 10)\nax = df.groupby(np.digitize(df.gpa, bins)).mean()['admit'].plot(marker = 'o',\n                                       fontsize = 12)\nax.set_ylabel('P[admit]', fontsize = 16)\nax.set_xlabel('GPA', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-8-output-1.png){width=611 height=440}\n:::\n:::\n\n\nFurthermore, we can see that the independent variables are strongly correlated:\n\n::: {#2494722d .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\ndf1 = df[df['rank']==1]\ndf2 = df[df['rank']==2]\ndf3 = df[df['rank']==3]\ndf4 = df[df['rank']==4]\n#\nfig = plt.figure(figsize = (10, 5))\nax1 = fig.add_subplot(221)\ndf1.plot.scatter('gre','admit', ax = ax1)\nplt.title('Rank 1 Institutions')\nax2 = fig.add_subplot(222)\ndf2.plot.scatter('gre','admit', ax = ax2)\nplt.title('Rank 2 Institutions')\nax3 = fig.add_subplot(223, sharex = ax1)\ndf3.plot.scatter('gre','admit', ax = ax3)\nplt.title('Rank 3 Institutions')\nax4 = fig.add_subplot(224, sharex = ax2)\nplt.title('Rank 4 Institutions')\ndf4.plot.scatter('gre','admit', ax = ax4);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-9-output-1.png){width=812 height=449}\n:::\n:::\n\n\n## Logistic Regression\n\nLogistic regression is concerned with estimating a __probability.__\n\nHowever, all that is available are categorical observations, which we will code as 0/1.\n\nThat is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n\nNow, a linear function like $\\alpha + \\beta x$ cannot be used to predict probability directly, because the linear function takes on all values (from -$\\infty$ to +$\\infty$), and probability only ranges over $(0, 1)$.\n\nHowever, there is a transformation of probability that works: it is called __log-odds__.\n\nFor any probabilty $p$, the __odds__ is defined as $p/(1-p)$.   Notice that odds vary from 0 to $\\infty$, and odds < 1 indicates that $p < 1/2$.\n\nNow, there is a good argument that to fit a linear function, instead of using odds, we should use log-odds.  That is simply $\\log p/(1-p)$.\n\n::: {#b2bf6f49 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\npvec = np.linspace(0.01, 0.99, 100)\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.plot(pvec, np.log(pvec / (1-pvec)))\nax.tick_params(labelsize=12)\nax.set_xlabel('Probability', fontsize = 14)\nax.set_ylabel('Log-Odds', fontsize = 14)\nax.set_title('Logit Function: $\\log (p/1-p)$', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:7: SyntaxWarning: invalid escape sequence '\\l'\n<>:7: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_49027/389764126.py:7: SyntaxWarning: invalid escape sequence '\\l'\n  ax.set_title('Logit Function: $\\log (p/1-p)$', fontsize = 16);\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-10-output-2.png){width=522 height=389}\n:::\n:::\n\n\nSo, logistic regression does the following: it does a linear regression of $\\alpha + \\beta x$ against $\\log p/(1-p)$.\n\nThat is, it fits:\n\n$$\\alpha + \\beta x = \\log \\frac{p(x)}{1-p(x)}$$\n\n$$e^{\\alpha + \\beta x} = \\frac{p(x)}{1-p(x)}$$\n\n$$e^{\\alpha + \\beta x} (1-p(x)) = p(x)$$\n\n$$e^{\\alpha + \\beta x}  = p(x) + p(x)e^{\\alpha + \\beta x}$$\n\n$$\\frac{e^{\\alpha + \\beta x}}{1 +e^{\\alpha + \\beta x}} = p(x)$$\n\nSo, logistic regression fits a probability of the following form:\n        $$p(x) = P(y=1\\mid x) = \\frac{e^{\\alpha+\\beta x}}{1+e^{\\alpha+\\beta x}}$$\n        \nThis is a sigmoid function; when $\\beta > 0$, $x\\rightarrow \\infty$, then $p(x)\\rightarrow 1$ and when $x\\rightarrow -\\infty$, then $p(x)\\rightarrow 0$.\n\n::: {#693d59ca .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nalphas = [-4, -8,-12,-20]\nbetas = [0.4,0.4,0.6,1]\nx = np.arange(40)\nfig = plt.figure(figsize=(8, 6)) \nax = plt.subplot(111)\n\nfor i in range(len(alphas)):\n    a = alphas[i]\n    b = betas[i]\n    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n#     plt.plot(x,y,label=r\"$\\frac{e^{%d + %3.1fx}}{1+e^{%d + %3.1fx}}\\;\\alpha=%d, \\beta=%3.1f$\" % (a,b,a,b,a,b))\n    ax.plot(x,y,label=r\"$\\alpha=%d,$    $\\beta=%3.1f$\" % (a,b))\nax.tick_params(labelsize=12)\nax.set_xlabel('x', fontsize = 14)\nax.set_ylabel('$p(x)$', fontsize = 14)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nax.set_title('Logistic Functions', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-11-output-1.png){width=967 height=536}\n:::\n:::\n\n\nParameter $\\beta$ controls how fast $p(x)$ raises from $0$ to $1$\n\nThe value of -$\\alpha$/$\\beta$ shows the value of $x$ for which $p(x)=0.5$\n\nAnother interpretation of $\\alpha$ is that it gives the __base rate__ -- the unconditional probability of a 1.   That is, if you knew nothing about a particular data item, then $p(x) = 1/(1+e^{-\\alpha})$.\n\nThe function $f(x) = \\log (x/(1-x))$ is called the __logit__ function.\n\nSo a compact way to describe logistic regression is that it finds regression coefficients $\\alpha, \\beta$ to fit:\n\n$$\\text{logit}\\left(p(x)\\right)=\\log\\left(\\frac{p(x)}{1-p(x)} \\right) = \\alpha + \\beta x$$\n\nNote also that the __inverse__ logit function is:\n\n$$\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}$$\n\nSomewhat confusingly, this is called the __logistic__ function.\n\nSo, the best way to think of logistic regression is that we compute a linear function:\n    \n$$\\alpha + \\beta x$$\n    \nand then \"map\" that to a probability using the $\\text{logit}^{-1}$ function:\n\n$$\\frac{e^{\\alpha+\\beta x}}{1+e^{\\alpha+\\beta x}}$$\n\n## Logistic vs Linear Regression\n\nLet's take a moment to compare linear and logistic regression.\n\nIn __Linear regression__ we fit \n\n$$y_i = \\alpha +\\beta x_i + \\epsilon_i.$$\n\nWe do the fitting by minimizing the sum of squared error ($\\Vert\\epsilon\\Vert$).   This can be done in closed form.  \n\n(Recall that the closed form is found by geometric arguments, or by calculus).\n\nNow, if $\\epsilon_i$ comes from a normal distribution with mean zero and some fixed variance, \n\nthen minimizing the sum of squared error is exactly the same as finding the maximum likelihood of the data with respect to the probability of the errors.\n\nSo, in the case of linear regression, it is a lucky fact that the __MLE__ of $\\alpha$ and $\\beta$ can be found by a __closed-form__ calculation.\n\nIn __Logistic regression__ we fit \n\n$$\\text{logit}(p(x_i)) = \\alpha + \\beta x_i.$$\n\n\nwith $\\text{Pr}(y_i=1\\mid x_i)=p(x_i).$\n\nHow should we choose parameters?   \n\nHere too, we use Maximum Likelihood Estimation of the parameters.\n\nThat is, we choose the parameter values that maximize the likelihood of the data given the model.\n\n$$ \\text{Pr}(y_i \\mid x_i) = \\left\\{\\begin{array}{lr}\\text{logit}^{-1}(\\alpha + \\beta x_i)& \\text{if } y_i = 1\\\\\n1 - \\text{logit}^{-1}(\\alpha + \\beta x_i)& \\text{if } y_i = 0\\end{array}\\right.$$\n\nWe can write this as a single expression:\n\n$$\\text{Pr}(y_i \\mid x_i) = \\text{logit}^{-1}(\\alpha + \\beta x_i)^{y_i} (1-\\text{logit}^{-1}(\\alpha + \\beta x_i))^{1-y_i} $$\n\nWe then use this to compute the __likelihood__ of parameters $\\alpha$, $\\beta$:\n\n$$L(\\alpha, \\beta \\mid x_i, y_i) = \\text{logit}^{-1}(\\alpha + \\beta x_i)^{y_i} (1-\\text{logit}^{-1}(\\alpha + \\beta x_i))^{1-y_i}$$\n\nwhich is a function that we can maximize via various kinds of gradient descent.\n\n## Logistic Regression In Practice\n\nSo, in summary, we have:\n\n**Input** pairs $(x_i,y_i)$\n\n**Output** parameters $\\widehat{\\alpha}$ and $\\widehat{\\beta}$ that maximize the likelihood of the data given these parameters for the logistic regression model.\n\n**Method** Maximum likelihood estimation, obtained by gradient descent.\n\nThe standard package will give us a correlation coefficient (a $\\beta_i$) for each independent variable (feature).\n\nIf we want to include a constant (ie, $\\alpha$) we need to add a column of 1s (just like in linear regression).\n\n::: {#e0d37dd5 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=11}\n``` {.python .cell-code}\ndf['intercept'] = 1.0\ntrain_cols = df.columns[1:]\ntrain_cols\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nIndex(['gre', 'gpa', 'rank', 'intercept'], dtype='object')\n```\n:::\n:::\n\n\n::: {#200105f4 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=12}\n``` {.python .cell-code}\nlogit = sm.Logit(df['admit'], df[train_cols])\n \n# fit the model\nresult = logit.fit() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.574302\n         Iterations 6\n```\n:::\n:::\n\n\n::: {#169c824e .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=13}\n``` {.python .cell-code}\nresult.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>admit</td>      <th>  No. Observations:  </th>  <td>   400</td>  \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   396</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Mon, 02 Sep 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.08107</td> \n</tr>\n<tr>\n  <th>Time:</th>                <td>15:46:47</td>     <th>  Log-Likelihood:    </th> <td> -229.72</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -249.99</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.207e-09</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>gre</th>       <td>    0.0023</td> <td>    0.001</td> <td>    2.101</td> <td> 0.036</td> <td>    0.000</td> <td>    0.004</td>\n</tr>\n<tr>\n  <th>gpa</th>       <td>    0.7770</td> <td>    0.327</td> <td>    2.373</td> <td> 0.018</td> <td>    0.135</td> <td>    1.419</td>\n</tr>\n<tr>\n  <th>rank</th>      <td>   -0.5600</td> <td>    0.127</td> <td>   -4.405</td> <td> 0.000</td> <td>   -0.809</td> <td>   -0.311</td>\n</tr>\n<tr>\n  <th>intercept</th> <td>   -3.4495</td> <td>    1.133</td> <td>   -3.045</td> <td> 0.002</td> <td>   -5.670</td> <td>   -1.229</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nNotice that all of our independent variables are considered significant (no confidence intervals contain zero).\n\n## Using the Model\n\nNote that by fitting a model to the data, we can make predictions for inputs that were never seen in the data.  \n\nFurthermore, we can make a prediction of a probability for cases where we don't have enough data to estimate the probability directly -- e.g, for specific parameter values.\n\nLet's see how well the model fits the data.\n\nWe have three independent variables, so in each case we'll use average values for the two that we aren't evaluating.\n\nGPA:\n\n::: {#8d751073 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nbins = np.linspace(df.gpa.min(), df.gpa.max(), 10)\ngroups = df.groupby(np.digitize(df.gpa, bins))\nprob = [result.predict([600, b, 2.5, 1.0]) for b in bins]\nax = plt.figure(figsize = (7, 5)).add_subplot()\nax.plot(bins, prob)\nax.plot(bins,groups.admit.mean(),'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('gpa', fontsize = 14)\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of GPA', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-15-output-1.png){width=609 height=462}\n:::\n:::\n\n\nGRE Score:\n\n::: {#12568e43 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nprob = [result.predict([b, 3.4, 2.5, 1.0]) for b in sorted(df.gre.unique())]\nax = plt.figure(figsize = (7, 5)).add_subplot()\nax.plot(sorted(df.gre.unique()), prob)\nax.plot(df.groupby('gre').mean()['admit'],'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('gre', fontsize = 14)\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of GRE', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-16-output-1.png){width=599 height=462}\n:::\n:::\n\n\nInstitution Rank:\n\n::: {#79018194 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nprob = [result.predict([600, 3.4, b, 1.0]) for b in range(1,5)]\nax = plt.figure(figsize = (7, 5)).add_subplot()\nax.plot(range(1,5), prob)\nax.plot(df.groupby('rank').mean()['admit'],'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('Rank', fontsize = 14)\nax.set_xlim([0.5,4.5])\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of Rank', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-17-output-1.png){width=622 height=462}\n:::\n:::\n\n\n## Logistic Regression in Perspective\n\nAt the start of lecture I emphasized that logistic regression is concerned with estimating a __probability__ model from __discrete__ (0/1) data. \n\nHowever, it may well be the case that we want to do something with the probability that amounts to __classification.__\n\nFor example, we may classify data items using a rule such as \"Assign item $x_i$ to Class 1 if $p(x_i) > 0.5$\".\n\nFor this reason, logistic regression could be considered a classification method.\n\nIn fact, that is what we did with Naive Bayes -- we used it to estimate something like a probability, and then chose the class with the maximum value to create a classifier.\n\nLet's use our logistic regression as a classifier.\n\nWe want to ask whether we can correctly predict whether a student gets admitted to graduate school.\n\nLet's separate our training and test data:\n\n::: {#736f6dad .cell slideshow='{\"slide_type\":\"-\"}' execution_count=17}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n        df[train_cols], df['admit'],\n        test_size=0.4, random_state=1)\n```\n:::\n\n\nNow, there are some standard metrics used when evaluating a binary classifier.\n\nLet's say our classifier is outputting \"yes\" when it thinks the student will be admitted.\n\nThere are four cases:\n* Classifier says \"yes\", and student __is__ admitted:  __True Positive.__\n* Classifier says \"yes\", and student __is not__ admitted:  __False Positive.__\n* Classifier says \"no\", and student __is__ admitted:  __False Negative.__\n* Classifier says \"no\", and student __is not__ admitted:  __True Negative.__\n\n__Precision__ is the fraction of \"yes\"es that are correct:\n    $$\\mbox{Precision} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Positives}}$$\n    \n__Recall__ is the fraction of admits that we say \"yes\" to:\n $$\\mbox{Recall} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Negatives}}$$\n\n::: {#704ba1bb .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=18}\n``` {.python .cell-code}\ndef evaluate(y_train, X_train, y_test, X_test, threshold):\n\n    # learn model on training data\n    logit = sm.Logit(y_train, X_train)\n    result = logit.fit(disp=False)\n    \n    # make probability predictions on test data\n    y_pred = result.predict(X_test)\n    \n    # threshold probabilities to create classifications\n    y_pred = y_pred > threshold\n    \n    # report metrics\n    precision = metrics.precision_score(y_test, y_pred)\n    recall = metrics.recall_score(y_test, y_pred)\n    return precision, recall\n\nprecision, recall = evaluate(y_train, X_train, y_test, X_test, 0.5)\n\nprint(f'Precision: {precision:0.3f}, Recall: {recall:0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.586, Recall: 0.340\n```\n:::\n:::\n\n\nNow, let's get a sense of average accuracy:\n\n::: {#fce91864 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=19}\n``` {.python .cell-code}\nPR = []\nfor i in range(20):\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            df[train_cols], df['admit'],\n            test_size=0.4)\n    PR.append(evaluate(y_train, X_train, y_test, X_test, 0.5))\n```\n:::\n\n\n::: {#9f4434ac .cell slideshow='{\"slide_type\":\"-\"}' execution_count=20}\n``` {.python .cell-code}\navgPrec = np.mean([f[0] for f in PR])\navgRec = np.mean([f[1] for f in PR])\nprint(f'Average Precision: {avgPrec:0.3f}, Average Recall: {avgRec:0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage Precision: 0.586, Average Recall: 0.202\n```\n:::\n:::\n\n\nSometimes we would like a single value that describes the overall performance of the classifier.\n\nFor this, we take the harmonic mean of precision and recall, called __F1 Score__:\n\n$$\\mbox{F1 Score} = 2 \\;\\;\\frac{\\mbox{Precision} \\cdot \\mbox{Recall}}{\\mbox{Precision} + \\mbox{Recall}}$$\n\nUsing this, we can evaluate other settings for the threshold.\n\n::: {#e7046aa7 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' execution_count=21}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef evalThresh(df, thresh):\n    PR = []\n    for i in range(20):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n                df[train_cols], df['admit'],\n                test_size=0.4)\n        PR.append(evaluate(y_train, X_train, y_test, X_test, thresh))\n    avgPrec = np.mean([f[0] for f in PR])\n    avgRec = np.mean([f[1] for f in PR])\n    return 2 * (avgPrec * avgRec) / (avgPrec + avgRec), avgPrec, avgRec\n\ntvals = np.linspace(0.05, 0.8, 50)\nf1vals = [evalThresh(df, tval)[0] for tval in tvals]\n```\n:::\n\n\n::: {#116728d6 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=22}\n``` {.python .cell-code}\nplt.plot(tvals,f1vals)\nplt.ylabel('F1 Score')\nplt.xlabel('Threshold for Classification')\nplt.title('F1 as a function of Threshold');\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-html/cell-23-output-1.png){width=589 height=449}\n:::\n:::\n\n\nBased on this plot, we can say that the best classification threshold appears to be around 0.3, where precision and recall are:\n\n::: {#454b303b .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=23}\n``` {.python .cell-code}\nF1, Prec, Rec = evalThresh(df, 0.3)\nprint('Best Precision: {:0.3f}, Best Recall: {:0.3f}'.format(Prec, Rec))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Precision: 0.413, Best Recall: 0.662\n```\n:::\n:::\n\n\nThe example here is based on\nhttp://blog.yhathq.com/posts/logistic-regression-and-python.html\nwhere you can find additional details.\n\n",
    "supporting": [
      "18-Regression-II-Logistic_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}