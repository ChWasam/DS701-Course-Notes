{
  "hash": "607ab9e7880136a95461eaa5f217bf8d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Decision Trees and Random Forests\"\njupyter: python3\nbibliography: references.bib\nnocite: |\n  @Hastie2009\n---\n\n\n\n\n## Outline\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/14-Classification-I-Decision-Trees.ipynb)\n\n- Build a decision tree manually\n- Look at single and collective impurity measures\n- Selecting splitting attributes and test conditions\n- Scikit-learn implementation\n- Model training and evaluation\n- Random forests\n\n## Introduction\n\n\n\nWe'll now start looking into how to build models to predict an outcome variable from labeled data.\n\n**Classification** problems:\n\n- predict a category\n- e.g. spam/not spam, fraud/not fraud, default/not default, malignant/benign, etc.\n\n**Regression** problems:\n\n- predict a numeric value\n- e.g. price of a house, salary of a person, etc.\n\n\n## Loan Default Example\n\nWe'll use an example from [@Tan2018].\n\n![](figs/L14-terrier-savings-logo.png){height=\"200px\"}\n\nYou are a loan officer at **Terrier Savings and Loan**. \n\nYou have a dataset on loans that you have made in the past.\n\nYou want to build a model to predict whether a loan will default.\n\n## Loans Data Set\n\n::: {#a5471712 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\nimport os\nimport urllib.request\n\n# Check if the directory exists, if not, create it\nif not os.path.exists('data'):\n    os.makedirs('data')\n\nif not os.path.exists('data/loans.csv'):\n    url = 'https://github.com/tools4ds/DS701-Course-Notes/blob/main/ds701_book/data/loans.csv'\n    urllib.request.urlretrieve(url, 'data/loans.csv')\n\nloans = pd.read_csv('data/loans.csv', index_col=0)\nloans\n```\n\n::: {.cell-output .cell-output-display execution_count=194}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Yes</td>\n      <td>Single</td>\n      <td>125000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>100000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>70000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yes</td>\n      <td>Married</td>\n      <td>120000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>No</td>\n      <td>Divorced</td>\n      <td>95000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>60000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Yes</td>\n      <td>Divorced</td>\n      <td>220000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>85000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>75000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>90000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nHere's the summary info of the data set.\n\n::: {#19990576 .cell execution_count=3}\n``` {.python .cell-code}\nloans.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   Home Owner          10 non-null     object\n 1   Marital Status      10 non-null     object\n 2   Annual Income       10 non-null     int64 \n 3   Defaulted Borrower  10 non-null     object\ndtypes: int64(1), object(3)\nmemory usage: 400.0+ bytes\n```\n:::\n:::\n\n\n---\n\nSince some of the fields are categorical, let's convert them to categorical data types.\n\n::: {#e682eeee .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nloans['Home Owner'] = loans['Home Owner'].astype('category')\nloans['Marital Status'] = loans['Marital Status'].astype('category')\nloans['Defaulted Borrower'] = loans['Defaulted Borrower'].astype('category')\nloans.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   Home Owner          10 non-null     category\n 1   Marital Status      10 non-null     category\n 2   Annual Income       10 non-null     int64   \n 3   Defaulted Borrower  10 non-null     category\ndtypes: category(3), int64(1)\nmemory usage: 570.0 bytes\n```\n:::\n:::\n\n\n## Simple Model\n\nLooking at the table, let's just start with the simplest model possible and just\npredict that no one will default.\n\nSo the output of our model is just to always predict \"No\".\n\n::: {#a4829185 .cell execution_count=5}\n``` {.python .cell-code}\nimport graphviz\n\ndot_data = \"\"\"\ndigraph Tree {\n    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n    edge [fontname=\"helvetica\"] ;\n    0 [label=\"values = [# No, # Yes] =[7,3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n}\n\"\"\"\n\n# Use graphviz to render the dot file\ngraph = graphviz.Source(dot_data)  \ngraph\n```\n\n::: {.cell-output .cell-output-display execution_count=197}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-6-output-1.svg){}\n:::\n:::\n\n\n<!-- TODO: Look at this reference to consider pydotplus for size control -->\n\n\nWe see a 30% error rate since 3 out of 10 loans defaulted.\n\n---\n\nLet's split the data based on the \"Home Owner\" field.\n\n::: {#6404af61 .cell execution_count=6}\n``` {.python .cell-code}\nimport graphviz\n\ndot_data = \"\"\"\ndigraph Tree {\n    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n    edge [fontname=\"helvetica\"] ;\n    0 [label=\"Home Owner?\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    2 [label=\"samples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = Yes\\\\nerror = 43%\", fillcolor=\"#ffffff\"] ;\n    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;}\n\"\"\"\n\n# Use graphviz to render the dot file\ngraph = graphviz.Source(dot_data)  \ngraph\n```\n\n::: {.cell-output .cell-output-display execution_count=198}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\nWe see that the left node (home owner == yes) has a 0% error rate since all the samples are no. We don't split this node since all the samples are of the same class. We call this node a **leaf node** and we'll color it orange.\n\nThe right node (home owner == no) has a 43% error rate since 3 out of 7 loans defaulted. \n\nLet's split this node into two nodes based on the \"Marital Status\" field.\n\n---\n\nLet's split on the \"Marital Status\" field.\n\nWe see that the 3 defaulted loans are all for single or divorced people. Since the node is\nall one class, we don't split this node and we call it a **leaf node**.\n\n::: {#ad4e97c9 .cell execution_count=7}\n``` {.python .cell-code}\nimport graphviz\n\ndot_data = \"\"\"\ndigraph Tree {\n    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n    edge [fontname=\"helvetica\"] ;\n    0 [label=\"Home Owner?\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    2 [label=\"Marital Status?\\\\n---\\\\nsamples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = Yes\", fillcolor=\"#ffffff\"] ;\n    3 [label=\"samples = 4\\\\nvalue = [1, 3]\\\\ndefaulted = Yes\\\\nerror = 25%\", fillcolor=\"#ffffff\"] ;\n    4 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;\n    2 -> 3 [labeldistance=2.5, labelangle=45, headlabel=\"Single,\\\\nDivorced\"] ;\n    2 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"Married\"] ;}\n\"\"\"\n\n# Use graphviz to render the dot file\ngraph = graphviz.Source(dot_data)  \ngraph\n```\n\n::: {.cell-output .cell-output-display execution_count=199}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-8-output-1.svg){}\n:::\n:::\n\n\n--- \n\nWe can list the subsets for the two criteria to calculate the error rate.\n\n::: {#8f24b4dd .cell .fig-cap-location-top execution_count=8}\n``` {.python .cell-code}\nloans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'].isin(['Single', 'Divorced']))]\n```\n\n::: {.cell-output .cell-output-display execution_count=200}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>70000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>No</td>\n      <td>Divorced</td>\n      <td>95000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>85000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>90000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\nTable: Home Owner == No and Marital Status == Single or Divorced --> Defaulted == Yes\n:::\n:::\n\n\n::: {#2d87e740 .cell .fig-cap-location-top execution_count=9}\n``` {.python .cell-code}\nloans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'] == \"Married\")]\n```\n\n::: {.cell-output .cell-output-display execution_count=201}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>100000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>60000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>75000</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\nTable: Home Owner == No and Marital Status == Married --> Defaulted == No\n:::\n:::\n\n\n---\n\nLet's try to split on the \"Annual Income\" field. We see that the person with income of 70K doesn't default, so we split the node into two nodes based on the \"Income\" field. We arbitrarily pick a threshold of $75K.\n\n::: {#7bbe0921 .cell execution_count=10}\n``` {.python .cell-code}\nimport graphviz\n\ndot_data = \"\"\"\ndigraph Tree {\n    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n    edge [fontname=\"helvetica\"] ;\n    0 [label=\"Home Owner\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    2 [label=\"Marital Status\\\\n---\\\\nsamples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = Yes\", fillcolor=\"#ffffff\"] ;\n    3 [label=\"Income <= 75K\\\\nsamples = 4\\\\nvalue = [1, 3]\\\\ndefaulted = Yes\\\\nerror = 25%\", fillcolor=\"#ffffff\"] ;\n    4 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    5 [label=\"samples = 1\\\\nvalue = [1, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    6 [label=\"samples = 3\\\\nvalue = [0, 3]\\\\ndefaulted = Yes\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;\n    2 -> 3 [labeldistance=2.5, labelangle=45, headlabel=\"Single,\\\\nDivorced\"] ;\n    2 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"Married\"] ;\n    3 -> 5 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n    3 -> 6 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;}\n\"\"\"\n\n# Use graphviz to render the dot file\ngraph = graphviz.Source(dot_data)  \ngraph\n```\n\n::: {.cell-output .cell-output-display execution_count=202}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-11-output-1.svg){}\n:::\n:::\n\n\n## Evaluating the Model\n\nWe've dispositioned every data point by walking down the tree to a leaf node.\n\nHow do we know if this tree is good? We arbitrarily picked the order of the fields to split on.\n\nIs there a way to systematically pick the order of the fields to split on? \n\n* This is called the **splitting criterion**.\n\nThere's also the question of when to stop splitting, or the **stopping criterion**. \n\nSo far, we stopped splitting when we reached a node of pure class but there are \nreasons to stop splitting even without pure classes, which we'll see later.\n\n## Specifying the Test Condition\n\nBefore we continue, we should take a moment to consider how we specify a test condition of a node.\n\nHow we specify a test condition depends on the attribute type which can be:\n\n* Binary (Boolean)\n* Nominal (Categorical, e.g. cat, dog, bird)\n* Ordinal (e.g. Small, Medium, Large)\n* Continuous (e.g., 1.5, 2.1, 3.7)\n\nAnd depends on the number of ways to split:\n\n* __multi-way__\n* __binary__\n\n---\n\nFor a __Nominal__ attribute:\n\nIn a __Multi-way split__ we can use as many partitions as there are distinct values of the attribute:\n\n::: {#0669ffb8 .cell execution_count=11}\n``` {.python .cell-code}\nimport graphviz\n\ndot_data = \"\"\"\ndigraph Tree {\n    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n    edge [fontname=\"helvetica\"] ;\n    0 [label=\"Marital Status\", fillcolor=\"#ffffff\"] ;\n    1 [label=\"Single\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n    2 [label=\"Divorced\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n    3 [label=\"Married\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n    0 -> 1 ;\n    0 -> 2 ;\n    0 -> 3 ; }\n\"\"\"\n\n# Use graphviz to render the dot file\ngraph = graphviz.Source(dot_data)  \ngraph\n```\n\n::: {.cell-output .cell-output-display execution_count=203}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-12-output-1.svg){}\n:::\n:::\n\n\n---\n\nIn a __Binary split__ we divide the values into two groups.  \n\nIn this case, we need to find an optimal partitioning of values into groups, which we discuss shortly.\n\n::: {layout-ncol=3}\n\n::: {#8d4355e3 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=204}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-13-output-1.svg){}\n:::\n:::\n\n\n::: {#e4c66579 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=205}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\n::: {#bac5df50 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=206}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-15-output-1.svg){}\n:::\n:::\n\n\n:::\n\n---\n\nFor an __Ordinal__ attribute, we can use a multi-way split with as many partitions\nas there are distinct values.\n\n::: {#f36d864d .cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=207}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-16-output-1.svg){}\n:::\n:::\n\n\n---\n\nOr we can use a binary split as long we preserve the ordering of the values.\n\n::: {layout-ncol=2}\n\n::: {#37aa5882 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display execution_count=208}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-17-output-1.svg){}\n:::\n:::\n\n\n::: {#5edfd81c .cell execution_count=17}\n\n::: {.cell-output .cell-output-display execution_count=209}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-18-output-1.svg){}\n:::\n:::\n\n\n:::\n\n::: {.callout-warning}\nBe careful not to violate the ordering of values such as {Small, Large} and {Medium, X-Large}.\n:::\n\n---\n\nA __Continuous__ attribute can be handled two ways:\n\n::: {layout-ncol=2}\n\n::: {#300c077d .cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=210}\n![It can be thresholded to form a binary split.](14-Classification-I-Decision-Trees_files/figure-html/cell-19-output-1.svg){}\n:::\n:::\n\n\n::: {#c3dedbfb .cell execution_count=19}\n\n::: {.cell-output .cell-output-display execution_count=211}\n![Or it can be split into contiguous ranges to form an ordinal categorical attribute.](14-Classification-I-Decision-Trees_files/figure-html/cell-20-output-1.svg){}\n:::\n:::\n\n\n:::\n\n---\n\nNote that finding good partitions for nominal attributes can be expensive, \npossibly involving combinatorial searching of groupings.  \n\nHowever for ordinal or continuous attributes, sweeping through a range of\nthreshold values can be more efficient.\n\n## Measures for Selecting Attribute and Test Condition\n\nIdeally, we want to pick attributes and test conditions that maximize the homogeneity of the splits.\n\nWe can use a impurity index to measure the homogeneity of a split.\n\nWe'll look at ways of measuring impurity of a node and the collective impurity of its child nodes.\n\n## Impurity Measures\n\nThe following are three impurity indices:\n\n$$\n\\begin{aligned}\n\\textnormal{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n\\textnormal{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n\\textnormal{Classification error} &= 1 - \\max_i p_i(t)\n\\end{aligned}\n$$\n\nwhere $p_i(t)$ is the **relative frequency** of training instances of class $i$ at a node $t$ and $c$ is the number of classes.\n\n::: {.callout-note}\nBy convention, we set $0 \\log_2 0 = 0$ in entropy calculations.\n:::\n\nAll three impurity indices equal 0 when all the records at a node belong to the same class.\n\nAll three impurity indices reach their maximum value when the classes are evenly distributed among the child nodes.\n\n## Impurity Example 1\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n| Node $N_1$ | Count |\n| --- | --- |\n| Class=0 | 0 |\n| Class=1 | 6 |\n\n:::\n::: {.column width=\"60%\"}\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{0}{6}\\right)^2 - \\left(\\frac{6}{6}\\right)^2 = 0 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{0}{6} \\log_2 \\frac{0}{6} - \\frac{6}{6} \\log_2 \\frac{6}{6}\\right) = 0 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{0}{6}, \\frac{6}{6}\\right] = 0\n\\end{aligned}\n$$\n\n:::\n::::\n\n## Impurity Example 2\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n| Node $N_2$ | Count |\n| --- | --- |\n| Class=0 | 1 |\n| Class=1 | 5 |\n\n:::\n::: {.column width=\"60%\"}\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{1}{6}\\right)^2 - \\left(\\frac{5}{6}\\right)^2 = 0.278 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{1}{6} \\log_2 \\frac{1}{6} - \\frac{5}{6} \\log_2 \\frac{5}{6}\\right) = 0.650 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{1}{6}, \\frac{5}{6}\\right] = 0.167\n\\end{aligned}\n$$\n\n:::\n::::\n\n## Impurity Example 3\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n| Node $N_3$ | Count |\n| --- | --- |\n| Class=0 | 3 |\n| Class=1 | 3 |\n\n:::\n::: {.column width=\"60%\"}\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{3}{6} \\log_2 \\frac{3}{6} - \\frac{3}{6} \\log_2 \\frac{3}{6}\\right) = 1 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{3}{6}, \\frac{3}{6}\\right] = 0.5\n\\end{aligned}\n$$\n\n:::\n::::\n\n---\n\nWe can plot the three impurity indices to get a sense of how they behave for binary classification problems.\n\n::: {#33f265d7 .cell execution_count=20}\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-21-output-1.png){width=672 height=523}\n:::\n:::\n\n\nThey all maintain the same ordering for every relative frequency, i.e. Entropy > Gini > Misclassification error.\n\n## Collective Impurity of Child Nodes\n\nWe can compute the collective impurity of child nodes by taking a weighted sum of the impurities of the child nodes.\n\n$$\nI(\\textnormal{children}) = \\sum_{j=1}^{k} \\frac{N(v_j)}{N}\\; I(v_j)\n$$\n\nHere we split $N$ training instances into $k$ child nodes, $v_j$ for $j=1, \\ldots, k$.\n\n$N(v_j)$ is the number of training instances at child node $v_j$ and $I(v_j)$ is the impurity at child node $v_j$.\n\n## Impurity Example\n\nLet's compute collective impurity on our loans dataset to see which feature to split on.\n\n::: {layout-ncol=3}\n\n::: {#0be626df .cell execution_count=21}\n\n::: {.cell-output .cell-output-display execution_count=213}\n![(a) Collective Entropy: 0.690](14-Classification-I-Decision-Trees_files/figure-html/cell-22-output-1.svg){}\n:::\n:::\n\n\n::: {#43289e62 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display execution_count=214}\n![(b) Collective Entropy: 0.686](14-Classification-I-Decision-Trees_files/figure-html/cell-23-output-1.svg){}\n:::\n:::\n\n\n::: {#f21d5778 .cell execution_count=23}\n\n::: {.cell-output .cell-output-display execution_count=215}\n![(c) Collective Entropy index: 0.00](14-Classification-I-Decision-Trees_files/figure-html/cell-24-output-1.svg){}\n:::\n:::\n\n\n:::\n\n::: {.callout-tip}\nTry calculating the collective Entropy for (a) and (b) and see if you get the same values.\n:::\n\n::: {.callout-important}\nThe collective entropy for (c) is 0. Why would we not want to use this node?\n:::\n\n## Gain Ratio\n\nFrom [@Tan2018, p. 127]:\n\n* Having a low impurity value alone is insufficient to find a good attribute test condition for a node. \n* Having more number of child nodes can make a decision tree more complex and consequently more susceptible to overfitting. \n\nHence, the number of children produced by the splitting attribute should also be taken into consideration while deciding the best attribute test condition. \n\n---\n\nThere are two ways to overcome this problem. \n\n1. One way is to _generate only binary decision trees_, thus avoiding the difficulty of handling attributes with varying\n   number of partitions. This strategy is employed by decision tree classifiers such as **CART**. \n2. Another way is to modify the splitting criterion to take into account the number of partitions produced by the\n   attribute. For example, in the **C4.5** decision tree algorithm, a measure known as **gain ratio** is used to compensate\n   for attributes that produce a large number of child nodes.\n\n## Identifying the Best Attribute Test Condition\n\n![](figs/L14-splitting-criteria-gini.png)\n\nHere's an example of how to identify the best attribute test condition using the Gini impurity index.\n\n## Splitting Continuous Attributes\n\nFor quantitative attributes like _Annual Income_, we need to find some threshold $\\tau$ that\nminimizes the impurity index.\n\nThe following table illustrates the process.\n\n![](figs/L14-splitting-continuous-attribs.png)\n\n**Procedure:**\n\n1. Sort all the training instances by _Annual Income_ in increasing order.\n2. Pick thresholds half way between consecutive values.\n3. Compute the Gini impurity index for each threshold.\n4. Select the threshold that minimizes the Gini impurity index.\n\n## Run Decision Tree on Loans Data Set\n\nLet's run the Scikit-learn Decision Tree, `sklearn.tree`, on the loans data set.\n\n`sklearn.tree` requires all fields to be numeric.\n\nSo first we have to convert the categorical fields to category index numeric fields.\n\n::: {#6b6826cd .cell execution_count=24}\n``` {.python .cell-code code-fold=\"false\"}\nloans['Defaulted Borrower'] = loans['Defaulted Borrower'].cat.codes\nloans['Home Owner'] = loans['Home Owner'].cat.codes\nloans['Marital Status'] = loans['Marital Status'].cat.codes\nloans.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=216}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>125000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>100000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2</td>\n      <td>70000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>120000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>95000</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nThen the independent variables are all the fields except the \"Defaulted Borrower\" field, which we'll assign to `X`.\n\nThe dependent variable is the \"Defaulted Borrower\" field, which we'll assign to `y`.\n\n::: {#753673ea .cell execution_count=25}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn import tree\n\nX = loans.drop('Defaulted Borrower', axis=1)\ny = loans['Defaulted Borrower']\n```\n:::\n\n\n<br>\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n`X`:\n\n::: {#42dba50a .cell execution_count=26}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 3 columns):\n #   Column          Non-Null Count  Dtype\n---  ------          --------------  -----\n 0   Home Owner      10 non-null     int8 \n 1   Marital Status  10 non-null     int8 \n 2   Annual Income   10 non-null     int64\ndtypes: int64(1), int8(2)\nmemory usage: 180.0 bytes\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n`y`:\n\n::: {#28d2d38f .cell execution_count=27}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.series.Series'>\nIndex: 10 entries, 1 to 10\nSeries name: Defaulted Borrower\nNon-Null Count  Dtype\n--------------  -----\n10 non-null     int8 \ndtypes: int8(1)\nmemory usage: 90.0 bytes\n```\n:::\n:::\n\n\n:::\n::::\n\n\n---\n\nLet's fit a decision tree to the data.\n\n::: {#5739fb69 .cell execution_count=28}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\nclf = clf.fit(X, y)\n```\n:::\n\n\nLet's plot the tree.\n\n::: {#e5174b59 .cell execution_count=29}\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-30-output-1.png){width=540 height=389}\n:::\n:::\n\n\nInterestingly, the tree was built using only the Income field.\n\nThat's arguably an advantage of Decision Trees: they automatically perform feature selection.\n\n\n\n## Ensemble Methods\n\n(See @Tan2018, Chapter 4)\n\nMotivated around the idea that combining several noisy classifers can result in a better prediction\nunder certain conditions.\n\n* The base classifiers are independent\n* The base classifiers are noisy (high variance)\n* The base classifiers are low (ideally zero) bias\n\n## Random Forests\n\nRandom forests are an ensemble of decision trees that:\n\n* Construct a set of base classifiers from random sub-samples of the training data.\n* Train each base classifier to completion.\n* Take a majority vote of the base classifiers to form the final prediction.\n\n## Titanic Example\n\nWe'll use the [Titanic data set](https://www.kaggle.com/competitions/titanic) and\nexcerpts of this [Kaggle tutorial](https://www.kaggle.com/code/jhoward/how-random-forests-really-work/)\nto illustrate the concepts of overfitting and random forests.\n\n::: {#24ff863b .cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\n\nimport os\nimport urllib.request\n\n# Check if the directory exists, if not, create it\nif not os.path.exists('data/titanic'):\n    os.makedirs('data/titanic')\n\nif not os.path.exists('data/titanic/train.csv'):\n    url = 'https://github.com/tools4ds/DS701-Course-Notes/blob/main/ds701_book/data/titanic/train.csv'\n    urllib.request.urlretrieve(url, 'data/titanic/train.csv')\n\ndf_train = pd.read_csv('data/titanic/train.csv', index_col='PassengerId')\n\nif not os.path.exists('data/titanic/test.csv'):\n    url = 'https://github.com/tools4ds/DS701-Course-Notes/blob/main/ds701_book/data/titanic/test.csv'\n    urllib.request.urlretrieve(url, 'data/titanic/test.csv')\n\ndf_test = pd.read_csv('data/titanic/test.csv', index_col='PassengerId')\n```\n:::\n\n\n---\n\nLet's look at the training data.\n\n::: {#0041a955 .cell execution_count=31}\n``` {.python .cell-code}\ndf_train.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 891 entries, 1 to 891\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Name      891 non-null    object \n 3   Sex       891 non-null    object \n 4   Age       714 non-null    float64\n 5   SibSp     891 non-null    int64  \n 6   Parch     891 non-null    int64  \n 7   Ticket    891 non-null    object \n 8   Fare      891 non-null    float64\n 9   Cabin     204 non-null    object \n 10  Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 83.5+ KB\n```\n:::\n:::\n\n\nWe can see that there are 891 entries with 11 fields. 'Age', 'Cabin', and 'Embarked' have missing values.\n\n---\n\n::: {#5815e820 .cell execution_count=32}\n``` {.python .cell-code}\ndf_train.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=224}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nLet's look at the test data.\n\n::: {#e846e132 .cell execution_count=33}\n``` {.python .cell-code}\ndf_test.info()\ndf_test.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 418 entries, 892 to 1309\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    418 non-null    int64  \n 1   Name      418 non-null    object \n 2   Sex       418 non-null    object \n 3   Age       332 non-null    float64\n 4   SibSp     418 non-null    int64  \n 5   Parch     418 non-null    int64  \n 6   Ticket    418 non-null    object \n 7   Fare      417 non-null    float64\n 8   Cabin     91 non-null     object \n 9   Embarked  418 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 35.9+ KB\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=225}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>892</th>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>893</th>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere are 418 entries in the test set with same fields except for 'Survived', which is what we need to predict.\n\n---\n\nWe'll do some data cleaning and preparation.\n\n::: {#59746324 .cell execution_count=34}\n``` {.python .cell-code}\nimport numpy as np\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True) # Fill missing values with the mode\n    df['LogFare'] = np.log1p(df['Fare'])  # Create a new column for the log of the fare + 1\n    df['Embarked'] = pd.Categorical(df.Embarked)  # Convert to categorical\n    df['Sex'] = pd.Categorical(df.Sex)  # Convert to categorical\n\nmodes = df_train.mode().iloc[0] # Get the mode for each column\n\nproc_data(df_train)\nproc_data(df_test)\n```\n:::\n\n\nLook at the dataframes again.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#5e204229 .cell execution_count=35}\n``` {.python .cell-code}\ndf_train.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 891 entries, 1 to 891\nData columns (total 12 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Survived  891 non-null    int64   \n 1   Pclass    891 non-null    int64   \n 2   Name      891 non-null    object  \n 3   Sex       891 non-null    category\n 4   Age       891 non-null    float64 \n 5   SibSp     891 non-null    int64   \n 6   Parch     891 non-null    int64   \n 7   Ticket    891 non-null    object  \n 8   Fare      891 non-null    float64 \n 9   Cabin     891 non-null    object  \n 10  Embarked  891 non-null    category\n 11  LogFare   891 non-null    float64 \ndtypes: category(2), float64(3), int64(4), object(3)\nmemory usage: 78.6+ KB\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#d6f531b7 .cell execution_count=36}\n``` {.python .cell-code}\ndf_test.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 418 entries, 892 to 1309\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Pclass    418 non-null    int64   \n 1   Name      418 non-null    object  \n 2   Sex       418 non-null    category\n 3   Age       418 non-null    float64 \n 4   SibSp     418 non-null    int64   \n 5   Parch     418 non-null    int64   \n 6   Ticket    418 non-null    object  \n 7   Fare      418 non-null    float64 \n 8   Cabin     418 non-null    object  \n 9   Embarked  418 non-null    category\n 10  LogFare   418 non-null    float64 \ndtypes: category(2), float64(3), int64(3), object(3)\nmemory usage: 33.7+ KB\n```\n:::\n:::\n\n\n:::\n::::\n\n---\n\nWe'll create lists of features by type.\n\n::: {#9faf1323 .cell execution_count=37}\n``` {.python .cell-code code-fold=\"false\"}\ncats=[\"Sex\",\"Embarked\"]  # Categorical\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]  # Continuous\ndep=\"Survived\"  # Dependent variable\n```\n:::\n\n\n---\n\nLet's explore some fields starting with survival rate by gender.\n\n::: {#28a2d2bf .cell execution_count=38}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df_train, y='Survived', x=\"Sex\", ax=axs[0], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Survival rate\")\nsns.countplot(data=df_train, x=\"Sex\", ax=axs[1], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Histogram\");\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-39-output-1.png){width=886 height=449}\n:::\n:::\n\n\nIndeed, \"women and children first\" was enforced on the Titanic.\n\n---\n\nSince we don't have labels for the test data, we'll split the training data into training and validation.\n\n::: {#32068493 .cell execution_count=39}\n``` {.python .cell-code code-fold=\"false\"}\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df_train, test_size=0.25)\n\n# Replace categorical fields with numeric codes\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n```\n:::\n\n\n---\n\nLet's split the independent (input) variables from the dependent (output) variable.\n\n::: {#d75fd94c .cell execution_count=40}\n``` {.python .cell-code code-fold=\"false\"}\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n```\n:::\n\n\n---\n\nHere's the predictions for our extremely simple model, where `female` is coded as `0`:\n\n::: {#d61a82f5 .cell execution_count=41}\n``` {.python .cell-code code-fold=\"false\"}\npreds = val_xs.Sex==0\n```\n:::\n\n\nWe'll use mean absolute error to measure how good this model is:\n\n::: {#3704c3e6 .cell execution_count=42}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=234}\n```\nnp.float64(0.21524663677130046)\n```\n:::\n:::\n\n\n---\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:\n\n::: {#6f050ae4 .cell execution_count=43}\n``` {.python .cell-code}\ndf_fare = trn_df[trn_df.LogFare>0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0], hue=dep, palette=[\"#3374a1\",\"#e1812d\"])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-44-output-1.png){width=874 height=429}\n:::\n:::\n\n\nThe [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. \n\nIt shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. \n\nSo it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n\n---\n\nLet's create a simple model based on this observation:\n\n::: {#7672ba9f .cell execution_count=44}\n``` {.python .cell-code code-fold=\"false\"}\npreds = val_xs.LogFare>2.7\n```\n:::\n\n\n...and test it out:\n\n::: {#5469fcac .cell execution_count=45}\n``` {.python .cell-code code-fold=\"false\"}\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=237}\n```\nnp.float64(0.336322869955157)\n```\n:::\n:::\n\n\nThis is quite a bit less accurate than our model that used `Sex` as the single binary split.\n\n## Full Decision Tree\n\nOk. Let's build a decision tree model using all the features.\n\n::: {#f4109fe0 .cell execution_count=46}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#41e0bd43 .cell execution_count=47}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-48-output-1.png){width=541 height=389}\n:::\n:::\n\n\n## Full Tree -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#7377464b .cell execution_count=48}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=240}\n```\nnp.float64(0.2645739910313901)\n```\n:::\n:::\n\n\n:::: {.fragment}\nThat is quite a bit worse than splitting on `Sex` alone!!\n::::\n\n## Stopping Criteria -- Minimum Samples Split\n\nLet's train the decision tree again but with stopping criteria based on the number of samples in a node.\n\n::: {#e7a49bed .cell execution_count=49}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, min_samples_split=20)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#5070f58f .cell execution_count=50}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-51-output-1.png){width=540 height=389}\n:::\n:::\n\n\n## Min Samples Split -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#65f04819 .cell execution_count=51}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=243}\n```\nnp.float64(0.18834080717488788)\n```\n:::\n:::\n\n\n## Decision Tree -- Maximum Depth\n\nLet's train the decision tree again but with a maximum depth of 3.\n\n::: {#3fcd9cb1 .cell execution_count=52}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, max_depth=3)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#1907e8aa .cell execution_count=53}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-54-output-1.png){width=540 height=389}\n:::\n:::\n\n\n## Maximum Depth -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#bfd79636 .cell execution_count=54}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=246}\n```\nnp.float64(0.19730941704035873)\n```\n:::\n:::\n\n\n## Random Forest\n\nLet's try a random forest.\n\n::: {#1f1fcc72 .cell execution_count=55}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's see how it does on the validation set.\n\n::: {#d22408a5 .cell execution_count=56}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=248}\n```\nnp.float64(0.21076233183856502)\n```\n:::\n:::\n\n\n## Recap\n\n* Decision Trees\n* Impurity Measures\n* Avoiding Overfitting\n* Random Forests\n\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "14-Classification-I-Decision-Trees_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}