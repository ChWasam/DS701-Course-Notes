{
  "hash": "cf65d2e5233fa623dcbee6b23df07a4a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Naive Bayes and Support Vector Machines\njupyter: python3\n---\n\n\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/16-Classification-III-NB-SVM.ipynb)\n\n\n<P><P><P><P>\n<center>\n\n<img src=\"figs/Thomas_Bayes.gif\" alt=\"Figure\" width=\"38%\" align = \"left\">\n<img src=\"figs/L16-SVM-6.png\" alt=\"Figure\" width=\"40%\" align = \"right\">\n    \n</center>\n\n<!--- By Unknown author - [2][3], Public Domain, https://commons.wikimedia.org/w/index.php?curid=14532025 --->\n\n\nToday we'll look at two more very commonly-used, widely-applicable classification methods.\n\n* the _Naive Bayes Classifier_\n* the _Support Vector Machine_\n\n## Naive Bayes\n\nThe classification problem most generally seeks to associate a label $y$ with a set of features $\\mathbf{x}$.  \n\nPerhaps the most fundamental way to attack this problem is via probability.\n\nWe'll now consider how to use probability to directly address the classification problem.\n\nA key tool will be __Bayes Rule,__ so let's review that.\n\n### Bayes Rule\n\nWe'll start from the definition of conditional probability:\n\n$$ P[A\\,|\\,C] = \\frac{P[A \\text{ and } C]}{P[C]} $$\n\nSo:\n\n$$ P[A \\text{ and } C] = P[A\\,|\\,C] \\cdot P[C]. $$\n\n::: {#986523e0 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\nplt.figure(figsize = (6,6))\nax=plt.gcf().gca()\ncircle = mp.patches.Circle([1, 1], 0.7, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\ncircle = mp.patches.Circle([1.7, 1], 0.4, facecolor = 'red', alpha = 0.2)\nax.add_artist(circle)\nax.axis('square')\nax.axis('off')\nplt.xlim([0, 2.5])\nplt.ylim([0, 2])\nplt.text(1, .95, 'A', fontsize = 16)\nplt.text(1.85, .95, 'C', fontsize = 16)\nplt.text(1.35, .95, 'A & C', fontsize = 16);\n# plt.title(r'$P[C\\,|\\,A]\\,P[A] = P[A\\,|\\,C]\\,P[C]$'); \n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-3-output-1.png){width=466 height=376}\n:::\n:::\n\n\nSwitching the roles of $A$ and $C$, we can equally say:\n\n$$ P[C \\text{ and } A] = P[C\\,|\\,A] \\cdot P[A]. $$\n\nNow $P[C \\text{ and } A] = P[A \\text{ and } C]$, so:\n    \n$$ P[A\\,|\\,C] \\cdot P[C] = P[C\\,|\\,A] \\cdot P[A] $$\n\nRearranging, we get __Bayes' Rule__:\n    \n$$ P[A\\,|\\,C] = \\frac{P[C\\,|\\,A] \\cdot P[A]}{P[C]}. $$\n\nSo there is nothing \"magic\" about Bayes' Rule: it simply follows from the basic laws of probability.\n\nHowever, it is __very__ useful!\n\nThe reason is that often, $P[A\\,|\\,C]$ is __not__ known, but the expressions on the right __are__ known.\n\n(We've already seen how this is used in the EM algorithm for Gaussian Mixture Modeling.)\n\n### Using Bayes Rule in a Classifier\n\nLet's start with a simple example:\n   * A doctor knows that meningitis causes a stiff neck 75% of the time.\n   * This is $P[S\\,|\\,M]$. \n   \nA patient presents with a stiff neck.  What is the probability she has meningitis?\n\nWell, we can't say.  \n\nWhat we know is $P[S\\,|\\,M]$, but what we want to know is $P[M\\,|\\,S]$!\n\nNow perhaps the doctor _also_ knows that \n   * 1 in 20 people in the population have a stiff neck at any given time\n       * $P[S] = 1/20$\n   * 1 in 10,000 people in the population have meningitis at any given time\n       * $P[M] = 1/10000$\n\n   \nNow, we can calculate the answer needed:\n\n$$ P[M\\,|\\,S] = \\frac{P[S\\,|\\,M]}{P[S]}\\cdot P[M] = \\frac{3/4}{1/20} \\cdot 1/10000 = \\fbox{$15 \\cdot 1/10000$} = 0.0015. $$\n\nI wrote the expressions above that way to point out the essence of Bayesian reasoning:\n\n* A random person has probability 1/10000 of having meningitis\n* When __we learn__ that the person has a stiff neck, it __increases their probability__ by a factor of 15.\n\nWe give these probabilities names according to their roles:\n\n* The random person's probability (1/10000) is called the __prior__ probability\n* The specific patient's probability (15 $\\cdot$ 1/10000) is called the __posterior__ probability.\n\nWe are going to use this same principle to construct a classifier.\n\nGiven a set of items with features and class labels:\n\nThe __class label__ will play the role of \"meningitis\" and the various __attributes__ of the item will play the role of \"stiff neck.\"\n\nWe will then ask \"how does the value of each feature change the probability of the class label?\"\n\nMore formally:\n    \nConsider an item $\\mathbf{x}$ having attributes $x_1, x_2, \\dots, x_n$.\n\nThere are various classes (labels) for items: $C_1, \\dots, C_k$.\n\nOur goal is to predict the class of $\\mathbf{x}$.\n\n\nTo do that, we will compute $P[C_1\\,|\\,\\mathbf{x}], P[C_2\\,|\\,\\mathbf{x}], \\dots, P[C_k\\,|\\,\\mathbf{x}]$.\n\nThese form a __soft classification__ of $\\mathbf{x}$.\n\nFrom them, we can form a hard classification.  One way would simply be to choose the class with highest probability.\n\nThis is the MAP (_Maximum A Posteriori_) Estimate: \n\n$$\\hat{C} = \\arg\\max_{\\{C_i\\}} P[C_i\\,|\\,\\mathbf{x}].$$\n\nNow $P[C_i\\,|\\,\\mathbf{x}] = P[C_i\\,|\\,x_1, x_2, \\dots, x_n]$\n\nHow can we approach the problem of computing $P[C_i\\,|\\,\\mathbf{x}]$?\n\nThe __key idea__ is that Bayes Rule makes clear that \n\n$$ P[C_i\\,|\\,\\mathbf{x}] = \\frac{P[\\mathbf{x}\\,|\\,C_i]}{P[\\mathbf{x}]} \\cdot P[C_i] $$\n\nNow, when we vary $C_i$ in the above expression, $P[\\mathbf{x}]$ is not changing.\n\nSo ... \n\nThe $\\hat{C}$ that maximizes \n\n$$P[C_i\\,|\\,x_1, x_2, \\dots, x_n]$$\n\nis the __same__ as the $\\hat{C}$ that maximizes \n\n$$P[x_1, x_2, \\dots, x_n\\,|\\,C_i]\\cdot P[C_i].$$\n\nThis gives us an angle of attack on the problem.\n\nThe difficult problem that remains is how to estimate\n\n$$ P[x_1, x_2, \\dots, x_n\\,|\\,C_i]. $$\n\nTo see the challenge, imagine if we tried to compute $P[x_1, x_2, \\dots, x_n\\,|\\,C_i]$ directly from data. \n\nAttacking head-on, we could use a histogram to estimate the necessary distribution.  In other words, simply count up how many times we see each combination of feature values.\n\nLet's say there are 20 features ($n = 20$), and 10 possible values for each feature. \n\nThen for each class $C_i$ we need to construct a histogram with $10^{20}$ bins!   \n\nWe would never have enough data to fill all those bins.\n\nThe underlying problem we face is the high dimensionality of the feature space.\n\nThe size of our histogram is __exponential__ in the number of features.\n\nSo, we need to find a way to reduce the exponential size of the estimation problem.\n\nWe will do that by __factoring__ the distribution $P[x_1, x_2, \\dots, x_n\\,|\\,C_i]$.\n\nHere is where the \"Naive\" part comes in.\n\nWe will __assume__ that __attributes are independent__ in their assignment to items.\n\nThat is, for two sets of attributes, the values of the attributes in one set do not affect the values in the other set.    So all correlations among attributes are zero.\n\nThis is indeed a \"naive\" assumption ... but it can be surprisingly effective in practice.\n\nThat implies that:\n    \n$$ P[x_1, x_2, \\dots, x_n\\,|\\,C_i] = P[x_1\\,|\\,C_i] \\cdot P[x_2\\,|\\,C_i] \\cdot \\dots P[x_n\\,|\\,C_i]  $$\n\nThis is very helpful computationally, because the factors $P[x_j\\,|\\,C_i]$ are individually much lower-dimensional than the full distribution.\n\nIn a naive Bayes model, the quantity we calculate for each class $C_i$ is\n\n$$ \\left(P[x_1\\,|\\,C_i] \\cdot P[x_2\\,|\\,C_i] \\cdot \\dots P[x_n\\,|\\,C_i]\\right) \\cdot P[C_i] $$\n\nYou can see each conditional probability as a \"correction factor\" to $P[C_i]$.\n\nEach factor $P[x_j\\,|\\,C_i]$ tells us how we should update our confidence in $C_i$ based on the value of a particular feature $x_j$.\n\nSo, what remains then is to estimate $P[x_j\\,|\\,C_i]$ for all $x_j$ and $C_i$.\n\nWe will estimate these quantities from our training data.\n\nTo summarize the steps of Naive Bayes:\n\n__Training__\n\n* Compute all the per-class attribute probabilities $P[x_j\\,|\\,C_i]$ from training data.\n* Compute all the class probabilities $P[C_i]$ from the training data.\n\n__Predicting__\n\n* For a given item $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$, \n    * For each class $C_i,$ compute $P[x_1\\,|\\,C_i] \\cdot P[x_2\\,|\\,C_i] \\cdot \\dots P[x_n\\,|\\,C_i] \\cdot P[C_i]$\n    * For a hard classification, return the class that maximizes the above expression.\n        * That is, the MAP estimate.\n\n### Computing Attribute Probabilities from Data\n\nAll that remains is to compute the conditional attribute probabilities from data.\n\nThe strategy depends on the attribute type: __discrete__ or __continuous.__\n\n<center>\n    \n<img src=\"figs/L16-sample-data.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n__Discrete Attributes__.\n\nDiscrete attributes, such as categorical or ordinal attributes, can be handled via histograms.\n\nIn the above table, to handle the <font color = \"blue\"> Marital Status </font> attribute for the <font color = \"blue\"> $\\text{Evade} = \\text{No}$ </font> class,  we need to compute\n\n$$ P[\\text{Single}\\,|\\,\\text{Evade }=\\text{ No}] = 2 / 7 = 0.29 $$\n\nand\n\n$$ P[\\text{Married}\\,|\\,\\text{Evade }=\\text{ No}] = 4 / 7 = 0.57 $$\n\nand\n\n$$ P[\\text{Divorced}\\,|\\,\\text{Evade }=\\text{ No}] = 1 / 7 = 0.14 $$\n\nOne problem that can arise is when a histogram bin has zero entries.  Then the conditional probability for this attribute value is zero, which overrides all the other factors and yields a zero probability.\n\nThere are various strategies for making small corrections to the counts that avoid this problem.\n\n<center>\n    \n<img src=\"figs/L16-sample-data.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n__Continuous Attributes.__\n\nContinuous attributes can be handled via histograms as well, by binning up the values.   \n\nIn the above example, we could create bins to hold ranges of values for <font color = \"blue\"> Taxable Income </font>.\n\nHowever, another commonly used approach is to assume that the data follow a parametric probability distribution.\n\nMost often the Gaussian is used (of course).\n\nSo we might form conditional probabilities for <font color = \"blue\"> Taxable Income </font> as\n\n$$ P[\\text{Taxable Income} = x\\,|\\,\\text{Evade }=\\text{ No}] = \\mathcal{N}(x; \\mu_\\text{No}, \\sigma_\\text{No}) $$\n\n::: {#54d3ef99 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nfrom scipy.stats import norm\neno = np.array([125000, 100000, 70000, 120000, 60000, 220000, 75000])\neyes = np.array([95000, 85000, 75000])\nmu_no = np.mean(eno)\nsig_no = np.std(eno)\nmu_yes = np.mean(eyes)\nsig_yes = np.mean(eyes)\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = mu_no, scale = sig_no), norm.ppf(0.999, loc = mu_no, scale = sig_no), 100)\nplt.plot(x, norm.pdf(x, loc = mu_no, scale = sig_no),'b-', lw = 5, alpha = 0.6, label = 'Evade = No')\nx = np.linspace(norm.ppf(0.001, loc = mu_yes, scale = sig_yes), norm.ppf(0.999, loc = mu_yes, scale = sig_yes), 100)\nplt.plot(x, norm.pdf(x, loc = mu_yes, scale = sig_yes),'g-', lw = 5, alpha = 0.6, label = 'Evade = Yes')\nplt.xlim([0, 200000])\nplt.xlabel('Taxable Income', size=14)\nplt.legend(loc = 'best')\nplt.title('Class-Conditional Distributions')\nplt.ylabel(r'$p(x)$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-4-output-1.png){width=609 height=455}\n:::\n:::\n\n\nIn summary:\n    \n* Naive Bayes attacks the classification problem through probability, \n    * which is perhaps the most natural formal tool for the problem.\n* Training is very simple, based on estimating class-conditional histograms or parametric densities of features.\n* Naive Bayes can work well in high-dimensional settings (many features) \n    * Many times the correct label is the MAP estimate, even if individual probabilities are less accurate.\n* Its principal drawback is its assumption of __independence__ among the features.\n\n## Support Vector Machines\n\nWe now turn to the support vector machine (SVM).  \n\nThe SVM is based on explicit __geometric__ considerations about how best to build a classifier.\n\nAs an example, here is a set of training data, considered as points in $\\mathbb{R}^d$:\n\n<center>\n    \n<img src=\"figs/L16-SVM-1.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nWe will start with the idea of a __linear separator__.\n\nThis is a hyperplane that forms a decision boundary.\n\nHere is one possible separator:\n\n<center>\n    \n<img src=\"figs/L16-SVM-2.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nHere is another possible separator:\n\n<center>\n    \n<img src=\"figs/L16-SVM-3.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nWhich separator is __better__?\n\nWell, they both perfectly separate the two classes in the training data.\n\n<center>\n    \n<img src=\"figs/L16-SVM-4.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nBut what we really care about is accuracy on the test data -- __generalization__ ability.\n\nIt seems that $B_1$ is a better choice, because it is __farther__ from __both__ classes.\n\nSo, new data falling in the region between training classes is more likely to be correctly classified by $B_1$.\n\nThis leads to a principle for choosing the best separator:\n    \n* We are concerned with the __margin__ between the separator and the data, and\n* We prefer the separator that __maximizes the margin__.\n\nIn fact, there are theoretical results suggesting that this is an optimal strategy for choosing a separator that has good generalization ability.\n\n<center>\n    \n<img src=\"figs/L16-SVM-5.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n### Linear SVM: Separable Case\n\nLet's see how we can train an SVM.\n\nAs usual, our training data consists of tuples $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$, and (by convention) $y_i \\in \\{-1, 1\\}$.\n\nWe're going to assume (for now) that our data can be perfectly separated by a hyperplane.\n\nAny hyperplane (such as $B_1$ below) can be written as:\n\n$$ w_1 x_1 + w_2 x_2 + \\dots + w_d x_d + b = 0$$\n\nor more concisely:\n\n$$ \\mathbf{w}^T\\mathbf{x} + b = 0. $$\n\nSo our decision boundary (ie, our classifier) has parameters $\\{w_i\\}$ and $b$.\n\n<center>\n    \n<img src=\"figs/L16-SVM-5.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nFor any $\\mathbf{x}_+$ from the positive class (circle) located __above__ the decision boundary, \n\n$$ \\mathbf{w}^T\\mathbf{x}_+ + b = k $$\n\nfor some __positive__ $k$.\n\n<center>\n    \n<img src=\"figs/L16-SVM-6.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n\nLikewise for any $\\mathbf{x}_-$ from the negative class (square) located __below__ the decision boundary, \n\n$$ \\mathbf{w}^T\\mathbf{x}_- + b = -k $$\n\nfor the same $k$.\n\nWe'll rescale the parameters $\\mathbf{w}$ and $b$ by dividing through by $k$, thus obtaining new equations for the __same__ hyperplanes:\n\n$$ b_{11}: \\mathbf{w}^T\\mathbf{x}_+ + b = 1 $$\n$$ b_{12}: \\mathbf{w}^T\\mathbf{x}_- + b = -1 $$\n\nHow far apart are these hyperplanes?\n\nThe vector $\\mathbf{w}$ is orthogonal to the hyperplanes $b_{11}$ and $b_{12}$.\n\nSo the distance between the two hyperplanes is the component of $(\\mathbf{x}_+ - \\mathbf{x}_-)$ that is in the direction of $\\mathbf{w}$.\n\nIn other words, the projection of $(\\mathbf{x}_+ - \\mathbf{x}_-)$ onto $\\mathbf{w}$.\n\nWhich is given by $$d = \\frac{\\mathbf{w}^T}{\\Vert\\mathbf{w}\\Vert}(\\mathbf{x}_+ - \\mathbf{x}_-).$$\n\nBut subtracting the equations for the hyperplanes, we get:\n\n$$ \\mathbf{w}^T(\\mathbf{x}_+ - \\mathbf{x}_-) = 2 $$\n\nSo we conclude:\n\n$$ d = \\frac{2}{\\Vert\\mathbf{w}\\Vert}. $$\n\nNow, we have a measure for how good a separator is in terms of its parameters $\\mathbf{w}$.\n\nWe want the separator to maximize $ d = \\frac{2}{\\Vert\\mathbf{w}\\Vert},$ or equivalently, minimize $ \\Vert\\mathbf{w}\\Vert$.\n\nWhat separators should we consider (ie, search through)?\n\nThe answer is, we consider all separators that correctly classify each point.\n\nThat is, for all training points ($\\mathbf{x}_i, y_i)$:\n    \n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\ge 1 \\text{ if } y_i = 1 $$\n\nand \n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\le -1 \\text{ if } y_i = -1 $$\n\nSo now we can formally state the problem of defining the __maximum margin separator__:\n\n$$ \\mathbf{w}^* = \\arg\\min_\\mathbf{w} \\Vert\\mathbf{w}\\Vert$$\n\nSubject to:\n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\ge 1 \\text{ if } y_i = 1 $$\n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\le -1 \\text{ if } y_i = -1 $$\n\nThis is a __constrained obtimization problem__ with a __quadratic objective function.__\n\nThe quadratic form $\\Vert \\mathbf{w}\\Vert$ is positive definite, so it is strictly convex (it has a unique global minimum).\n\nSuch problems are called <font color = \"blue\">quadratic programs.</font>   \n\nThere are standard methods for solving them.   The methods are effective but can be slow.\n\nThe complexity of the problem grows with the number of constraints (ie, the number of training points).\n\nUltimately, only a subset of the training points (constraints) will determine the final solution.\n\nThe points that determine the solution \"support\" it.  They are the __support vectors.__\n\n<center>\n    \n<img src=\"figs/L16-SVM-6.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n### Linear SVM: Non-Separable Case\n\nIt may well happen that there is no hyperplane that perfectly separates the classes.\n\n<center>\n    \n<img src=\"figs/L16-SVM-7.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nIn this case, we allow points to fall on the \"wrong\" side of their separator, but we add a penalty for this occurrence.\n\nTo express this formally, we introduce __slack__ variables, one per data point: $\\xi_i$.   \n\nEach $\\xi_i$ measures how far the data point is on the \"wrong side\" of its separator.\n\nSo the constraints are loosened:\n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\ge 1-\\xi_i \\text{ if } y_i = 1 $$\n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\le -1+\\xi_i \\text{ if } y_i = -1 $$\n\n$$ \\xi_i \\ge 0.$$\n\nAnd the new problem is:\n\n$$ \\mathbf{w}^* = \\arg\\min_\\mathbf{w} \\left(\\Vert\\mathbf{w}\\Vert + C (\\sum_{i=1}^N \\xi_i)^k\\right),$$\n\nsubject to the above constraints.\n\nNotice we have introduced a hyperparameter: $C$.\n    \n$C$ essentially controls the complexity of the SVM.  \n\nHence it needs to be set by cross-validation.\n\nA small value of $C$ will result in allowing many points to fall on the wrong side of their corresponding hyperplane (having a nonzero $\\xi_i$) resulting in a large number of support vectors.   So small $C$ results in a more stable decision boundary.\n\nLarge $C$ values will penalize violations heavily, and will result in fewer nonzero $\\xi_i$s, leading to fewer support vectors.\n\n### Nonlinear SVM\n\nFinally, we consider the case in which the decision boundary is strongly nonlinear.\n\n<center>\n    \n<img src=\"figs/L16-SVM-8.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\nThe basic idea here is that we take the data and transform it into another, higher-dimensional space.\n\nHere is the same dataset on transformed coordinates:\n\n$$ x_1 \\rightarrow x_1 $$\n\n$$ x_2 \\rightarrow (x_1 + x_2)^4 $$\n\n<center>\n    \n<img src=\"figs/L16-SVM-9.png\" alt=\"Figure\" width=\"40%\">\n    \n</center>\n\n(We are not showing more dimensions here, but the principle is the same.)\n\nIn the higher dimensional space, the points may be (approximately) separable.\n\nTo achieve this using the framework of the SVM, we use a __kernel__.\n\nA kernel is a similarity function $K(\\mathbf{x}, \\mathbf{y})$.\n\nThere are many ways to define kernels. \n\nThe most popular kernels are:\n    \n* Linear (aka Inner Product): $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^T\\mathbf{y}$\n* Polynomial: $K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T\\mathbf{y})^d$\n* Gaussian: $K(\\mathbf{x}, \\mathbf{y}) = \\text{exp}(-\\gamma\\Vert\\mathbf{x}-\\mathbf{y}\\Vert^2)$\n\nThe Gaussian kernel is also called a \"Radial Basis Function\".   \n\nThere is an efficient way to train an SVM using a kernel for a similarity function.\n\nThe basic idea is that the standard linear SVM is trained using Euclidean distance.\n\nHowever, the squared Euclidean distance between two vectors is:\n    \n$$ \\Vert\\mathbf{x} - \\mathbf{y}\\Vert^2 = (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y}) = \\mathbf{x}^T\\mathbf{x} + \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{x}^T\\mathbf{y}$$\n\nSo the Euclidean distance can be defined entirely in terms of the __inner product__ kernel!\n\nTo train an SVM with a different kernel, we just replace all the inner products with calls to our new kernel function. \n\nThe result is that we can obtain highly curved decision boundaries (we'll demonstrate next).\n\nIn practice, RBF works well in many cases.\n\n### SVM: Summary\n\n* In practice, SVMs have shown good results on many problems.\n\n* In particular, it is effective at dealing with high-dimensional data and avoids the curse of dimensionality.\n\n* Since all data is represented as vectors, and we are relying on distance functions like Euclidean distance, it is important to pay attention to feature scaling when using SVMs.\n\n## SVM and Naive Bayes in Python\n\nWe work with a dataset describing Italian wine samples (https://archive.ics.uci.edu/ml/datasets/Wine). \n\nWe will take the alcohol content of the wine, and its Malic acid content, and use it to predict the grape type (cultivar).\n\n::: {#bb6d5dc4 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=5}\n``` {.python .cell-code}\nimport sklearn.utils as utils\nimport sklearn.svm as svm\nimport sklearn.model_selection as model_selection\nimport sklearn.datasets as datasets\n```\n:::\n\n\n::: {#ce04c7bf .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nwine = pd.read_table(\"data/wine.data\", sep=',')\n\nwine.columns = [\n            'region',\n            'Alcohol',\n            'Malic acid',\n            'Ash',\n            'Alcalinity of ash',\n            'Magnesium',\n            'Total phenols',\n            'Flavanoids',\n            'Nonflavanoid phenols',\n            'Proanthocyanins',\n            'Color intensity',\n            'Hue',\n            'OD280/OD315 of diluted wines',\n            'Proline']\n\nX = wine[['Alcohol', 'Malic acid']].values\ny = wine['region'].values\n```\n:::\n\n\nWe'll first fit a linear SVM to the data:\n\n::: {#cae07a79 .cell execution_count=7}\n``` {.python .cell-code}\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n                                                                    y, \n                                                                    test_size = 0.5)\n```\n:::\n\n\n::: {#7813c7c5 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=8}\n``` {.python .cell-code}\nsvc = svm.SVC(kernel = 'linear')\nsvc.fit(X_train, y_train)\ny_pred_test = svc.predict(X_test)\nprint(f'Accuracy of SVM on test set: {svc.score(X_test, y_test):0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of SVM on test set: 0.764\n```\n:::\n:::\n\n\nLet's visualize the decision boundaries:\n\n::: {#bfb660ba .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nfrom matplotlib.colors import ListedColormap\n# Create color maps for 3-class classification problem, as with iris\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\ndef plot_estimator(estimator, X, y):\n    \n    try:\n        X, y = X.values, y.values\n    except AttributeError:\n        pass\n    \n    estimator.fit(X, y)\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.axis('tight')\n    # plt.axis('off')\n    plt.tight_layout()\n```\n:::\n\n\n::: {#bf0ca78a .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nplot_estimator(svc, X, y)\nplt.xlabel('Alcohol')\nplt.ylabel('Malic Acid');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-10-output-1.png){width=681 height=489}\n:::\n:::\n\n\nNote that in practice we should pay attention to feature scaling when using SVMs.  We haven't done that here.\n\nAs described already, the SVM gets its name from the samples in the dataset from each class that lie closest to the other class. \n\nThese training samples are called \"support vectors\" because changing their position in the $d$-dimensional feature space would change the location of the decision boundary.\n\nIn `scikit-learn`, the indices of the support vectors for each class can be found in the `support_vectors_` attribute of the SVC object. \n\nHere, we will use just two of the three classes for clarity. \n\nThe support vectors are circled.   Can you visualize the two separator hyperplanes?\n\n::: {#a4d5915c .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\n# Extract classes 1 and 2\nX, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])]\n\nplot_estimator(svc, X, y)\nplt.xlabel('Alcohol')\nplt.ylabel('Malic Acid')\nplt.scatter(svc.support_vectors_[:, 0], \n           svc.support_vectors_[:, 1], \n           s=120, \n           facecolors='none', \n           edgecolors = 'k',\n           linewidths=2,\n           zorder=10);\nplt.title(f'2-class accuracy on entire dataset: {svc.score(X, y):0.3f}');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-11-output-1.png){width=681 height=509}\n:::\n:::\n\n\n### Regularization\n\nSince the classes are not linearly separable, there are nonzero slack variables, each of which is associated with a support vector.\n\nTherefore we should consider how regularization is tuned via the $C$ parameter. \n\nIn practice, a large $C$ value means that the number of support vectors is small (less regularization, more model complexity), while a small $C$ implies many support vectors (more regularization, less model complexity). \n\n`scikit-learn` sets a default value of $C=1$.\n\n::: {#45d705eb .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\nsvc = svm.SVC(kernel='linear', C=1e4)\nplot_estimator(svc, X, y)\nplt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n            facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title(f'C = 10000: small number of support vectors (acc: {svc.score(X, y):0.3f})');\n\nsvc = svm.SVC(kernel='linear', C=1e-2)\nplot_estimator(svc, X, y)\nplt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n            facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title(f'C = 0.01: high number of support vectors (acc: {svc.score(X, y):0.3f})');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-12-output-1.png){width=662 height=490}\n:::\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-12-output-2.png){width=662 height=490}\n:::\n:::\n\n\n### Kernels\n\nWe can also choose from a suite of available kernels:\n\n* linear, \n* poly, \n* rbf, \n* sigmoid, or \n* precomputed.\n\nOr, a custom kernel can be passed as a function. \n\nNote that the radial basis function (rbf) kernel is just a Gaussian kernel, but with parameter $\\gamma = \\frac{1}{\\sigma^2}$.\n\n__Linear Kernel__\n\n::: {#6b54a38b .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nsvc_lin = svm.SVC(kernel = 'linear')\nplot_estimator(svc_lin, X, y)\nplt.scatter(svc_lin.support_vectors_[:, 0], svc_lin.support_vectors_[:, 1], \n            s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('Linear kernel')\ny_pred_test = svc_lin.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-13-output-1.png){width=662 height=490}\n:::\n:::\n\n\n__Polynomial Kernel__\n\n::: {#6178a527 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nsvc_poly = svm.SVC(kernel='poly', degree=4)\nplot_estimator(svc_poly, X, y)\nplt.scatter(svc_poly.support_vectors_[:, 0], svc_poly.support_vectors_[:, 1], \n           s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('Polynomial kernel')\ny_pred_test = svc_poly.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-14-output-1.png){width=662 height=490}\n:::\n:::\n\n\n__RBF kernel__\n\n::: {#e64cfc33 .cell hide_input='false' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nsvc_rbf = svm.SVC(kernel='rbf', gamma=100, C = 1e2)\nplot_estimator(svc_rbf, X, y)\nplt.scatter(svc_rbf.support_vectors_[:, 0], svc_rbf.support_vectors_[:, 1], \n           s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('RBF kernel')\ny_pred_test = svc_rbf.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-15-output-1.png){width=662 height=490}\n:::\n:::\n\n\n### Cross-Validation\n\nLet's evaluate our choice of hyperparameter $C$.\n\nWe have seen how to tune hyperparameters already using `model_selection.train_test_split()`.\n\nNow we'll use a utility `model_selection.cross_val_score()` which will do $k$-fold cross validation for us, for a single hyperparmeter setting, automatically:\n\n::: {#9dd89c0f .cell slideshow='{\"slide_type\":\"-\"}' execution_count=16}\n``` {.python .cell-code}\nf = svm.SVC(kernel = 'linear', C = 1)\nscores = model_selection.cross_val_score(f, \n                                         wine[['Alcohol', 'Malic acid']], \n                                         wine['region'], \n                                         cv = 5)\n\nprint(f'Scores: {scores}')\nprint(f'Accuracy: {scores.mean():0.2f} (+/- {scores.std()/np.sqrt(5):0.2f})')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScores: [0.69444444 0.80555556 0.82857143 0.74285714 0.68571429]\nAccuracy: 0.75 (+/- 0.03)\n```\n:::\n:::\n\n\nLet's use this to do a grid search to tune $C$:\n\n::: {#9c64240c .cell hide_input='false' slideshow='{\"slide_type\":\"-\"}' execution_count=17}\n``` {.python .cell-code}\nmeans = []\nstds = []\nfolds = 5\nC_vals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor C_val in C_vals:\n    f = svm.SVC(kernel='linear', C = C_val)\n    scores = model_selection.cross_val_score(f, wine[['Alcohol', 'Malic acid']], wine['region'], cv = folds)\n    means.append(np.mean(scores))\n    stds.append(np.std(scores) / np.sqrt(folds))\nacc = np.array(means)\nstderr = np.array(stds)\nC_s = np.array(C_vals)\n```\n:::\n\n\n::: {#02016c20 .cell hide_input='false' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nplt.errorbar(np.log10(C_vals), acc, stderr)\nplt.xlabel('log10(C)')\nplt.ylabel('Accuracy')\nplt.title(r'Generalization Accuracy of Linear SVM as Function of $C$');\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-18-output-1.png){width=597 height=450}\n:::\n:::\n\n\n### SVM and NB: the Iris Data\n\nTo compare SVM and Naive Bayes, we'll look at the Iris dataset again, and just using two features for visualization.\n\nWe will not hold out data since we're just interested in the shapes of the decision boundaries.\n\n::: {#2a69cedb .cell slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=19}\n``` {.python .cell-code}\niris = datasets.load_iris()\nX = iris.data[:, :2]                     \ny = iris.target\n```\n:::\n\n\n::: {#b3a1dc10 .cell execution_count=20}\n``` {.python .cell-code}\nC = 1.0  \n\nsvc = svm.SVC(kernel = 'linear', C = C).fit(X, y)\nrbf_svc = svm.SVC(kernel = 'rbf', gamma = 0.7, C = C).fit(X, y)\npoly_svc = svm.SVC(kernel = 'poly', degree = 3, C = C).fit(X, y)\n```\n:::\n\n\nTo use Naive Bayes \"out of the box\", one has to treat all the features as either:\n* Gaussian\n* Multinomial (Categorical)\n* Binary\n\n`scikit-learn` provides a Naive Bayes classifier for each of these cases.\n\nWe'll use the Gaussian.\n\n::: {#1a8be633 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB().fit(X, y)\n```\n:::\n\n\n::: {#630690cf .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=22}\n``` {.python .cell-code}\n# create a mesh to plot in\nh = .02  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVM with linear kernel',\n          'Naive Bayes',\n          'SVM with RBF kernel', 'SVM with poly kernel']\n\nfig = plt.figure(figsize=(12,12))\n\nfor i, clf in enumerate((svc, gnb, rbf_svc, poly_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i], size = 20)\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-22-output-1.png){width=1142 height=1143}\n:::\n:::\n\n\n",
    "supporting": [
      "16-Classification-III-NB-SVM_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}