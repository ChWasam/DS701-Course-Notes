{
  "hash": "ca0c0f481e20ffca7a3d536636811dec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Naive Bayes and Support Vector Machines\njupyter: python3\nfig-align: center\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/16-Classification-III-NB-SVM.ipynb)\n\n\n\nToday we'll look at two more very commonly-used, widely-applicable classification methods.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![Thomas Bayes](figs/Thomas_Bayes.gif){height=\"250px\"}\n:::\n::: {.column width=\"50%\"}\n![SVM](figs/L16-SVM-6.png){height=\"250px\"}\n:::\n::::\n\n\n* the _Naive Bayes Classifier_\n* the _Support Vector Machine_\n\nBefore we dive into this material we will briefly touch on the topic of validation and testing.\n\n# Validation and Testing\n\n## Validation and Test Sets\n\n### Validation Set\n\n- **Purpose:** Used to tune model parameters and select the best model.\n- **Usage:** Helps in model selection and hyperparameter tuning.\n- **Size:** Typically 10-20% of the dataset.\n- **Example:** Adjusting the learning rate in a neural network.\n\n---\n\n### Test Set\n\n- **Purpose:** Used to evaluate the final model's performance.\n- **Usage:** Provides an unbiased evaluation of the model.\n- **Size:** Typically 10-20% of the dataset.\n- **Example:** Assessing the accuracy of a trained model on unseen data.\n\n---\n\n### Key Differences\n- **Validation Set:** Used during model training.\n- **Test Set:** Used after model training is complete.\n- **Goal:** Ensure the model generalizes well to new data.\n\n## Train/Test Split\n\n- **Purpose:** Simple method to evaluate model performance.\n- **Usage:** Split the dataset into two parts: training and testing.\n- **Typical Split:** 80% training, 20% testing.\n- **Pros:** Easy to implement, quick evaluation.\n- **Cons:** May not provide the best model tuning.\n\n## Train/Validate/Test Split\n\n- **Purpose:** More robust method for model evaluation and tuning.\n- **Usage:** Split the dataset into three parts: training, validation, and testing.\n- **Typical Split:** 60% training, 20% validation, 20% testing.\n- **Pros:** Allows for model tuning and unbiased evaluation.\n- **Cons:** Requires more data, more complex.\n\n## Key Differences\n\n- **Train/Test Split:** \n  - Simpler, faster.\n  - May lead to overfitting or underfitting.\n\n- **Train/Validate/Test Sets:**\n  - More comprehensive.\n  - Better for hyperparameter tuning and model selection.\n\n## K-Fold Cross Validation\n\n### What is k-Fold Cross-Validation?\n\n- **Definition:** A technique to evaluate the performance of a model by dividing the data into k subsets (folds).\n- **Process:**\n\n  1. Split the dataset into k equal-sized folds.\n  2. Train the model on k-1 folds.\n  3. Validate the model on the remaining fold.\n  4. Repeat the process k times, each time with a different fold as the validation set.\n  5. Average the results to get the final performance metric.\n\n--- \n\n### Benefits\n\n- **More Reliable Estimates:** Provides a better estimate of model performance compared to a single train/test split.\n- **Efficient Use of Data:** Utilizes the entire dataset for both training and validation.\n- **Reduces Overfitting:** Helps in detecting overfitting by validating on multiple subsets.\n\n---\n\n### Example\n\n- **5-Fold Cross-Validation:** The dataset is split into 5 folds. The model is trained and validated 5 times, each time with a different fold as the validation set.\n\n### Key Points\n\n- **Choice of k:** Common choices are 5 or 10, but it can vary depending on the dataset size.\n- **Computational Cost:** More computationally intensive than a single train/test split.\n\n## Implementing 5-Fold Cross-Validation in sklearn\n\n```python\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\nX, y = data.data, data.target\n\nmodel = RandomForestClassifier()\n\nscores = cross_val_score(model, X, y, cv=5)\n\nprint(\"Cross-Validation Scores:\", scores)\nprint(\"Mean Score:\", scores.mean())\n```\n\n# Naive Bayes\n\nThe classification problem is given a data point $\\mathbf{x}$ predict the corresponding label $y$.  \n\nA fundamental way to attack this problem is via probability.\n\nA key tool will be __Bayes Rule__.\n\n## Bayes Rule\n\nLet $A, B$ be events from a sample space $\\Omega$. Recall that Bayes' rule states\n    \n$$ P(A \\vert B) = \\frac{P(B\\vert A) \\cdot P(A)}{P(B)}. $$\n\nThis was derived in our [Probability and Statistics Refresher](03-Probability-and-Statistics-Refresher.qmd).\n\nWe've now seen several examples of the importance of Bayes' rule (e.g., [GMMs](09-Clustering-IV-GMM-EM.qmd)) where we computed the *unknown* quantity $P(A \\vert B)$ by using the *known* values for $P(B\\vert A)$, $P(A)$, and $P(B)$.\n\n\n## Using Bayes Rule in a Classifier\n\nLet's start with a simple example\n\n> A doctor knows that meningitis causes a stiff neck 75% of the time, i.e., $P(S\\vert M) = 0.75$\n\n:::: {.fragment}\nIf a patient comes in with a stiff neck, what is the probability she has meningitis, i.e., what is $P(M\\vert S)$?\n::::\n\n\n:::: {.fragment}\nWe only currently know $P(S\\vert M)$ so we would need additional information to compute $P(M\\vert S)$.\n::::\n\n---\n\nIf the doctor _also_ knows the following information \n\n:::: {.incremental}\n* 1 in 20 people in the population have a stiff neck at any given time, i.e., $P(S) = 0.05$\n* 1 in 10,000 people in the population have meningitis at any given time, i.e., $P(M) = 0.0001$\n::::\n\n:::: {.fragment}\nThen we can calculate the answer needed\n\n$$ P(M\\vert S) = \\frac{P(S\\vert M)}{P(S)}\\cdot P(M)= \\frac{0.75}{0.05} \\cdot 0.00001 = \\fbox{$15 \\cdot 1/10000$} = 0.0015. $$\n::::\n\n\n---\n\n::: {.content-visible when-profile=\"slides\"}\n$$ P(M\\vert S) = \\frac{P(S\\vert M)}{P(S)}\\cdot P(M)= \\frac{0.75}{0.05} \\cdot 0.00001 = \\fbox{$15 \\cdot 1/10000$} = 0.0015. $$\n:::\n\n\nThe above expressions shows the essence of Bayesian reasoning\n\n:::: {.incremental}\n* We have prior knowledge about the probability of someone having meningitis, i.e., a random person has probability 1/10000 of having meningitis.\n* When __we learn__ that the person has a stiff neck, it __increases their probability__ of having meningitis by a factor of 15.\n::::\n\n## Priors and Posteriors\n\nWe give these probabilities names according to their roles.\n\n:::: {.incremental}\n* The random person's probability (1/10000) is called the __prior__ probability\n* The specific patient's probability (15 $\\cdot$ 1/10000) is called the __posterior__ probability.\n::::\n\n:::: {.fragment}\nWe use this same principle to construct a classifier.\n::::\n\n## Probabilistic Classification\n\nIn classification problems we assume that we are given a set of data points $\\mathbf{x}\\in\\mathbb{R}^{d}$ with $d$ features and corresponding class labels $y$. \n\nIn our meningitis example, there is a single __attribute__ which is stiff neck. \n\nThe __class label__ is either meningitis or no meningitis.\n\n:::: {.fragment}\nHow does the value of each feature change the probability of the class label?\n::::\n\n---\n\nMore generally, consider a data point $\\mathbf{x}$ having attributes $x_1, x_2, \\dots, x_d$ and various classes (labels) for items: $C_1, \\dots, C_k$.\n\nOur goal is to predict the class of $\\mathbf{x}$.\n\nTo do that, we will compute \n$$P(C_1\\vert\\mathbf{x}), P(C_2\\vert\\mathbf{x}), \\ldots, P(C_k\\vert\\mathbf{x}).$$\n\nThese form a __soft classification__ of $\\mathbf{x}$.\n\n\n## Maximum A Posteriori\n\nFrom the soft labels, we can form a hard classification. One way would be to choose the class with the highest probability.\n\nThis is the MAP (_Maximum A Posteriori_) Estimate: \n\n$$\\hat{C} = \\arg\\max_{\\{C_i\\}} P(C_i\\vert\\mathbf{x}).$$\n\nNow $P(C_i\\vert\\mathbf{x}) = P(C_i\\vert x_1, x_2, \\ldots, x_d)$\n\n:::: {.fragment}\nHow can we approach the problem of computing $P(C_i\\vert\\mathbf{x})$?\n::::\n\n--- \n\nThe __key idea__ is that Bayes' Rule makes clear that \n\n$$ P(C_i\\vert\\mathbf{x}) = \\frac{P(\\mathbf{x}\\vert C_i)}{P(\\mathbf{x})} \\cdot P(C_i).$$\n\nWhen we vary $C_i$ in the above expression, $P(\\mathbf{x})$ is not changing. \n\nThis implies that the $\\hat{C}$ that maximizes \n\n$$P(C_i\\vert x_1, x_2, \\ldots, x_d)$$\n\nis the __same__ as the $\\hat{C}$ that maximizes \n\n$$P(x_1, x_2, \\ldots, x_d\\vert C_i)\\cdot P(C_i).$$\n\n---\n\n::: {.content-visible when-profile=\"web\"}\nThis gives us an possible way to solve the problem.\n:::\n\nThe issue that remains is how to estimate\n\n$$ P(x_1, x_2, \\ldots, x_d\\vert C_i).$$\n\n:::: {.fragment}\nWhy is this difficult?\n::::\n\n:::: {.fragment}\nImagine if we tried to compute $P(x_1, x_2, \\ldots, x_d\\vert C_i)$ directly from data. \n\nWe could use a histogram to estimate the necessary distribution, i.e.,  count how many times we see each combination of feature values.\n\nIf there were 20 features ($d = 20$) and 10 possible labels for each feature, then for each class $C_i$ we need to construct a histogram with $10^{20}$ bins.\n::::\n\n:::: {.fragment}\nThis is impossible.\n::::\n\n---\n\nThe underlying problem we face is the high dimensionality of the feature space.\n\nThe size of our histogram is __exponential__ in the number of features.\n\nSo, we need to find a way to reduce the exponential size of the estimation problem.\n\nWe will do that by __factoring__ the distribution $P(x_1, x_2, \\ldots, x_d\\vert C_i)$.\n\nHere is where the *naive* part comes in.\n\n## Naive Bayes\n\nWe will __assume__ that __attributes are independent__ in their assignment to items.\n\nThat is, for two sets of attributes, the values of the attributes in one set do not affect the values in the other set, i.e., all correlations among attributes are zero.\n\nThis is indeed a *naive* assumption ... but it can be surprisingly effective in practice.\n\nThe assumption implies\n    \n$$ P(x_1, x_2, \\ldots, x_d\\vert C_i) = P(x_1\\vert C_i) \\cdot P(x_2\\vert C_i) \\cdots P(x_d\\vert C_i)  $$\n\nThis is very helpful computationally, because the individual factors $P(x_j\\vert C_i)$ reside in a lower-dimensional space than the full distribution.\n\n---\n\nIn a naive Bayes model, the quantity we calculate for each class $C_i$ is\n\n$$ \\left(P(x_1\\vert C_i) \\cdot P(x_2\\vert C_i) \\cdot \\dots P(x_d\\vert C_i)\\right) \\cdots P(C_i).$$\n\nYou can see each conditional probability as a \"correction factor\" to $P(C_i)$.\n\nEach factor $P(x_j\\vert C_i)$ tells us how we should update our confidence in $C_i$ based on the value of a particular feature $x_j$.\n\nSo, what remains then is to estimate $P(x_j\\vert C_i)$ for all $x_j$ and $C_i$.\n\nWe will estimate these quantities from our training data.\n\n## Steps of Naive Bayes\n\nTo summarize the steps of Naive Bayes:\n\n__Train__\n\n* Compute all the per-class attribute probabilities $P(x_j\\vert C_i)$ from training data.\n* Compute all the class probabilities $P(C_i)$ from the training data.\n\n__Predict__\n\n* For a given data point $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$, \n    * For each class $C_i,$ compute $P(x_1\\vert C_i) \\cdot P(x_2\\vert C_i) \\cdot \\dots P(x_d\\vert C_i) \\cdot P(C_i)$\n    * For a hard classification return the  MAP estimate, i.e., the class that maximizes the above expression, \n\n## Computing Attribute Probabilities from Data\n\nAll that remains is to compute the conditional attribute probabilities from data.\n\nThe strategy depends on the attribute type \n\n- __discrete__ or \n- __continuous.__\n\n\n## Discrete Attributes\n\nDiscrete attributes, such as categorical or ordinal attributes, can be handled via histograms.\n\n\nIn the table, to handle the <font color = \"blue\"> Marital Status </font> attribute for the <font color = \"blue\"> $\\text{Evade} = \\text{No}$ </font> class,  we need to compute the following\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n$$ \n\\begin{align*}\nP(\\text{Single}\\vert\\text{Evade } &=\\text{ No}) = 2 / 7 = 0.29,\\\\\nP(\\text{Married}\\vert\\text{Evade } &=\\text{ No}) = 4 / 7 = 0.57,\\\\\nP(\\text{Divorced}\\vert\\text{Evade }&=\\text{ No}) = 1 / 7 = 0.14.\n\\end{align*}\n$$\n:::\n::: {.column width=\"35%\"}\n![](figs/L16-sample-data.png){height=\"250px\"}\n:::\n::::\n\nOne problem that can arise is when a histogram bin has zero entries.  Then the conditional probability for this attribute value is zero, which overrides all the other factors and yields a zero probability.\n\n::: {.content-visible when-profile=\"web\"}\nThere are various strategies for making small corrections to the counts that avoid this problem.\n:::\n\n##  Continuous Attributes\n\nContinuous attributes can be handled via histograms as well. This is done by binning up the values.   \n\nIn the previous example, we could create bins to hold ranges of the continuous values for <font color = \"blue\"> Taxable Income </font>.\n\nHowever, another commonly used approach is to assume that the data follow a parametric probability distribution, such as a Normal (Gaussian) distribution.\n\n---\n\nWe can form conditional probabilities for <font color = \"blue\"> Taxable Income </font> as\n\n$$ P(\\text{Taxable Income} = x\\vert\\text{Evade }=\\text{ No}) = \\mathcal{N}(\\mu_\\text{No}, \\sigma_\\text{No}).$$\n\n::: {#998f1747 .cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.stats import norm\neno = np.array([125000, 100000, 70000, 120000, 60000, 220000, 75000])\neyes = np.array([95000, 85000, 75000])\nmu_no = np.mean(eno)\nsig_no = np.std(eno)\nmu_yes = np.mean(eyes)\nsig_yes = np.mean(eyes)\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = mu_no, scale = sig_no), norm.ppf(0.999, loc = mu_no, scale = sig_no), 100)\nplt.plot(x, norm.pdf(x, loc = mu_no, scale = sig_no),'b-', lw = 5, alpha = 0.6, label = 'Evade = No')\nx = np.linspace(norm.ppf(0.001, loc = mu_yes, scale = sig_yes), norm.ppf(0.999, loc = mu_yes, scale = sig_yes), 100)\nplt.plot(x, norm.pdf(x, loc = mu_yes, scale = sig_yes),'g-', lw = 5, alpha = 0.6, label = 'Evade = Yes')\nplt.xlim([0, 200000])\nplt.xlabel('Taxable Income', size=14)\nplt.legend(loc = 'best')\nplt.title('Class-Conditional Distributions')\nplt.ylabel(r'$p(x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-3-output-1.png){width=609 height=455 fig-align='center'}\n:::\n:::\n\n\n## NB Summary\n\nIn summary:\n    \n* Naive Bayes solves the classification problem through probability.\n* Training is simple, based on estimating class-conditional histograms or parametric densities of features.\n* Naive Bayes can work well in high-dimensional settings\n* Many times the correct label is the MAP estimate, even if individual probabilities are less accurate.\n* Its principal drawback is the assumption of __independence__ among the features.\n\n# Support Vector Machines\n\nWe now turn to the support vector machine (SVM).  \n\nThe SVM is based on explicit __geometric__ considerations about how best to build a classifier.\n\n---\n\nAs an example, here is a set of training data, considered as points in $\\mathbb{R}^d$:\n\n![](figs/L16-SVM-1.png){width=\"40%\"}\n\n## Linear Separator\n\nWe will start with the idea of a __linear separator__. This is a hyperplane that forms a decision boundary.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nHere is one possible separator\n\n![](figs/L16-SVM-2.png){height=\"400px\"}\n:::\n::: {.column width=\"5-%\"}\nHere is another possible separator\n\n![](figs/L16-SVM-3.png){height=\"400px\"}\n:::\n::::\n\n:::: {.fragment}\nWhich separator is __better__?\n::::\n\n---\n\nWell, they both perfectly separate the two classes in the training data.\n\n![](figs/L16-SVM-4.png){width=\"40%\"}\n\nBut what we really care about is accuracy on the test data, i.e., __generalization__ ability.\n\nIt seems that $B_1$ is a better choice because it is __farther__ from __both__ classes.\n\nNew data falling in the region between training classes is more likely to be correctly classified by $B_1$.\n\n## Criterion for Best Separator\n\nThis leads to a principle for choosing the best separator:\n    \n* We are concerned with the __margin__ between the separator and the data, and\n* We prefer the separator that __maximizes the margin__.\n\nIn fact, there are theoretical results suggesting that this is an optimal strategy for choosing a separator that has good generalization ability.\n\n![](figs/L16-SVM-5.png){width=\"40%\"}\n    \n## Linear SVM: Separable Case\n\nLet's see how we can train an SVM.\n\nConsider a training dataset consisting of tuples $\\{(\\mathbf{x}_i, y_i)\\}$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}$.\n\nWe're going to assume that our data can be perfectly separated by a hyperplane.\n\nAny hyperplane (such as $B_1$ below) can be written as:\n\n$$ w_1 x_1 + w_2 x_2 + \\dots + w_d x_d + b = 0$$\n\nor more concisely:\n\n$$ \\mathbf{w}^T\\mathbf{x} + b = 0. $$\n\nSo our decision boundary (i.e., our classifier) has parameters $\\{w_i\\}$ and $b$.\n\n---\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nFor any $\\mathbf{x}_+$ from the positive class (circle) located __above__ the decision boundary, \n\n$$ \\mathbf{w}^T\\mathbf{x}_+ + b = k $$\n\nfor some __positive__ $k$.\n\nLikewise for any $\\mathbf{x}_-$ from the negative class (square) located __below__ the decision boundary, \n\n$$ \\mathbf{w}^T\\mathbf{x}_- + b = -k $$\n\nfor the same $k$.\n:::\n::: {.column width=\"40%\"}\n![](figs/L16-SVM-6.png)\n:::\n::::\n\n---\n\nRescaling the parameters $\\mathbf{w}$ and $b$ by $k$, we obtain new equations for the __same__ hyperplanes\n\n$$ \n\\begin{align*}\nb_{11}: \\mathbf{w}^T\\mathbf{x}_+ + b &= 1 \\\\\nb_{12}: \\mathbf{w}^T\\mathbf{x}_- + b &= -1 \n\\end{align*}\n$$\n\n:::: {.fragment}\nHow far apart are these hyperplanes?\n::::\n\n---\n\nThe vector $\\mathbf{w}$ is orthogonal to the hyperplanes $b_{11}$ and $b_{12}$.\n\nSo the distance between the two hyperplanes is the magnitude of the component of $(\\mathbf{x}_+ - \\mathbf{x}_-)$ that is in the direction of $\\mathbf{w}$.\n\nSpecifically it is the magnitude of the projection of $(\\mathbf{x}_+ - \\mathbf{x}_-)$ onto $\\mathbf{w}$.\n\nThis is given by \n\n$$d = \\frac{\\mathbf{w}^T}{\\Vert\\mathbf{w}\\Vert}(\\mathbf{x}_+ - \\mathbf{x}_-).$$\n\nBut subtracting the equations for the hyperplanes, we get\n\n$$ \\mathbf{w}^T(\\mathbf{x}_+ - \\mathbf{x}_-) = 2.$$\n\nThis implies that\n\n$$ d = \\frac{2}{\\Vert\\mathbf{w}\\Vert}.$$\n\n---\n\nNow, we have a measure for how good a separator is in terms of its parameters $\\mathbf{w}$.\n\nWe want the separator to maximize $d=\\frac{2}{\\Vert\\mathbf{w}\\Vert},$ or equivalently, minimize $\\Vert\\mathbf{w}\\Vert$.\n\n:::: {.fragment}\nWhat separators should we consider?\n::::\n\n:::: {.fragment}\nWe consider all separators that correctly classify each point.\n::::\n\n:::: {.fragment}\nThat is, for all training points $\\{(\\mathbf{x}_i, y_i)\\}$:\n    \n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\ge 1 \\text{ if } y_i = 1 $$\n\nand \n\n$$ \\mathbf{w}^T\\mathbf{x}_i + b \\le -1 \\text{ if } y_i = -1.$$\n::::\n\n## Maximum Margin Separator\n\nWe now formally state the problem of defining the __maximum margin separator__.\n\nMinimize\n\n$$ \\mathbf{w}^* = \\arg\\min_\\mathbf{w} \\Vert\\mathbf{w}\\Vert^{2},$$\n\nsubject to\n\n$$ \n\\begin{align*}\n\\mathbf{w}^T\\mathbf{x}_i + b \\ge 1 \\text{ if } y_i &= 1, \\\\\n\\mathbf{w}^T\\mathbf{x}_i + b \\le -1 \\text{ if } y_i &= -1. \n\\end{align*}\n$$\n\nThis is a __constrained obtimization problem__ with a __quadratic objective function.__\n\nThe quadratic form $\\Vert \\mathbf{w}\\Vert^{2}$ is positive definite, i.e., it is strictly convex and has a unique global minimum.\n\n## Quadratic Programs\n\nSuch problems are called <font color = \"blue\">quadratic programs.</font>   \n\nThere are standard methods for solving them. The methods are effective but can be slow.\n\nThe complexity of the problem grows with the number of constraints, i.e., the number of training points.\n\nUltimately, only a subset of the training points (constraints) will determine the final solution.\n\nThe points that determine the solution *support* it and are called the __support vectors.__\n\n![](figs/L16-SVM-6.png){width=\"40%\"}\n\n## Linear SVM: Non-Separable Case\n\nIt may well happen that there is no hyperplane that perfectly separates the classes.\n\n![](figs/L16-SVM-7.png){width=\"40%\"}\n\nIn this case, we allow points to fall on the \"wrong\" side of their separator, but we add a penalty for this occurrence.\n\nTo express this formally, we introduce __slack__ variables $\\xi_i$.   \n\nEach $\\xi_i$ measures how far the data point is on the \"wrong side\" of its separator.\n\n---\n\nThe new problem is minimize\n\n$$ \\mathbf{w}^* = \\arg\\min_\\mathbf{w} \\left(\\Vert\\mathbf{w}\\Vert + C \\left(\\sum_{i=1}^N \\xi_i\\right)^k\\right),$$\n\nsubject to\n\n$$\n\\begin{align*} \n\\mathbf{w}^T\\mathbf{x}_i + b \\ge 1-\\xi_i \\text{ if } y_i &= 1, \\\\\n\\mathbf{w}^T\\mathbf{x}_i + b \\le -1+\\xi_i \\text{ if } y_i &= -1,\\\\\n\\xi_i &\\ge 0.\n\\end{align*}\n$$\n\n---\n\nNotice we have introduced a hyperparameter $C$, which controls the complexity of the SVM.  This hyperparameter needs to be set by cross-validation.\n\nA small value of $C$ allows many points to fall on the wrong side of their corresponding hyperplane (having a nonzero $\\xi_i$) and results in a large number of support vectors. Small $C$ results in a more stable decision boundary.\n\nLarge $C$ values will penalize violations heavily, and will result in fewer nonzero $\\xi_i$'s, leading to fewer support vectors.\n\n## Nonlinear SVM\n\nFinally, we consider the case in which the decision boundary is strongly nonlinear.\n\n![](figs/L16-SVM-8.png){width=\"40%\"}\n\n---\n\nThe basic idea here is that we take the data and transform it into another, higher-dimensional space.\n\nHere is the same dataset on transformed coordinates:\n\n$$ \n\\begin{align*}\nx_1 &\\rightarrow x_1 \\\\\nx_2 &\\rightarrow (x_1 + x_2)^4 \n\\end{align*}\n$$\n\n![](figs/L16-SVM-9.png){fig-align=\"center\" width=\"35%\"}\n\n::: aside\nWe are not showing more dimensions here, but the principle is the same.\n:::\n\nIn the higher dimensional space, the points may be (approximately) separable.\n\n## Kernels\n\nTo achieve this using the framework of the SVM we use a __kernel__.\n\nA kernel is a  function $K(\\mathbf{x}, \\mathbf{y})$.\n\nThere are many ways to define kernels. \n\nThe most popular kernels are:\n    \n* Linear (inner product): $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^T\\mathbf{y}$\n* Polynomial: $K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T\\mathbf{y})^d$\n* Gaussian: $K(\\mathbf{x}, \\mathbf{y}) = \\text{exp}(-\\gamma\\Vert\\mathbf{x}-\\mathbf{y}\\Vert^2)$\n\n::: aside\nThe Gaussian kernel is also called a *Radial Basis Function*.  \n:::\n\n---\n \nThere is an efficient way to train an SVM using a kernel for a similarity function.\n\nThe basic idea is that the standard linear SVM is trained using Euclidean distance.\n\nHowever, the squared Euclidean distance between two vectors is\n    \n$$ \\Vert\\mathbf{x} - \\mathbf{y}\\Vert^2 = (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y}) = \\mathbf{x}^T\\mathbf{x} + \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{x}^T\\mathbf{y}.$$\n\nSo the Euclidean distance can be defined entirely in terms of the __inner product__ kernel.\n\nTo train an SVM with a different kernel, we replace all the inner products with calls to our new kernel function. \n\nThe result is that we can obtain highly curved decision boundaries.\n\nIn practice, RBF works well in many cases.\n\n## SVM: Summary\n\n* In practice, SVMs have shown good results on many problems.\n* In particular, it is effective at dealing with high-dimensional data and avoids the curse of dimensionality.\n* Since all data is represented as vectors, and we are relying on distance functions like Euclidean distance, it is important to pay attention to feature scaling when using SVMs.\n\n## SVM and Naive Bayes in Python\n\nWe work with a dataset describing [Italian wine samples](https://archive.ics.uci.edu/ml/datasets/Wine). \n\nWe will use as features the alcohol content of the wine and its Malic acid content to predict the grape type (cultivar).\n\n\n\n::: {#a8f617c7 .cell execution_count=5}\n``` {.python .cell-code}\nwine = pd.read_table(\"data/wine.data\", sep=',')\n\nwine.columns = ['region',\n                'Alcohol',\n                'Malic acid',\n                'Ash',\n                'Alcalinity of ash',\n                'Magnesium',\n                'Total phenols',\n                'Flavanoids',\n                'Nonflavanoid phenols',\n                'Proanthocyanins',\n                'Color intensity',\n                'Hue',\n                'OD280/OD315 of diluted wines',\n                'Proline']\n\nX = wine[['Alcohol', 'Malic acid']].values\ny = wine['region'].values\n```\n:::\n\n\nWe'll first fit a linear SVM to the data.\n\n::: {#ad7f3639 .cell execution_count=6}\n``` {.python .cell-code}\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size = 0.5)\n```\n:::\n\n\n::: {#81c5562c .cell execution_count=7}\n``` {.python .cell-code}\nsvc = svm.SVC(kernel = 'linear')\nsvc.fit(X_train, y_train)\ny_pred_test = svc.predict(X_test)\nprint(f'Accuracy of SVM on test set: {svc.score(X_test, y_test):0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of SVM on test set: 0.764\n```\n:::\n:::\n\n\n---\n\n\nLet's visualize the decision boundaries.\n\n::: {#0a0f9f6a .cell execution_count=8}\n``` {.python .cell-code}\nfrom matplotlib.colors import ListedColormap\n# Create color maps for 3-class classification problem, as with iris\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\ndef plot_estimator(estimator, X, y):\n    \n    try:\n        X, y = X.values, y.values\n    except AttributeError:\n        pass\n    \n    estimator.fit(X, y)\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.axis('tight')\n    plt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {#0aee8c52 .cell execution_count=9}\n``` {.python .cell-code}\nplot_estimator(svc, X, y)\nplt.xlabel('Alcohol')\nplt.ylabel('Malic Acid')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-9-output-1.png){width=681 height=489 fig-align='center'}\n:::\n:::\n\n\n---\n\nNote that in practice we should pay attention to feature scaling when using SVMs. We haven't done that here.\n\nAs described already, the SVM gets its name from the samples in the dataset from each class that lies closest to the other class. \n\nThese training samples are called \"support vectors\" because changing their position in the $d$-dimensional feature space would change the location of the decision boundary.\n\nIn `scikit-learn`, the indices of the support vectors for each class can be found in the `support_vectors_` attribute of the SVC object. \n\n---\n\nHere, we will use just two of the three classes for clarity. \n\nThe support vectors are circled.  Can you visualize the two separator hyperplanes?\n\n::: {#e7cd7175 .cell execution_count=10}\n``` {.python .cell-code}\n# Extract classes 1 and 2\nX, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])]\n\nplot_estimator(svc, X, y)\nplt.xlabel('Alcohol')\nplt.ylabel('Malic Acid')\nplt.scatter(svc.support_vectors_[:, 0], \n           svc.support_vectors_[:, 1], \n           s=120, \n           facecolors='none', \n           edgecolors = 'k',\n           linewidths=2,\n           zorder=10)\nplt.title(f'2-class accuracy on entire dataset: {svc.score(X, y):0.3f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/hy/hnd1x3g539dfkglbs96k3myh00d1lj/T/ipykernel_63289/3910156314.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  X, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-10-output-2.png){width=681 height=509 fig-align='center'}\n:::\n:::\n\n\n## Regularization\n\nSince the classes are not linearly separable, there are nonzero slack variables, each of which is associated with a support vector.\n\nTherefore we should consider how regularization is tuned via the $C$ parameter. \n\nIn practice, a large $C$ value means that the number of support vectors is small (less regularization, more model complexity), while a small $C$ implies many support vectors (more regularization, less model complexity). \n\n`scikit-learn` sets a default value of $C=1$.\n\n## Large $C$\n\n::: {#ee89ea88 .cell execution_count=11}\n``` {.python .cell-code}\nsvc = svm.SVC(kernel='linear', C=1e4)\nplot_estimator(svc, X, y)\nplt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n            facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title(f'C = 10000: small number of support vectors (acc: {svc.score(X, y):0.3f})')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-11-output-1.png){width=662 height=490 fig-align='center'}\n:::\n:::\n\n\n## Small $C$\n\n::: {#c7655f39 .cell execution_count=12}\n``` {.python .cell-code}\nsvc = svm.SVC(kernel='linear', C=1e-2)\nplot_estimator(svc, X, y)\nplt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80, \n            facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title(f'C = 0.01: high number of support vectors (acc: {svc.score(X, y):0.3f})')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-12-output-1.png){width=662 height=490 fig-align='center'}\n:::\n:::\n\n\n## Kernels\n\nWe can also choose from a suite of available kernels:\n\n* linear, \n* poly, \n* rbf, \n* sigmoid, or \n* precomputed.\n\nOr, a custom kernel can be passed as a function. \n\nNote that the radial basis function (rbf) kernel is just a Gaussian kernel, but with parameter $\\gamma = \\frac{1}{\\sigma^2}$.\n\n## Linear Kernel\n\n::: {#da01b918 .cell execution_count=13}\n``` {.python .cell-code}\nsvc_lin = svm.SVC(kernel = 'linear')\nplot_estimator(svc_lin, X, y)\nplt.scatter(svc_lin.support_vectors_[:, 0], svc_lin.support_vectors_[:, 1], \n            s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('Linear kernel')\ny_pred_test = svc_lin.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-13-output-1.png){width=662 height=490 fig-align='center'}\n:::\n:::\n\n\n## Polynomial Kernel\n\n::: {#c0641475 .cell execution_count=14}\n``` {.python .cell-code}\nsvc_poly = svm.SVC(kernel='poly', degree=4)\nplot_estimator(svc_poly, X, y)\nplt.scatter(svc_poly.support_vectors_[:, 0], svc_poly.support_vectors_[:, 1], \n           s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('Polynomial kernel')\ny_pred_test = svc_poly.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-14-output-1.png){width=662 height=490 fig-align='center'}\n:::\n:::\n\n\n## RBF Kernel\n\n::: {#ea9f47fc .cell execution_count=15}\n``` {.python .cell-code}\nsvc_rbf = svm.SVC(kernel='rbf', gamma=100, C = 1e2)\nplot_estimator(svc_rbf, X, y)\nplt.scatter(svc_rbf.support_vectors_[:, 0], svc_rbf.support_vectors_[:, 1], \n           s=80, facecolors='none', edgecolors = 'k', linewidths=2, zorder=10)\nplt.title('RBF kernel')\ny_pred_test = svc_rbf.predict(X_test)\nplt.title(f'Accuracy on test set: {svc.score(X_test, y_pred_test):0.3f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-15-output-1.png){width=662 height=490 fig-align='center'}\n:::\n:::\n\n\n## Cross-Validation\n\nLet's evaluate our choice of hyperparameter $C$.\n\nWe have seen how to tune hyperparameters already using `model_selection.train_test_split()`.\n\nNow we'll use a utility `model_selection.cross_val_score()` which will automatically do $k$-fold cross validation for us, for a single hyperparmeter.\n\n::: {#ef3a46bf .cell execution_count=16}\n``` {.python .cell-code}\nf = svm.SVC(kernel = 'linear', C = 1)\nscores = model_selection.cross_val_score(f, \n                                         wine[['Alcohol', 'Malic acid']], \n                                         wine['region'], \n                                         cv = 5)\n\nprint(f'Scores: {scores}')\nprint(f'Accuracy: {scores.mean():0.2f} (+/- {scores.std()/np.sqrt(5):0.2f})')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScores: [0.69444444 0.80555556 0.82857143 0.74285714 0.68571429]\nAccuracy: 0.75 (+/- 0.03)\n```\n:::\n:::\n\n\n---\n\nLet's use this to do a grid search to tune $C$.\n\n::: {#c1f44c96 .cell execution_count=17}\n``` {.python .cell-code}\nmeans = []\nstds = []\nfolds = 5\nC_vals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor C_val in C_vals:\n    f = svm.SVC(kernel='linear', C = C_val)\n    scores = model_selection.cross_val_score(f, wine[['Alcohol', 'Malic acid']], wine['region'], cv = folds)\n    means.append(np.mean(scores))\n    stds.append(np.std(scores) / np.sqrt(folds))\nacc = np.array(means)\nstderr = np.array(stds)\nC_s = np.array(C_vals)\n```\n:::\n\n\n::: {#2b5a8b0e .cell execution_count=18}\n``` {.python .cell-code}\nplt.errorbar(np.log10(C_vals), acc, stderr)\nplt.xlabel('log10(C)')\nplt.ylabel('Accuracy')\nplt.title(r'Generalization Accuracy of Linear SVM as Function of $C$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-18-output-1.png){width=597 height=450 fig-align='center'}\n:::\n:::\n\n\n## SVM and NB: the Iris Dataset\n\nTo compare SVM and Naive Bayes, we'll look at the Iris dataset again, and just using two features for visualization.\n\nWe will not hold out data since we're just interested in the shapes of the decision boundaries.\n\n::: {#eaadf3b3 .cell execution_count=19}\n``` {.python .cell-code}\niris = datasets.load_iris()\nX = iris.data[:, :2]                     \ny = iris.target\n```\n:::\n\n\n::: {#2192db16 .cell execution_count=20}\n``` {.python .cell-code}\nC = 1.0  \n\nsvc = svm.SVC(kernel = 'linear', C = C).fit(X, y)\nrbf_svc = svm.SVC(kernel = 'rbf', gamma = 0.7, C = C).fit(X, y)\npoly_svc = svm.SVC(kernel = 'poly', degree = 3, C = C).fit(X, y)\n```\n:::\n\n\n---\n\nTo use Naive Bayes, one has to treat all the features as either\n\n* Gaussian\n* Multinomial (Categorical)\n* Binary\n\n`scikit-learn` provides a Naive Bayes classifier for each of these cases.\n\nWe'll use the Gaussian.\n\n---\n\n::: {#d6f75ae7 .cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB().fit(X, y)\n```\n:::\n\n\n::: {#684ce4c2 .cell execution_count=22}\n``` {.python .cell-code}\n# create a mesh to plot in\nh = .02  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVM with linear kernel',\n          'Naive Bayes',\n          'SVM with RBF kernel', 'SVM with poly kernel']\n\nfig = plt.figure(figsize=(12,12))\n\nfor i, clf in enumerate((svc, gnb, rbf_svc, poly_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i], size = 20)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](16-Classification-III-NB-SVM_files/figure-html/cell-22-output-1.png){width=1142 height=1143}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Recap\n\nWe covered the following classifiers\n\n- Naive Bayes\n- Support vector machines\n\nWe then compared the two classifiers on the Iris dataset.\n:::\n\n",
    "supporting": [
      "16-Classification-III-NB-SVM_files"
    ],
    "filters": [],
    "includes": {}
  }
}