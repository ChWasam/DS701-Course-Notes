{
  "hash": "2abddbef2e0125f3a9fc28eaab58aada",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: $k$-Nearest Neighbors and Model Selection\njupyter: python3\n---\n\n\nToday we'll expand our repetoire of classification techniques.\n\nIn so doing we'll look at a first example of a new kind of model: nonparametric.\n\n## Parametric vs. Nonparametric Models\n\nThere are many ways to define models (whether supervised or unsupervised).\n\nHowever a key distinction is this:  does the model have a fixed number of parameters, or does the number of parameters grow with the training data?\n\nIf the model has a __fixed number__ of parameters, it is called __parametric.__\n\nIf the number of parameters grows with the data, the model is called __nonparametric.__\n\nParametric models have \n* the advantage of often being faster to use, \n* but the disadvantage of making strong assumptions about the nature of data distributions.\n\nNonparametric models are\n* more flexible,\n* but can be computationally intractable for large datasets.\n\nThe classic example of a nonparametric classifier is called __$k$-Nearest Neighbors.__\n\n\n## $k$-Nearest Neighbors\n\n> When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.\n\n--James Whitcomb Riley (1849 - 1916)\n\n<center>\n\n<img src=\"figs/L15-duck.jpg\" width=\"100px\">\n\n</center>\n\nLike any classifier, $k$-Nearest Neighbors is trained by providing it a set of labeled data.\n\nHowever, at training time, the classifier does very little.   It just stores away the training data.\n\n::: {#bee92055 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\ndemo_y = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\ndemo_X = np.array([[-3,1], [-2, 4], [-2, 2], [-1.5, 1], [-1, 3], [0, 0], [1, 1.5], [2, 0.5], [2, 3], [2, 0], [3, 1], [4, 4], [0, 1]])\ntest_X = [-0.3, 0.7]\n#\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold)\nplt.axis('equal')\nplt.axis('off')\nplt.title('Training Points: 2 Classes');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-4-output-1.png){width=540 height=409}\n:::\n:::\n\n\nThe idea of the $k$-Nearest Neighbors classifier is that, at test time, it simply \"looks at\" the $k$ points in the training set that are nearest to the test input $x$, and makes a decision based on the labels on those points.\n\nBy \"nearest\" we usually mean in Euclidean distance.\n\n::: {#1affe87c .cell hide_input='true' slideshow='{\"slide_type\":\"slide\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold)\nplt.plot(test_X[0], test_X[1], 'ok')\nplt.annotate('Test Point', test_X, [75, 25], \n             textcoords = 'offset points', fontsize = 14, \n             arrowprops = {'arrowstyle': '->'})\nplt.axis('equal')\nplt.axis('off')\nplt.title('Training Points: 2 Classes');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-5-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#a78108cf .cell hide_input='true' slideshow='{\"slide_type\":\"slide\"}' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold)\nplt.plot(test_X[0], test_X[1], 'ok')\nax=plt.gcf().gca()\ncircle = mp.patches.Circle(test_X, 0.5, facecolor = 'red', alpha = 0.2)\nplt.axis('equal')\nplt.axis('off')\nax.add_artist(circle)\nplt.title('1-Nearest-Neighbor: Classification: Red');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-6-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#95fafecb .cell hide_input='true' slideshow='{\"slide_type\":\"slide\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold)\ntest_X = [-0.3, 0.7]\nplt.plot(test_X[0], test_X[1], 'ok')\nax=plt.gcf().gca()\n    #ellipse = mp.patches.Ellipse(gmm.means_[clus], 3 * e[0], 3 * e[1], angle, color = 'r')\ncircle = mp.patches.Circle(test_X, 0.9, facecolor = 'gray', alpha = 0.3)\nplt.axis('equal')\nplt.axis('off')\nax.add_artist(circle)\nplt.title('2-Nearest-Neighbor');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-7-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#8161675a .cell hide_input='true' slideshow='{\"slide_type\":\"slide\"}' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\nplt.figure()\nax=plt.gcf().gca()\n    #ellipse = mp.patches.Ellipse(gmm.means_[clus], 3 * e[0], 3 * e[1], angle, color = 'r')\ncircle = mp.patches.Circle(test_X, 1.4, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold)\ntest_X = [-0.3, 0.7]\nplt.plot(test_X[0], test_X[1], 'ok')\nplt.axis('equal')\nplt.axis('off')\nplt.title('3-Nearest-Neighbor: Classification: Blue');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-8-output-1.png){width=540 height=409}\n:::\n:::\n\n\nNote that $k$-Nearest Neighbors can do either __hard__ or __soft__ classification.\n\nAs a hard classifier, it returns the majority vote of the labels on the $k$ Nearest Neighbors.\n\nWhich may be indeterminate, as above.\n\nIt is also reasonable to weight the votes of neighborhood points according to their distance from $x$.\n\nAs a soft classifier it returns:\n    \n$$ p(x = c\\,|\\,\\mathbf{x}, k) = \\frac{\\text{number of points in neighborhood with label } c}{k} $$\n\n## Model Selection for $k$-NN\n\nEach value of $k$ results in a different model.\n\nThe complexity of the resulting model is therefore controlled by the hyperparameter $k$.\n\nHence we will want to select $k$ using held-out data to avoid overfitting.\n\nConsider this dataset where items fall into three classes:\n\n::: {#4d2a138b .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nimport sklearn.datasets as sk_data\nX, y = sk_data.make_blobs(n_samples=150, \n                          centers=[[-2, 0],[1, 5], [2.5, 1.5]],\n                          cluster_std = [2, 2, 3],\n                          n_features=2,\n                          center_box=(-10.0, 10.0),random_state=0)\nplt.figure(figsize = (5,5))\nplt.axis('equal')\nplt.axis('off')\nplt.scatter(X[:,0], X[:,1], c = y, cmap = cmap_bold, s = 80);\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-9-output-1.png){width=391 height=389}\n:::\n:::\n\n\nLet's observe how the complexity of the resulting model changes as we vary $k$.\n\nWe'll do this by plotting the __decision regions__.   These show how the method would classify each potential test point in the space.\n\n::: {#437ea722 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .1  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\n```\n:::\n\n\n::: {#fe383b5f .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nf, axs = plt.subplots(1, 3, figsize=(15, 5))\nfor i, k in enumerate([1, 5, 25]):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X, y)\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    axs[i].pcolormesh(xx, yy, Z, cmap = cmap_light, shading = 'auto')\n    axs[i].axis('equal')\n    axs[i].axis('off')\n    axs[i].set_title(f'Decision Regions for $k$ = {k}');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-11-output-1.png){width=1135 height=410}\n:::\n:::\n\n\nNotice how increasing $k$ results in smoother decision boundaries.\n\nThese are more likely to show good generalization ability.\n\n## Challenges for $k$-NN\n\nWorking with a $k$-NN classifier can involve some challenges.\n\n1. First and foremost, the computational cost of classification grows with the size of the training data. (Why?) While certain data structures may help, essentially the classification time grows linearly with the data set size.\n\nNote the tradeoff here: the training step is trivial, but the classification step can be prohibitively expensive.\n\n2. Second, since Euclidean distance is the most common distance function used, data scaling is important.   \n\nAs previously discussed, features should be scaled to prevent distance measures from being dominated by a small subset of features.\n\n\n\n\n\n````{margin}\n```{note}\nThe phrase `curse of dimensionality' was coined by the mathematician Richard Bellman in 1957 in his book _Dynamic Programming._\n```\n````\n\n\n\n\n\n<div style = \"float: left; width: 55%;\">\n    \n3. Third concerns the __curse of dimensionality.__\n\nIf training data lives in a high dimensional space, Euclidean distance measures become less effective.\n\nThis is subtle but important, so we will now look at the curse of dimensionality more closely.\n    \n</div>\n    \n<img src=\"figs/L15-curse-frankenstein-edited.jpeg\" alt=\"Figure\" width=\"20%\" float = \"right\">\n\n### The Curse of Dimensionality\n\nThe Curse of Dimensionality is a somewhat tongue in cheek term for serious problems that arise when we use geometric algorithms in high dimensions.\n\nThere are various aspects of the Curse that affect $k$-NN.\n\n__1. Points are far apart in high dimension.__\n\n$k$-NN relies on there being one or more \"close\" points to the test point $x$.   \n\nIn other words, we need the training data to be relatively dense, so there are \"close\" points everywhere.\n\nUnfortunately, the amount of space we work in grows exponentially with the dimension $d$.\n\nSo the amount of data we need to maintain a given density also grows exponentially with dimension $d$.\n\nHence, points in high-dimensional spaces tend not to be close to one another at all.\n\nOne very intuitive way to think about it is this:\n\nIn order for two points to be close in $\\mathbb{R}^d$, they must be close in __each__ of the $d$ dimensions.\n\nAs the number of dimensions grows, it becomes harder and harder for a pair of points to be close in __each__ dimension.\n\n\n\n\n\n````{margin}\n```{note}\nFor a fascinating exploration of the properties of unit spheres in high dimension, see [An Adventure in the Nth Dimension](https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension) by Brian Hayes in _American Scientist._\n```\n````\n\n\n\n\n\n__2. Points tend to all be at similar distances in high dimension.__\n\nThis one is a little harder to visualize.  We'll use formulas instead to guide us.\n\nLet's say points are uniformly distributed in space, so that number of points in a region is proportional to the region's volume.  \n\nHow does volume relate to distance as dimension $d$ grows?\n\nConsider you are at some point in space (say, the test point $x$), and you want to know how many points are within a unit distance from you.\n\nThis is proportional to the volume of a hypersphere with radius 1.\n\nNow, the volume of a hypersphere is $k_d \\,r^d$.  \n\nFor each $d$ there is a different $k_d$.  \n* For $d = 2$, $k_d$ is $4\\pi$, and \n* for $d = 3$, $k_d$ is 4/3 $\\pi$, \n* etc.\n\n::: {#cdb5157d .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nax = plt.figure(figsize = (7,7)).add_subplot(projection = '3d')\n# coordinates of sphere surface\nu, v = np.mgrid[0:2*np.pi:50j, 0:np.pi:50j]\nx = np.cos(u)*np.sin(v)\ny = np.sin(u)*np.sin(v)\nz = np.cos(v)\n#\nax.plot_surface(x, y, z, color='r', alpha = 0.3)\ns3 = 1/np.sqrt(3)\nax.quiver(0, 0, 0, s3, s3, s3, color = 'b')\nax.text(s3/2, s3/2, s3/2-0.2, 'r', size = 14)\nax.set_axis_off()\nplt.title('Hypersphere in $d$ dimensions\\nVolume is $k_d \\,r^d$');\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:13: SyntaxWarning: invalid escape sequence '\\,'\n<>:13: SyntaxWarning: invalid escape sequence '\\,'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_17179/4285470560.py:13: SyntaxWarning: invalid escape sequence '\\,'\n  plt.title('Hypersphere in $d$ dimensions\\nVolume is $k_d \\,r^d$');\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-12-output-2.png){width=537 height=581}\n:::\n:::\n\n\nLet's also ask how many points are within a slightly smaller distance, let's say 0.99.  \n\nThe new distance can be thought of as $1 - \\epsilon$ for some small $\\epsilon$.\n\nThe number of points then of course is proprtional to $k_d (1-\\epsilon)^d$\n\nNow, what is the fraction $f_d$ of all the points that are within a unit distance, but __not__ within a distance of 0.99?\n\n(That is, not within the the hypersphere with radius $1-\\epsilon$)?\n\nThis is \n\n$$ f_d = \\frac{k_d 1^d - k_d (1-\\epsilon)^d}{k^d 1^d} $$\n\n::: {#b59db432 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\nax = plt.figure(figsize = (7,7)).add_subplot(projection = '3d')\n# coordinates of sphere surface\nu, v = np.mgrid[0:2*np.pi:50j, 0:np.pi:50j]\nx = np.cos(u)*np.sin(v)\ny = np.sin(u)*np.sin(v)\nz = np.cos(v)\n#\nax.plot_surface(x, y, z, color='r', alpha = 0.2)\ns3 = 1/np.sqrt(3)\nax.quiver(0, 0, 0, s3, s3, s3, color = 'b')\nax.text(s3/2, s3/2, s3/2-0.2, '1', size = 14)\n#\neps = 0.9\n#\nax.plot_surface(eps * x, eps * y, eps * z, color='b', alpha = 0.2)\nax.quiver(0, 0, 0, eps, 0, 0, color = 'k')\nax.text(1/2-0.2, 0, -0.4, r'$1-\\epsilon$', size = 14)\nax.set_axis_off()\nplt.title('Inner and Outer Hyperspheres');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-13-output-1.png){width=537 height=557}\n:::\n:::\n\n\nNow, $(1-\\epsilon)^d$ goes to 0 as $d \\rightarrow \\infty$.\n\nSo, $f_d$ goes to 1 as $d \\rightarrow \\infty$.\n\nWhich means: in the limit of high $d$, all of the points that are __within__ 1 unit of our location, are almost __exactly__ 1 unit from our location!\n\n::: {#d3efcf5c .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nax = plt.figure(figsize = (7,7)).add_subplot(projection = '3d')\n# coordinates of sphere surface\nu, v = np.mgrid[0:2*np.pi:50j, 0:np.pi:50j]\nx = np.cos(u)*np.sin(v)\ny = np.sin(u)*np.sin(v)\nz = np.cos(v)\n#\nax.plot_surface(x, y, z, color='r', alpha = 0.2)\ns3 = 1/np.sqrt(3)\nax.quiver(0, 0, 0, s3, s3, s3, color = 'b')\nax.text(s3/2, s3/2, s3/2-0.2, '1', size = 14)\n#\neps = 0.9\n#\nax.plot_surface(eps * x, eps * y, eps * z, color='b', alpha = 0.2)\nax.quiver(0, 0, 0, eps, 0, 0, color = 'k')\nax.text(1/2-0.2, 0, -0.4, r'$1-\\epsilon$', size = 14)\nax.set_axis_off()\nplt.title('In high-$d$, All Points Lie in Outer Shell');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-14-output-1.png){width=537 height=558}\n:::\n:::\n\n\n````{margin}\n```{note}\nThe following example is based on _Data Science from Scratch,_ Joel Grus, Second Edition, Chapter 12.  \n```\n````\n\n\n\n\n\nLet's demonstrate this effect in practice.\n\nWhat we will do is create 100 points, scattered at random within a $d$-dimensional space.\n\nWe will look at two quantities:\n* The __minimum__ distance between any two points, and\n* The __average__ distance between any two points.\n\nas we vary $d$.\n\n::: {#af151516 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\n\nnsamples = 1000\nunif_X = np.random.default_rng().uniform(0, 1, nsamples).reshape(-1, 1)\neuclidean_dists = metrics.euclidean_distances(unif_X)\n# extract the values above the diagonal\ndists = euclidean_dists[np.triu_indices(nsamples, 1)]\nmean_dists = [np.mean(dists)]\nmin_dists = [np.min(dists)]\nfor d in range(2, 101):\n    unif_X = np.column_stack([unif_X, np.random.default_rng().uniform(0, 1, nsamples)])\n    euclidean_dists = metrics.euclidean_distances(unif_X)\n    dists = euclidean_dists[np.triu_indices(nsamples, 1)]\n    mean_dists.append(np.mean(dists))\n    min_dists.append(np.min(dists))\n```\n:::\n\n\n::: {#0b4f7005 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nplt.plot(min_dists, label = \"Minimum Distance\")\nplt.plot(mean_dists, label = \"Average Distance\")\nplt.xlabel(r'Number of dimensions ($d$)')\nplt.ylabel('Distance')\nplt.legend(loc = 'best')\nplt.title(f'Comparison of Minimum Versus Average Distance Between {nsamples} Points\\nAs Dimension Grows');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-16-output-1.png){width=615 height=468}\n:::\n:::\n\n\nThe average distance between points grows, but it seems that the minimum distance between points grows about as fast.\n\nSo the ratio of the minimum distance to the average distance grows as well!\n\nLet's look at that ratio:\n\n::: {#1f1b96ef .cell hide_input='true' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nplt.plot([a/b for a, b in zip(min_dists, mean_dists)])\nplt.xlabel(r'Number of dimensions ($d$)')\nplt.ylabel('Ratio')\nplt.title(f'Ratio of Minimum to Average Distance Between {nsamples} Points\\nAs Dimension Grows');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-17-output-1.png){width=589 height=468}\n:::\n:::\n\n\nThis shows that, for any test point $x$, the distance to the __closest__ point to $x$, relatively speaking, gets closer and closer to the __average__ distance between points.\n\nOf course, if we used a point at the average distance for classifying $x$, we'd get a very poor classifier.\n\n__Implications of the Curse.__\n\nFor $k$-NN, the Curse of Dimensionality means that in high dimension, most points are nearly the same distance from the test point.\n\nThis makes $k$-NN ineffective:  it cannot reliably tell which are the $k$ nearest neighbors, and its performance degrades.\n\n__What Can be Done?__\n\nThe problem is that you simply cannot have enough data to do a good job using $k$-NN in high dimensions.\n\nOne approach can be to use a different dissimilarity metric, since Euclidean distance is not useful.   \n\nFor example, you may use cosine distance as your metric instead; this does not suffer from the same problems as Euclidean distance, but you will have to decide if it is a good metric for your problem.\n\nIf you must use $k$-NN with Euclidean distance for your task, another option may be to __reduce__ the dimension of your data.\n\nSurprisingly, this can often be done at little cost in accuracy.\n\nWe will discuss dimensionality reduction techniques at length later in the course.\n\n## In Practice: Model Selection\n\nNext we'll look at two classification methods in practice: \n\n* Decision Trees, and \n* k-Nearest Neighbors.\n\nTo compare them, we will need to understand methods for __model selection.__\n\nTo compare these methods, the question arises:\n    \n### How do we evaluate a classifier?\n\nIn the simple case of a binary classifier, we can call one class the 'Positive' class and one the 'Negative' class.\n\nThe most basic measure of success for a classifer is __accuracy__: what fraction of test points are correctly classified?\n\nOf course, accuracy is important, but it can be too simplistic at times.\n\nFor example, let's say we have a dataset showing __class imbalance__: for example 90% of the data are the Positive class and 10% are the Negative class.\n\nFor this dataset, consider a classifier that always predicts 'Positive'.   Its accuracy is 90%, but it is a very 'stupid' classifier!  (ie, it could be one line of code: `print(Positive)`!)\n\nA better way to measure the classifier's performance is using a Confusion Matrix:\n\n<img src=\"figs/L15-confusion-matrix.png\" alt=\"Figure\" width=\"30%\" float = \"right\">\n\nDiagonal elements represent successes, and off diagonals represent errors.\n\nUsing the confusion matrix we can define some more useful measures:\n   * __Recall__ - defined as the fraction of actual positives correctly classified:  \n       * TP/(TP + FN)\n   * __Precision__ - defined as the fraction of classified positives correctly classified: \n       * TP/(TP + FP)\n\n### Evaluating $k$- Nearest Neighbors and Decision Trees\n\nFirst we'll generate some synthetic data to work with.\n\n::: {#cdff18af .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=17}\n``` {.python .cell-code}\nX, y = datasets.make_circles(noise=.1, factor=.5, random_state=1)\nprint('Shape of data: {}'.format(X.shape))\nprint('Unique labels: {}'.format(np.unique(y)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of data: (100, 2)\nUnique labels: [0 1]\n```\n:::\n:::\n\n\nHere is what the data looks like:\n\n::: {#f6b4a644 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nplt.figure(figsize = (6,6))\nplt.prism()  # this sets a nice color map\nplt.scatter(X[:, 0], X[:, 1], c=y, s = 80)\nplt.axis('off')\nplt.axis('equal');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-19-output-1.png){width=466 height=463}\n:::\n:::\n\n\nRecall that we always want to test on data separate from our training data.\n\nFor now, we will something very simple: take the first 50 examples for training and the rest for testing.   (Later we will do this a better way.)\n\n::: {#db6b0f95 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=19}\n``` {.python .cell-code}\nX_train = X[:50]\ny_train = y[:50]\nX_test = X[50:]\ny_test = y[50:]\n```\n:::\n\n\n::: {#9a51cb73 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=20}\n``` {.python .cell-code}\nfig_size = (12, 5)\n```\n:::\n\n\n::: {#0c7f6fcc .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=21}\n``` {.python .cell-code}\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.title('Training Data')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_test, s = 80)\nplt.title('Test Data')\nplt.axis('equal')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-22-output-1.png){width=912 height=409}\n:::\n:::\n\n\nFor our first example, we will classify the points (in the two classes) using a k-nn classifier.\n\nWe will specify that $k=5$, i.e., we will classify based on the majority vote of the 5 nearest neighbors.\n\n::: {#36d555dd .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=22}\n``` {.python .cell-code}\nk = 5\nknn5 = KNeighborsClassifier(n_neighbors = k)    \n```\n:::\n\n\nIn the context of supervised learning, the `scikit-learn` `fit()` function corresponds to __training__ and the `predict()` function corresponds to __testing.__\n\n::: {#a226f3f4 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=23}\n``` {.python .cell-code}\nknn5.fit(X_train,y_train)\nprint(f'Accuracy on test data: {knn5.score(X_test, y_test)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy on test data: 0.72\n```\n:::\n:::\n\n\nAccuracy of 72% sounds good -- but let's dig deeper. \n\nWe'll call the red points the Positive class and the green points the Negative class.\n\nHere is the confusion matrix:\n\n::: {#007514c5 .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=24}\n``` {.python .cell-code}\ny_pred_test = knn5.predict(X_test)\npd.DataFrame(metrics.confusion_matrix(y_test, y_pred_test), \n             columns = ['Predicted +', 'Predicted -'], \n             index = ['Actual +', 'Actual -'])\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted +</th>\n      <th>Predicted -</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual +</th>\n      <td>14</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>Actual -</th>\n      <td>0</td>\n      <td>22</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLooks like the classifier is getting all of the Negative class correct, but only achieving accuracy of 50% on the Positive class.\n\nThat is, its __precision__ is 100%, but its __recall__ is only 50%.\n\nLet's visualize the results.\n\n::: {#c981297f .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=25}\n``` {.python .cell-code}\nk = 5\n\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\n\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.title('Training')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn5.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-26-output-1.png){width=912 height=429}\n:::\n:::\n\n\nIndeed, the Positive (red) points in the upper half of the test data are all classified incorrectly. \n\nLet's look at one of the points that the classifier got wrong:\n\n::: {#244158fa .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=26}\n``` {.python .cell-code}\nk=5 \ntest_point = np.argmax(X_test[:,1])\nneighbors = knn5.kneighbors([X_test[test_point]])[1]\n\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.scatter(X_train[neighbors,0], X_train[neighbors,1],\n            c = y_train[neighbors], marker='o', \n            facecolors='none', edgecolors='b', s = 80)\nradius = np.max(metrics.euclidean_distances(X_test[test_point].reshape(1, -1), X_train[neighbors][0]))\nax = plt.gcf().gca()\ncircle = mp.patches.Circle(X_test[test_point], radius, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.axis('equal')\nplt.axis('off')\nplt.title(r'Training')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.scatter(X_test[test_point,0], X_test[test_point,1], marker='o', \n            facecolors='none', edgecolors='b', s = 80)\nplt.title('Testing $k$={}\\nAccuracy: {}'.format(k,knn5.score(X_test, y_test)))\nplt.axis('equal')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-27-output-1.png){width=912 height=429}\n:::\n:::\n\n\nFor comparison purposes, let's try $k$ = 3.\n\n::: {#434c1ca3 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=27}\n``` {.python .cell-code}\nk = 3\nknn3 = KNeighborsClassifier(n_neighbors=k)    \nknn3.fit(X_train,y_train)\ny_pred_test = knn3.predict(X_test)\n\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.title(r'Training')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn3.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-28-output-1.png){width=912 height=429}\n:::\n:::\n\n\nAnd let's look at the same individual point as before:\n\n::: {#6f6de372 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=28}\n``` {.python .cell-code}\nk = 3\ntest_point = np.argmax(X_test[:,1])\nX_test[test_point]\nneighbors = knn3.kneighbors([X_test[test_point]])[1]\n\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s = 80)\nplt.scatter(X_train[neighbors, 0], X_train[neighbors, 1], marker = 'o', \n            facecolors = 'none', edgecolors = 'b', s = 80)\nradius = np.max(metrics.euclidean_distances(X_test[test_point].reshape(1, -1), \n                                            X_train[neighbors][0]))\nax = plt.gcf().gca()\ncircle = mp.patches.Circle(X_test[test_point], radius, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.axis('equal')\nplt.axis('off')\nplt.title(r'Training')\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.scatter(X_test[test_point,0], X_test[test_point,1], marker = 'o', \n            facecolors = 'none', edgecolors = 'b', s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn3.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-29-output-1.png){width=912 height=429}\n:::\n:::\n\n\nSo how confident can we be that the test accuracy is 92% in general?\n\nWhat we really need to do is consider __many__ different train/test splits.\n\nThus, the proper way to evaluate generalization ability (accuracy on the test data) is:\n    \n1. Form a random train/test split\n2. Train the classifier on the training split\n3. Test the classifier on the testing split\n4. Accumulate statistics\n5. Repeat from 1. until enough statistics have been collected.\n\n::: {#46460f1a .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=29}\n``` {.python .cell-code}\nimport sklearn.model_selection as model_selection\n\nnreps = 50\nkvals = range(1, 10)\nacc = []\nnp.random.seed(4)\nfor k in kvals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n                                                                            y, \n                                                                            test_size = 0.5)\n        knn = KNeighborsClassifier(n_neighbors = k)    \n        knn.fit(X_train, y_train)\n        train_rep.append(knn.score(X_train, y_train))\n        test_rep.append(knn.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\naccy = np.array(acc)\n```\n:::\n\n\n::: {#203df36b .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=30}\n``` {.python .cell-code}\nplt.plot(kvals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(kvals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel(r'$k$')\nplt.ylabel('Accuracy')\nplt.title('Train/Test Comparision of $k$-NN')\nplt.legend(loc = 'best');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-31-output-1.png){width=597 height=450}\n:::\n:::\n\n\nBased on the generalization error (ie, accuracy on test (held-out) data), it looks like $k = 2$ is the best choice. \n\nHere is the __decision boundary__ for $k$-NN with $k = 2$.\n\n\n::: {#9dffa72f .cell hide_input='true' tags='[\"hide-input\"]' execution_count=32}\n``` {.python .cell-code}\nnp.random.seed(1)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.5)\n\nk = 2\nknn = KNeighborsClassifier(n_neighbors = k)  \nknn.fit(X_train, y_train)\ny_pred_train = knn.predict(X_train)\ny_pred_test = knn.predict(X_test)\n\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize = fig_size)\nplt.subplot(1, 2, 1)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'{k}-NN - Training Data\\nAccuracy: {knn.score(X_train, y_train)}');\n\nplt.subplot(1, 2, 2)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'{k}-NN - Test Data\\nAccuracy: {knn.score(X_test, y_test)}');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-33-output-1.png){width=912 height=427}\n:::\n:::\n\n\n## Decision Tree\n\nNext, we'll use a decision tree on the same data set.\n\n::: {#aef8b36a .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=33}\n``` {.python .cell-code}\nimport sklearn.tree as tree\ndtc = tree.DecisionTreeClassifier(max_leaf_nodes = 5)\n\ndtc.fit(X_train,y_train)\ny_pred_test = dtc.predict(X_test)\nprint('DT accuracy on test data: ', dtc.score(X_test, y_test))\ny_pred_train = dtc.predict(X_train)\nprint('DT accuracy on training data: ', dtc.score(X_train, y_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDT accuracy on test data:  0.94\nDT accuracy on training data:  0.98\n```\n:::\n:::\n\n\n::: {#a634d452 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=34}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 6))\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, marker='^', s=80)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=80)\nplt.axis('equal')\nplt.axis('off')\nplt.title(F'Decision Tree\\n Triangles: Test Data, Circles: Training Data\\nAccuracy: {dtc.score(X_test, y_test)}');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-35-output-1.png){width=466 height=519}\n:::\n:::\n\n\nLet's visualize the __decision boundary__ of the Decision Tree.\n\n::: {#54525c1a .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=35}\n``` {.python .cell-code}\nZ = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize = (12, 6))\nplt.subplot(1, 2, 1)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'Decision Tree - Training Data\\nAccuracy: {dtc.score(X_train, y_train)}');\n\nplt.subplot(1, 2, 2)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, marker = '^', s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'Decision Tree - Test Data\\nAccuracy: {dtc.score(X_test, y_test)}');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-36-output-1.png){width=912 height=501}\n:::\n:::\n\n\n## Comparing $k$-NN and Decision Tree\n\nIt appears that $k$-NN and a Decision Tree have approximately comparable performance on this dataset.\n\nHowever - there is a difference in __interpretability.__\n\nInterpretability is a big deal!  It means the ability to __explain__ why the classifier made the decision it did.\n\nIt can be relatively difficult to __understand__ why $k$-NN is making a specific prediction.  It depends on the data in the neighborhood of the test point.\n\nOn the other hand, the Decision Tree can be easily understood.\n\nWe sometimes use the terms \"black box\" for an uninterpretable classifier like $k$-NN, and \"white box\" for an interpretable classifier like DT.\n\nLet's see an example of the interpretability of the Decision Tree:\n\n::: {#8951694f .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=36}\n``` {.python .cell-code}\ndot_data = tree.export_graphviz(dtc, out_file=None,\n                         feature_names=['X','Y'],\n                         class_names=['Red','Green'],\n                         filled=True, rounded=True,  \n                         special_characters=True) \nimport pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data) \n# graph.write_pdf(\"dt.pdf\") \nImage(graph.create_png())\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n![](15-Classification-II-kNN_files/figure-html/cell-37-output-1.png){}\n:::\n:::\n\n\n## Real Data\n\nTo explore a few more issues, we'll now turn to some famous datasets that have been extensively studied in the past.\n\n### The Iris Dataset\n\nThe Iris dataset is a famous dataset used by Ronald Fisher in a classic 1936 paper on classification.\n\n<center>\n\n<img src=\"figs/R._A._Fisher.png\" alt=\"Figure\" width=\"35%\">\n    \nR. A. Fisher\n\n</center>\n\nBy http://www.swlearning.com/quant/kohler/stat/biographical_sketches/Fisher_3.jpeg, Public Domain, https://commons.wikimedia.org/w/index.php?curid=4233489\n\nQuoting from Wikipedia:\n\n>The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n\n<center>\n    \n<img src=\"figs/Iris_setosa.png\" alt=\"I. setosa\" width=\"200px\" float = \"left\">\n    \nI. setosa\n    \n<img src=\"figs/Iris_versicolor.png\" alt=\"I. versicolor\" width=\"200px\" float = \"left\">\n    \nI. versicolor\n    \n<img src=\"figs/Iris_virginica.png\" alt=\"I. virginica\" width=\"200px\" float = \"left\">\n    \nI. virginica\n    \n</center>\n\n::: {#1c778f9f .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=37}\n``` {.python .cell-code}\niris = datasets.load_iris()\n```\n:::\n\n\n::: {#3e3aea9a .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=38}\n``` {.python .cell-code}\nX = iris.data\ny = iris.target\nynames = iris.target_names\nprint(X.shape, y.shape)\nprint(X[1,:])\nprint(iris.target_names)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(150, 4) (150,)\n[4.9 3.  1.4 0.2]\n['setosa' 'versicolor' 'virginica']\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n```\n:::\n:::\n\n\nFirst, we'll explore setting hyperparameters.\n\nWe start with $k$-NN.\n\nTo set the hyperparameter $k$, we evaluate error on the test set for many train/test splits:\n\n::: {#165006f5 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=39}\n``` {.python .cell-code}\nkvals = range(2, 20)\nnreps = 50\n\nacc = []\nstd = []\nnp.random.seed(0)\nfor k in kvals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X, y, test_size = 0.33)\n        knn = KNeighborsClassifier(n_neighbors = k)    \n        knn.fit(X_train, y_train)\n        train_rep.append(knn.score(X_train, y_train))\n        test_rep.append(knn.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\n    std.append([np.std(np.array(test_rep)), np.std(np.array(train_rep))])\n```\n:::\n\n\n::: {#fdcf7309 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=40}\n``` {.python .cell-code}\naccy = np.array(acc)\nstds = np.array(std)/np.sqrt(nreps)\nplt.plot(kvals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(kvals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.xticks(kvals)\nplt.legend(loc = 'best');\nprint(f'Max Test Accuracy at k = {kvals[np.argmax(accy[:, 0])]} with accuracy {np.max(accy[:, 0]):.03f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMax Test Accuracy at k = 13 with accuracy 0.969\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-41-output-2.png){width=606 height=429}\n:::\n:::\n\n\nNow, it looks llike $k$ = 13 is the best-performing value of the hyperparameter.\n\nCan we be sure?\n\nBe careful!  Each point in the above plot is the mean of 50 random train/test splits!\n\nIf we are going to be __sure__ that $k$ = 13 is best, then it should be be statistically distinguishable from the other values.\n\nTo make this call, let's plot $\\pm 1 \\sigma$ confidence intervals on the mean values.\n\n(See the Probability Refresher for details on the proper formula.)\n\n::: {#c6b98542 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=41}\n``` {.python .cell-code}\nplt.errorbar(kvals, accy[:, 0], stds[:, 0], label = 'Accuracy on Test Data')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'lower center')\nplt.xticks(kvals)\nplt.title(r'Test Accuracy with $\\pm 1\\sigma$ Errorbars');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-42-output-1.png){width=606 height=450}\n:::\n:::\n\n\nIt looks like $k$ = 13 is a reasonable value,\n\nalthough a case can be made that 9 and 11 are not statistically distinguishable from 13.\n\nTo gain insight onto the complexity of the model for $k$ = 13, let's look at the decision boundary.\n\nNote that we will re-run the classifier using only two (of four) features, so we can visualize.\n\n::: {#a7d2b96d .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=42}\n``` {.python .cell-code}\n# Create color maps\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we will use only the first two (of four) features, so we can visualize\nX = X_train[:, :2] \nh = .02  # step size in the mesh\nk = 13\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X, y_train)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=cmap_bold)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(f\"3-Class $k$-NN classification ($k$ = {k})\");\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-43-output-1.png){width=571 height=432}\n:::\n:::\n\n\nThere are a few artifacts, but overall this looks like a reasonably smooth set of decision boundaries.\n\nNow we'll compare to a decision tree.\n\nHow do we control the complexity of a Decision Tree?\n\nThere are a variety of ways (see the `sklearn` documentation) but the simplest one is to control the number of leaf nodes in the tree.  \n\nA small number of leaf nodes is a low-complexity model, and a large number of nodes is a high-complexity model.\n\n::: {#91f39639 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=43}\n``` {.python .cell-code}\nX = iris.data\ny = iris.target\n```\n:::\n\n\n::: {#ecb82ef4 .cell hide_input='false' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=44}\n``` {.python .cell-code}\nleaf_vals = range(3, 20)\nnreps = 50\n\nacc = []\nstd = []\nnp.random.seed(0)\nfor leaf_count in leaf_vals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.10)\n        dtc = tree.DecisionTreeClassifier(max_leaf_nodes = leaf_count)   \n        dtc.fit(X_train, y_train)\n        train_rep.append(dtc.score(X_train, y_train))\n        test_rep.append(dtc.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\n    std.append([np.std(np.array(test_rep)), np.std(np.array(train_rep))])\naccy = np.array(acc)\nstds = np.array(std)/np.sqrt(nreps)\n```\n:::\n\n\n::: {#3d914310 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=45}\n``` {.python .cell-code}\nplt.plot(leaf_vals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(leaf_vals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel('Max Leaf Nodes')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'best')\nplt.xticks(leaf_vals)\nbest_leaf = leaf_vals[np.argmax(accy[:, 0])]\nplt.title(f'Test/Train Error for Decision Tree\\nMax Test Accuracy at {best_leaf} leaf nodes with accuracy {np.max(accy[:, 0]):.03f}');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-46-output-1.png){width=597 height=467}\n:::\n:::\n\n\n::: {#77b0c30d .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=46}\n``` {.python .cell-code}\nplt.errorbar(leaf_vals, accy[:, 0], stds[:, 0], label = 'Accuracy on Test Data')\nplt.xlabel('Max Leaf Nodes')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'lower center')\nplt.xticks(leaf_vals)\nplt.title(r'Test Accuracy with $\\pm 1\\sigma$ Errorbars');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-47-output-1.png){width=606 height=450}\n:::\n:::\n\n\nIt looks like 9 leaf nodes is appropriate, but we would be justified to choose 4 or 13 as well.\n\nAnd now let's visualize the decision boundary for the DT:\n\n::: {#0909e132 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=47}\n``` {.python .cell-code}\n# we will use only the first two (of four) features, so we can visualize\nX = X_train[:, :2] \nh = .02  # step size in the mesh\ndtc = tree.DecisionTreeClassifier(max_leaf_nodes = best_leaf) \ndtc.fit(X, y_train)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\nZ = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=cmap_bold)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(f\"3-Class DT Classification\\n{best_leaf} leaf nodes\");\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-48-output-1.png){width=571 height=449}\n:::\n:::\n\n\n### MNIST dataset\n\nNIST used to be called the \"National Bureau of Standards.\"  These are the folks who bring you the reference meter, reference kilogram, etc.\n\nNIST constructed datasets for machine learning of handwritten digits. These were collected from Census Bureau employees and also from high-school students.\n\nThese data have been used repeatedly for many years to evaluate classifiers.  For a peek at some of the work done with this dataset you can visit http://yann.lecun.com/exdb/mnist/. \n\n::: {#8e878065 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=48}\n``` {.python .cell-code}\nimport sklearn.utils as utils\n\ndigits = datasets.load_digits()\nX, y = utils.shuffle(digits.data, digits.target, random_state = 1)\n\nprint ('Data shape: {}'.format(X.shape))\nprint ('Data labels: {}'.format(y))\nprint ('Unique labels: {}'.format(digits.target_names))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData shape: (1797, 64)\nData labels: [1 5 0 ... 9 1 5]\nUnique labels: [0 1 2 3 4 5 6 7 8 9]\n```\n:::\n:::\n\n\nAn individual item is an $8 \\times 8$ image, encoded as a matrix:\n\n::: {#88414556 .cell slideshow='{\"slide_type\":\"-\"}' execution_count=49}\n``` {.python .cell-code}\ndigits.images[3]\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\narray([[ 0.,  0.,  7., 15., 13.,  1.,  0.,  0.],\n       [ 0.,  8., 13.,  6., 15.,  4.,  0.,  0.],\n       [ 0.,  2.,  1., 13., 13.,  0.,  0.,  0.],\n       [ 0.,  0.,  2., 15., 11.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  1., 12., 12.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  1., 10.,  8.,  0.],\n       [ 0.,  0.,  8.,  4.,  5., 14.,  9.,  0.],\n       [ 0.,  0.,  7., 13., 13.,  9.,  0.,  0.]])\n```\n:::\n:::\n\n\nLet's show the matrix as an image:\n\n::: {#1e9959f3 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=50}\n``` {.python .cell-code}\nplt.gray() \n# plt.rc('axes', grid = False);\nplt.matshow(digits.images[3], cmap = plt.cm.gray_r);\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-51-output-2.png){width=409 height=418}\n:::\n:::\n\n\nIt is easier to visualize if we blur the pixels a little bit.\n\n::: {#25f4c254 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=51}\n``` {.python .cell-code}\nplt.rc('image', cmap = 'binary', interpolation = 'bilinear')\nplt.figure(figsize = (4, 4))\nplt.axis('off')\nplt.imshow(digits.images[3]);\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-52-output-1.png){width=315 height=315}\n:::\n:::\n\n\nHere are some more samples from the dataset:\n\n::: {#0985642a .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=52}\n``` {.python .cell-code}\nfor t in range(4):\n    plt.figure(figsize = (8, 2))\n    for j in range(4):\n        plt.subplot(1, 4, 1 + j)\n        plt.imshow(X[4*t + j].reshape(8, 8))\n        plt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-53-output-1.png){width=614 height=149}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-53-output-2.png){width=614 height=149}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-53-output-3.png){width=614 height=149}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-53-output-4.png){width=614 height=149}\n:::\n:::\n\n\nAlthough this is an 8 $\\times$ 8 image, we can just treat it as a vector of length 64.\n\nTo do model selection, we will again average over many train/test splits.\n\nHowever, recall one issue of $k$-NN: it can be slow on a large training set (why?). \n\nWe may thus decide to limit the size of our testing set to speed up testing.\n\nHow does the train/test split affect results?\n\nLet's consider two cases:  \n1. Train: 90% of data, Test: 10% of data\n2. Train: 67% of data, Test: 33% of data\n\n::: {#d2d3f9b8 .cell slideshow='{\"slide_type\":\"skip\"}' execution_count=53}\n``` {.python .cell-code}\ndef test_knn(kvals, test_fraction, nreps):\n    acc = []\n    std = []\n    np.random.seed(0)\n    #\n    for k in kvals:\n        test_rep = []\n        train_rep = []\n        for i in range(nreps):\n            X_train, X_test, y_train, y_test = model_selection.train_test_split(\n                X, y, test_size = test_fraction)\n            knn = KNeighborsClassifier(n_neighbors = k)    \n            knn.fit(X_train, y_train)\n            test_rep.append(knn.score(X_test, y_test))\n        acc.append(np.mean(np.array(test_rep)))\n        std.append(np.std(np.array(test_rep)))\n    return(np.array(acc), np.array(std)/np.sqrt(nreps))\n\ntest_fraction1 = 0.33\naccy1, stds1 = test_knn(range(2, 20), test_fraction1, 50)\ntest_fraction2 = 0.10\naccy2, stds2 = test_knn(range(2, 20), test_fraction2, 50)\n```\n:::\n\n\n::: {#e7035aa6 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=54}\n``` {.python .cell-code}\nplt.figure(figsize = (7, 5))\nplt.errorbar(kvals, accy1, stds1, \n             label = f'{test_fraction1:.0%} Used for Testing; accuracy {np.max(accy1):.03f}')\nplt.errorbar(kvals, accy2, stds2, \n             label = f'{test_fraction2:.0%} Used for Testing; accuracy {np.max(accy2):.03f}')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'best')\nplt.xticks(kvals)\nbest_k = kvals[np.argmax(accy1)]\nplt.title(f'Test Accuracy with $\\pm 1\\sigma$ Error Bars');\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:11: SyntaxWarning: invalid escape sequence '\\p'\n<>:11: SyntaxWarning: invalid escape sequence '\\p'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_17179/3534144654.py:11: SyntaxWarning: invalid escape sequence '\\p'\n  plt.title(f'Test Accuracy with $\\pm 1\\sigma$ Error Bars');\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-55-output-2.png){width=606 height=450}\n:::\n:::\n\n\nThese plots illustrate important principles:\n    \n* The more data used for training, the better the classifier will tend to perform\n* The less data used for testing, the more variable will be the testing results \n    (but the faster testing will go)\n    \n    \nNote that the key decision here is what value to choose for $k$.   So it makes sense to use the 33% test split, because the smaller error bars give us better confidence in our decision.\n   \n\nWe can get a sense of why $k$-NN can succeed at this task by looking at the nearest neighbors of some points:\n\n::: {#46d57774 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=55}\n``` {.python .cell-code}\nknn = KNeighborsClassifier(n_neighbors = 3)    \nknn.fit(X, y)\nneighbors = knn.kneighbors(X[:3,:], n_neighbors=3, return_distance=False)\n```\n:::\n\n\n::: {#99123859 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=56}\n``` {.python .cell-code}\nplt.rc(\"image\", cmap=\"binary\")  # this sets a black on white colormap\n# plot X_digits_valid[0]\nfor t in range(3):\n    plt.figure(figsize=(8,2))\n    plt.subplot(1, 4, 1)\n    plt.imshow(X[t].reshape(8, 8))\n    plt.axis('off')\n    plt.title(\"Query\")\n    # plot three nearest neighbors from the training set\n    for i in [0, 1, 2]:\n        plt.subplot(1, 4, 2 + i)\n        plt.title(\"neighbor {}\".format(i))\n        plt.imshow(X[neighbors[t, i]].reshape(8, 8))\n        plt.axis('off')\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-57-output-1.png){width=614 height=169}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-57-output-2.png){width=614 height=169}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-html/cell-57-output-3.png){width=614 height=169}\n:::\n:::\n\n\n",
    "supporting": [
      "15-Classification-II-kNN_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}