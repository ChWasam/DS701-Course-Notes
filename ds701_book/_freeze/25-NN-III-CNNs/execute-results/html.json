{
<<<<<<< HEAD
  "hash": "425fbc8bb951f0df360c1f4191a1dad8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NN III -- Stochastic Gradient Descent, Batches and Convolutional Neural Networks'\njupyter: python3\n---\n\n\n\n## Recap\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/25-NN-III-CNNs.ipynb)\n\nWe have covered the following topics\n\n* Gradients, gradient descent, and back propagation\n* Fully connected neural networks (Multi-Layer Perceptron)\n* Training of MLPs using back propagation\n\nNow we cover\n\n* _Stochastic_ gradient descent (SGD)\n* Convolutional Neural Networks (CNNs)\n* Training a CNN with SGD\n\n# Stochastic Gradient Descent\n\n## Batches and Stochastic Gradient Descent\n\n\n* Compute the gradient (e.g., forward pass and backward pass) with only a _random subset_\nof the input data.\n\n> This subset is called a _batch_.\n\n* Work through the dataset by _randomly sampling without replacement_. This is the _stochastic_ part.\n* One forward and backward pass through all the batches of data is called an _epoch_.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches and Stochastic Gradient Descent\n:::\n\nThe squared error loss for (full-batch) gradient descent for $N$ input samples is\n\n$$\nL = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left( y_i - \\hat{y}_i  \\right)^2.\n$$\n\nIn _Stochastic Gradient Descent_, the loss is calculated for a single _batch_ of data, i.e.,\n\n$$\nL_t = \\sum_{i \\in \\mathcal{B}_t} \\ell_i = \\sum_{i \\in \\mathcal{B}_t} \\left( y_i - \\hat{y}_i  \\right)^2,\n$$\n\nwhere $\\mathcal{B}_t$ is the $t$-th batch.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nHere is an example.\n\n::: {#b790450b .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-3-output-1.png){width=790 height=411 fig-align='center'}\n:::\n:::\n\n\nGiven a training data set of 12 points and we want to use a _batch size_ of 3.\n\nThe 12 points are divided into batches of 3 by randomly selecting points without replacement.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nThe points can be resampled again to create a different set of batches.\n\n::: {#7f42636b .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-4-output-1.png){width=790 height=411 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nFor every training iteration, you calculate the loss after a forward and backward pass with the data from a single batch.\n\n::: {#9adcbd5d .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-5-output-1.png){width=641 height=653 fig-align='center'}\n:::\n:::\n\n\n## Vocabulary Summary\n\nWe have introduced the following terms:\n\n- **batch** - a subset of the full training data\n- **batch size** - the number of data points in the batch\n- **iteration** - a forward and backward pass with a single batch of data\n- **epoch** - a forward and backward pass over all the batches of data.\n\nWith 12 instances of data split into 4 batches, the batch size is 3, and it takes 4 iterations for a single epoch.\n\n\n## Advantages of SGD\n\nThere are two main advantages to _Stochastic Gradient Descent_.\n\n1. Avoid reading and computing on every input data sample for every training iteration.\n    * Speeds up the iterations while still making optimization progress.\n    * Works better with limited GPU memory and CPU cache. Avoid slow downs by thrashing limited memory.\n\n2. Improve training convergence by adding _noise_ to the weight updates.\n    * Possibly avoid getting stuck in a local minima.\n\n::: {.content-visible when-profile=\"slides\"}\n## Advantages of SGD\n:::\n\nConsider the following example.\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-GD-vs-SGD.png){width=\"75%\" fig-align=\"center\"}\n\nThis contour plot shows a loss surface for a model with only 2 parameters.\n\nWith full-batch gradient descent, starting points 1 and 3 converge to the\nglobal minimum, but starting point 2 gets stuck in a local minimum.\n\nWith SGD, starting point 1 converges to the global minimum. \nHowever, starting point 2 now avoids the local minimum and converges to\nthe global minimum.\n\n# Load an Image Dataset in Batches in PyTorch\n\n\n\n## DataSet and DataLoader\n\n![](figs/DataSetDataLoader.png)\n\n---\n\n- **`Dataset` Object**:\n  - Abstract class representing a dataset.\n  - Custom datasets are created by subclassing `Dataset` and implementing `__len__` and `__getitem__`.\n\n  - **`DataLoader` Object**:\n  - Provides an iterable over a dataset.\n  - Handles batching, shuffling, and loading data in parallel.\n\n  - **Key Features**:\n    - **Batching**: Efficiently groups data samples into batches.\n    - **Shuffling**: Randomizes the order of data samples.\n    - **Parallel Loading**: Uses multiple workers to load data in parallel, improving performance.\n\n\n## 1. Load and Scale MNIST\n\nLoad MNIST handwritten digit dataset with 60K training samples and 10K test samples.\n\n::: {#f966543e .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\n# Define a transform to scale the pixel values from [0, 255] to [-1, 1]\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\nbatch_size = 64\n\n# Download and load the training data\ntrainset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n                                          shuffle=True)\n\n# Download and load the test data\ntestset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n                                         shuffle=True)\n```\n:::\n\n\n* `torchvision.dataset.MNIST` is a convenience class which inherits from\n  `torch.utils.data.Dataset` (see [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset))\n   that wraps a particular dataset and overwrites a `__getitem__()` method which\n   retrieves a data sample given an \n   index or a key.\n\n* If we give the argument `train=True`, it returns the training set, while the \n  argument `train=False` returns the test set.\n\n* `torch.utils.data.DataLoader()` takes a dataset as in the previous line and\n  returns a python _iterable_ which lets you loop through the data.\n\n* We give `DataLoader` the _batch size_, and it will return a batch of data samples\n  on each iteration.\n\n* By passing `shuffle=True`, we are telling the data loader to shuffle the batches\n  after every epoch.\n\n::: {#1f7c6ed6 .cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"No. of training images: {len(trainset)}\")\nprint(f\"No. of test images: {len(testset)}\")\nprint(\"The dataset classes are:\")\nprint(trainset.classes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo. of training images: 60000\nNo. of test images: 10000\nThe dataset classes are:\n['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 1. Load and Scale MNIST\n:::\n\nWe can see the data loader, `trainloader` in action in the code below to\nget a batch and visualize it along with the labels.\n\nEverytime we rerun the cell we will get a different batch.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#325cf4c9 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-9-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#075692a8 .cell execution_count=10}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [trainset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7 - seven</td>\n      <td>4 - four</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n      <td>2 - two</td>\n      <td>6 - six</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>3 - three</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 - three</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>5 - five</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1 - one</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n      <td>2 - two</td>\n      <td>7 - seven</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>5 - five</td>\n      <td>4 - four</td>\n      <td>1 - one</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3 - three</td>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n# Convolutional Neural Networks\n\n## Problems with Fully-Connected Networks\n\n* Size\n    * 224x224 RGB image = 150,528 dimensions\n    * Hidden layers generally larger than inputs\n    * One hidden layer = $150,520 \\times 150,528\\approx 22$ billion weights\n* Nearby pixels statistically related\n    * Fully connected networks don't exploit spatial correlation\n\n## Convolutional Neural Network (CNN)\n\n- **Definition**:\n  - A type of deep learning model designed for processing structured grid data, such as images.\n  - Utilizes convolutional layers to automatically and adaptively learn spatial hierarchies of features.\n\n--- \n\n- **Key Components**:\n  - **Convolutional Layers**: Apply filters to input data to create feature maps.\n  - **Pooling Layers**: Reduce the dimensionality of feature maps while retaining important information.\n  - **Fully Connected Layers**: Perform classification based on the features extracted by convolutional and pooling layers.\n\n---\n\n- **Advantages**:\n  - **Parameter Sharing**: Reduces the number of parameters, making the network more efficient.\n  - **Translation Invariance**: Recognizes patterns regardless of their position in the input.\n\n## Convolutional Network Applications\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-img-class.svg){width=\"75%\" fig-align=\"center\"}\n\n* Multi-class classification problem ( >2 possible classes)\n* Convolutional network with classification output\n\n::: {.content-visible when-profile=\"slides\"}\n## Convolutional Network Applications\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-obj-det.png){width=\"75%\" fig-align=\"center\"}\n\n* Localize and classify objects in an image\n* Convolutional network with classification _and_ regression output\n\n::: {.content-visible when-profile=\"slides\"}\n## Convolutional Network Applications\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-img-seg.png){width=\"75%\" fig-align=\"center\"}\n\n* Classify each pixel in an image to 2 or more classes\n* Convolutional encoder-decoder network with a classification values for each pixel\n\n\n\n## Classification Invariant to Shift\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-shift-img-class.png){width=\"75%\" fig-align=\"center\"}\n\n* Let's say we want to do classification on these two images.\n* If you look carefully, one image is shifted w.r.t. the other.\n* An FCN would have to learn a new set of weights for each shift.\n\n\n## Image Segmentation Invariant to Shift\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-shift-seg.png){.r-stretch width=\"65%\" fig-align=\"center\"}\n\n* Same thing for image segmentation.\n* An FCN would have to learn a new set of weights for each shift.\n\n:::: {.fragment}\nSolution: Convolutional Neural Networks\n\n* Parameters only look at local data regions\n* Shares parameters across image or signal\n::::\n\n## 1-D Convolution\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n\nIn CNNs, we define a set of weights that are moved across\nthe input data.\n\nHere is an example with 3 weights and input of length 6.\n\nIn Figure (a), we calculate \n\n$$\nz_2 = \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution\n![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n:::\n\nTo calculate $z_3$, we shift the weights over 1 place (figure (b)) and then\nweight and sum the inputs. We can generalize the equation slightly to\n\n$$\nz_i = \\omega_1 x_{i - 1} + \\omega_2 x_i + \\omega_3 x_{i+1}.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Edge Cases\n![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n:::\n\nWhat do we do about $z_1$?\n\nWe calculate $z_1$ by _padding_ our input data. In figure (c), we\nsimply add (pad with) $0$. This allows us to calculate $z_1$.\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Edge Cases\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-conv07.png){width=\"85%\" fig-align=\"center\"}\n\nAlternatively, we can just reduce the size of the output, by only calculating where\nwe have _valid_ input data, as in figure (d).\n\nFor 1-D data, this reduces the output size by 1 at the beginning and end of the\ndata. This means that for a length-3 filter, the size of the output is reduced by 2.\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n\nThere are a few design choices one can make with convolution layers, such as:\n\n1. __filter length__, e.g., size 3 in figures (a) and (b) and 5 in (c).\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n:::\n\n2. __stride__, the shift of the weights to calculate the next output. Common values are\n    1. _stride 1_ as we saw in the previous examples and in figures (c) and (d),\n    2. _stride 2_, which effectively halves the size of the output as in figures (a) and (b).\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n:::\n\n3. __dilation__, the spacing between elements in the filter. There is an example of dilation=2 in the filter  in figure (d)\n\n## 2D Convolution\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Input Image\n$$\n\\begin{bmatrix}\n1 & 2 & 3 & 0 \\\\\n4 & 5 & 6 & 1 \\\\\n7 & 8 & 9 & 2 \\\\\n0 & 1 & 2 & 3\n\\end{bmatrix}\n$$\n:::\n::: {.column width=\"-5%\"}\n### Kernel\n$$\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n$$\n:::\n::::\n\n## Convolution Operation\n\nThe convolution operation involves sliding the kernel over the input image and computing the dot product at each position.\n\n### Computing the Feature Map\n$$\n\\begin{bmatrix}\n(1*1 + 5*(-1)) & (2*1 + 6*(-1)) & (3*1 +  1*(-1)) \\\\\n(4*1 + 8*(-1)) & (5*1 +  9*(-1)) & (6*1 +  2*(-1)) \\\\\n(7*1 + 1*(-1)) & (8*1 + 2*(-1)) & (9*1 + 3*(-1))\n\\end{bmatrix}\n$$\n\n### Feature Map\n$$\n\\begin{bmatrix}\n-4 & -4 & 2 \\\\\n-4 & -4 & 4 \\\\\n6 & 6 & 7\n\\end{bmatrix}\n$$\n\n## Explanation\n\n- The kernel is applied to each 2x2 submatrix of the input image.\n- The resulting values form the feature map, which can highlight patterns of the input image.\n- Given an $n\\times n$ image, $m\\times m$ kernel size, and a stride of 1, the output of the convolution is an $n-m +1 \\times n-m +1$ matrix.\n\n\n## 2D Convolution\n\nFor images and video frames we use a two-dimensional convolution\n(called `conv2d` in PyTorch) which is an extension of the 1-D\nconvolution.\nFrom [cs231n](https://cs231n.github.io/convolutional-networks/).\n\nLet's look at a 2D convolution layer: $7 \\times 7 \\times 3 \\rightarrow 3 \\times 3 \\times 2$\n\n<!-- Image Credit \"https://cs231n.github.io/convolutional-networks/\"-->\n\n<iframe src=\"figs/NN-figs/conv-demo/index.html\" width=\"100%\" height=\"800px\"></iframe>\n\n<!-- ![](figs/NN-figs/L25-conv-2d.png){width=\"75%\" fig-align=\"center\"} -->\n\n## Max Pooling\n\n\n:::: {.columns}\n::: {.column width=\"5-%\"}\n### Input Image\n$$\n\\begin{bmatrix}\n1 & 3 & 2 & 4 \\\\\n5 & 6 & 1 & 2 \\\\\n7 & 8 & 3 & 0 \\\\\n4 & 2 & 1 & 5\n\\end{bmatrix}\n$$\n:::\n::: {.column width=\"5-%\"}\n### Max Pooling Operation\n\n- **Filter size**: $2 \\times 2$\n- **Stride**: 2\n:::\n::::\n\n## Steps\n\n$$\n\\begin{bmatrix}\n{\\color{cyan}1} & {\\color{cyan}3} & {\\color{magenta}2} & {\\color{magenta}4} \\\\\n{\\color{cyan}5} & {\\color{cyan}6} & {\\color{magenta}1} & {\\color{magenta}2} \\\\\n{\\color{orange}7} & {\\color{orange}8} & {\\color{teal}3} & {\\color{teal}0} \\\\\n{\\color{orange}4} & {\\color{orange}2} & {\\color{teal}1} & {\\color{teal}5}\n\\end{bmatrix}\n$$\n\n1. Apply the $2 \\times 2$ filter to the top-left corner of the input image:\n$$\n\\begin{bmatrix}\n{\\color{cyan}1} & {\\color{cyan}3} \\\\\n{\\color{cyan}5} & {\\color{cyan}6}\n\\end{bmatrix}\n$$\nMax value: 6\n\n\n---\n\n$$\n\\begin{bmatrix}\n{\\color{cyan}1} & {\\color{cyan}3} & {\\color{magenta}2} & {\\color{magenta}4} \\\\\n{\\color{cyan}5} & {\\color{cyan}6} & {\\color{magenta}1} & {\\color{magenta}2} \\\\\n{\\color{orange}7} & {\\color{orange}8} & {\\color{teal}3} & {\\color{teal}0} \\\\\n{\\color{orange}4} & {\\color{orange}2} & {\\color{teal}1} & {\\color{teal}5}\n\\end{bmatrix}\n$$\n\n2. Move the filter to the next position (stride 2):\n$$\n\\begin{bmatrix}\n{\\color{magenta}2} & {\\color{magenta}4} \\\\\n{\\color{magenta}1} & {\\color{magenta}2}\n\\end{bmatrix}\n$$\nMax value: 4\n\n---\n\n$$\n\\begin{bmatrix}\n{\\color{cyan}1} & {\\color{cyan}3} & {\\color{magenta}2} & {\\color{magenta}4} \\\\\n{\\color{cyan}5} & {\\color{cyan}6} & {\\color{magenta}1} & {\\color{magenta}2} \\\\\n{\\color{orange}7} & {\\color{orange}8} & {\\color{teal}3} & {\\color{teal}0} \\\\\n{\\color{orange}4} & {\\color{orange}2} & {\\color{teal}1} & {\\color{teal}5}\n\\end{bmatrix}\n$$\n\n3. Move the filter down to the next row:\n$$\n\\begin{bmatrix}\n {\\color{orange}7} & {\\color{orange}8} \\\\ \n {\\color{orange}4} & {\\color{orange}2} \n\\end{bmatrix}\n$$\nMax value: 8\n\n---\n\n$$\n\\begin{bmatrix}\n{\\color{cyan}1} & {\\color{cyan}3} & {\\color{magenta}2} & {\\color{magenta}4} \\\\\n{\\color{cyan}5} & {\\color{cyan}6} & {\\color{magenta}1} & {\\color{magenta}2} \\\\\n{\\color{orange}7} & {\\color{orange}8} & {\\color{teal}3} & {\\color{teal}0} \\\\\n{\\color{orange}4} & {\\color{orange}2} & {\\color{teal}1} & {\\color{teal}5}\n\\end{bmatrix}\n$$\n\n4. Move the filter to the next position (stride 2):\n$$\n\\begin{bmatrix}\n{\\color{teal}3} & {\\color{teal}0} \\\\ \n{\\color{teal}1} & {\\color{teal}5} \n\\end{bmatrix}\n$$\nMax value: 5\n\n\n## Resulting Feature Map\n$$\n\\begin{bmatrix}\n6 & 4 \\\\\n8 & 5\n\\end{bmatrix}\n$$\n\n### Explanation\n\n- Max pooling reduces the dimensionality of the input image by taking the maximum value from each $2 \\times 2$ region.\n- This operation helps to retain the most important features while reducing the computational complexity.\n\n\n## Define a CNN in PyTorch\n\nWe will do the following steps in order:\n\n1. Load and scale the MNIST training and test datasets using\n   ``torchvision`` (already done)\n2. Define a Convolutional Neural Network architecture\n3. Define a loss function\n4. Train the network on the training data\n5. Test the network on the test data\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nDefine and instantiate a CNN for MNIST.\n\n::: {#a3c4f83e .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n# network for MNIST\nimport torch\nfrom torch import nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.fc2(x)\n        output = nn.functional.log_softmax(x, dim=1)\n        return output\n\nnet = Net()\nprint(net)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNet(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\n```\n:::\n:::\n\n\nThe `Conv2d` layer is defined as:\n\n```python\nCLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n                      stride=1, padding_mode='valid', ...)\n```\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nWe can see the layers and shapes of the data as it passes through the network.\n\n\n| Layer   | Kernel Size | Stride | Input Shape | Input Channels | Output Channels | Output Shape |\n| ------- | ----------- | ------ | ----------- | -------------- | --------------- | ------------ |\n| Conv2D/ReLU  | (3x3)       | 1      |  28x28      |    1           |    32           |  26x26       |\n| Conv2D/ReLU  | (3x3)       | 1      |  26x26      |    32          |    64           |  24x24       |\n| Max_pool2d | (2x2)    | 2      |  24x24      |    64          |    64           |  12x12       |\n| Flatten |             |        |  12x12      |    64          |    1            |  9216x1      |\n| FC/ReLU |             |        |  9216x1     |    1           |    1            |  128x1       |\n| FC Linear |           |        |  128x1      |    1           |    1            |  10x1        |\n| Soft Max |            |        |  10x1      |    1           |    1            |  10x1        |\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nHere's a common way to visualize a CNN architecture.\n\n<!-- Image Credit \"https://alexlenail.me/NN-SVG/AlexNet.html\"-->\n\n![](figs/NN-figs/L25-mnist-cnn2.svg){width=\"75%\" fig-align=\"center\"}\n\n[NN-SVG](https://alexlenail.me/NN-SVG/AlexNet.html)\n\n## 3. Define a Loss function and optimizer\n\nWe'll use a Classification Cross-Entropy loss and SGD with momentum.\n\n::: {#9cf401e7 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n:::\n\n\n## Cross Entropy Loss\n\n* Popular loss function for multi-class classification that measures the \n  _dissimilarity_ between the predicted class log probability $\\log(\\hat{y}_i)$ and\n  the true class $y_i$.\n\n$$\n- \\sum_i y_i \\log(\\hat{y}_i).\n$$\n\nSee this\n[link](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) \nfor more information.\n\n## Momentum\n\nMomentum is a useful technique in optimization. It accelerates\ngradients vectors in the right directions, which can lead to faster convergence. \n\nIt is inspired by physical laws of motion. The optimizer uses 'momentum' to push\nover hilly terrains and valleys to find the global minimum.\n\nIn gradient descent, the weight update rule with momentum is given by:\n\n$$ \nm_{t+1} = \\beta m_t + \\eta \\nabla J(w),\n$$\n\n$$\nw_{t+1} = w_t - m_{t+1},\n$$\n\nwhere\n\n* $m_t$ is the momentum (which drives the update at iteration $t$), \n* $\\beta \\in [0, 1)$, typically 0.9, controls the degree to which the gradient is smoothed over time, and \n* $\\eta$ is the learning rate.\n\nSee _Understanding Deep Learning_, Section 6.3 to learn more.\n\n## 4. Train the network\n\n::: {#3cd417f0 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\nprint(f\"[Epoch #, Iteration #] loss\")\n\n# loop over the dataset multiple times\n# change this value to 2\nfor epoch in range(1):  \n    \n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch #, Iteration #] loss\n[1,   100] loss: 0.111\n[1,   200] loss: 0.077\n[1,   300] loss: 0.031\n[1,   400] loss: 0.021\n[1,   500] loss: 0.018\n[1,   600] loss: 0.018\n[1,   700] loss: 0.016\n[1,   800] loss: 0.015\n[1,   900] loss: 0.013\nFinished Training\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nDisplay some of the images from the test set with the ground truth labels.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#bd107c5f .cell execution_count=14}\n``` {.python .cell-code}\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-14-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#5edf0ee1 .cell execution_count=15}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border and smaller font size\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1 - one</td>\n      <td>1 - one</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>1 - one</td>\n      <td>6 - six</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n      <td>7 - seven</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>7 - seven</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>5 - five</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n      <td>0 - zero</td>\n      <td>4 - four</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nLet's run inference (forward pass) on the model to get numeric outputs.\n\n::: {#5ff51b15 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\noutputs = net(images)\n```\n:::\n\n\nGet the index of the element with highest value and print the label \nassociated with that index.\n\n::: {#61015f37 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\n_, predicted = torch.max(outputs, 1)\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nWe can display the predicted labels for the images.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#29f7625a .cell execution_count=18}\n``` {.python .cell-code}\n# print images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-revealjs/cell-18-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#780f78a6 .cell execution_count=19}\n``` {.python .cell-code}\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[predicted[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>6 - six</td>\n      <td>3 - three</td>\n      <td>1 - one</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1 - one</td>\n      <td>1 - one</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>1 - one</td>\n      <td>6 - six</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n      <td>7 - seven</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>7 - seven</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n      <td>0 - zero</td>\n      <td>4 - four</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nEvaluate over the entire test set.\n\n::: {#946d3073 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"false\"}\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the network on the 10000 test images: 91 %\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nEvaluate the performance per class.\n\n::: {#578ef7e5 .cell execution_count=21}\n``` {.python .cell-code}\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in testset.classes}\ntotal_pred = {classname: 0 for classname in testset.classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[testset.classes[label]] += 1\n            total_pred[testset.classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy for class: 0 - zero is 98.3 %\nAccuracy for class: 1 - one is 97.3 %\nAccuracy for class: 2 - two is 88.3 %\nAccuracy for class: 3 - three is 92.4 %\nAccuracy for class: 4 - four is 97.8 %\nAccuracy for class: 5 - five is 82.6 %\nAccuracy for class: 6 - six is 96.1 %\nAccuracy for class: 7 - seven is 87.7 %\nAccuracy for class: 8 - eight is 93.7 %\nAccuracy for class: 9 - nine is 81.3 %\n```\n:::\n:::\n\n\n## To Dig Deeper\n\nTry working with common CNN network architectures. \n\nFor example see [_Understanding Deep Learning_](https://udlbook.github.io/udlbook/)\nsection 10.5 or \n[PyTorch models and pre-trained weights](https://pytorch.org/vision/stable/models.html).\n\n## Recap\n\nWe covered the following topics:\n\n* Convolutional Neural Networks\n* 1-D and 2-D convolutions\n* Common CNN architectures\n* Training a CNN in PyTorch\n\n",
=======
  "hash": "66c7e9d74e3ebcd8b8bf22020dc3eaea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NN III -- Stochastic Gradient Descent, Batches and Convolutional Neural Networks'\njupyter: python3\n---\n\n\n\n## Recap\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/25-NN-III-CNNs.ipynb)\n\nWe have covered the following topics\n\n* Gradients, gradient descent, and back propagation\n* Fully connected neural networks (Multi-Layer Perceptron)\n* Training of MLPs using back propagation\n\nNow we cover\n\n* _Stochastic_ gradient descent (SGD)\n* Convolutional Neural Networks (CNNs)\n* Training a CNN with SGD\n\n# Stochastic Gradient Descent\n\n## Batches and Stochastic Gradient Descent\n\n\n* Compute the gradient (e.g., forward pass and backward pass) with only a _random subset_\nof the input data.\n\n> This subset is called a _batch_.\n\n* Work through the dataset by _randomly sampling without replacement_. This is the _stochastic_ part.\n* One forward and backward pass through all the batches of data is called an _epoch_.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches and Stochastic Gradient Descent\n:::\n\nThe squared error loss for (full-batch) gradient descent for $N$ input samples is\n\n$$\nL = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left( y_i - \\hat{y}_i  \\right)^2.\n$$\n\nIn _Stochastic Gradient Descent_, the loss is calculated for a single _batch_ of data, i.e.,\n\n$$\nL_t = \\sum_{i \\in \\mathcal{B}_t} \\ell_i = \\sum_{i \\in \\mathcal{B}_t} \\left( y_i - \\hat{y}_i  \\right)^2,\n$$\n\nwhere $\\mathcal{B}_t$ is the $t$-th batch.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nHere is an example.\n\n::: {#c0164bd7 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-3-output-1.png){width=566 height=411 fig-align='center'}\n:::\n:::\n\n\nGiven a training data set of 12 points and we want to use a _batch size_ of 3.\n\nThe 12 points are divided into batches of 3 by randomly selecting points without replacement.\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nThe points can be resampled again to create a different set of batches.\n\n::: {#1d8455d1 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-4-output-1.png){width=566 height=411 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Batches\n:::\n\nFor every training iteration, you calculate the loss after a forward and backward pass with the data from a single batch.\n\n::: {#1089e382 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-5-output-1.png){width=641 height=653 fig-align='center'}\n:::\n:::\n\n\n## Vocabulary Summary\n\nWe have introduced the following terms:\n\n- **batch** - a subset of the full training data\n- **batch size** - the number of data points in the batch\n- **iteration** - a forward and backward pass with a single batch of data\n- **epoch** - a forward and backward pass over all the batches of data.\n\nWith 12 instances of data split into 4 batches, the batch size is 3, and it takes 4 iterations for a single epoch.\n\n\n## Advantages of SGD\n\nThere are two main advantages to _Stochastic Gradient Descent_.\n\n1. Avoid reading and computing on every input data sample for every training iteration.\n    * Speeds up the iterations while still making optimization progress.\n    * Works better with limited GPU memory and CPU cache. Avoid slow downs by thrashing limited memory.\n\n2. Improve training convergence by adding _noise_ to the weight updates.\n    * Possibly avoid getting stuck in a local minima.\n\n::: {.content-visible when-profile=\"slides\"}\n## Advantages of SGD\n:::\n\nConsider the following example.\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-GD-vs-SGD.png){width=\"75%\" fig-align=\"center\"}\n\nThis contour plot shows a loss surface for a model with only 2 parameters.\n\nWith full-batch gradient descent, starting points 1 and 3 converge to the\nglobal minimum, but starting point 2 gets stuck in a local minimum.\n\nWith SGD, starting point 1 converges to the global minimum. \nHowever, starting point 2 now avoids the local minimum and converges to\nthe global minimum.\n\n# Load an Image Dataset in Batches in PyTorch\n\n\n\n## DataSet and DataLoader\n\n![](figs/DataSetDataLoader.png)\n\n---\n\n- **`Dataset` Object**:\n  - Abstract class representing a dataset.\n  - Custom datasets are created by subclassing `Dataset` and implementing `__len__` and `__getitem__`.\n\n  - **`DataLoader` Object**:\n  - Provides an iterable over a dataset.\n  - Handles batching, shuffling, and loading data in parallel.\n\n  - **Key Features**:\n    - **Batching**: Efficiently groups data samples into batches.\n    - **Shuffling**: Randomizes the order of data samples.\n    - **Parallel Loading**: Uses multiple workers to load data in parallel, improving performance.\n\n\n## 1. Load and Scale MNIST\n\nLoad MNIST handwritten digit dataset with 60K training samples and 10K test samples.\n\n::: {#f4e13097 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\n# Define a transform to scale the pixel values from [0, 255] to [-1, 1]\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\nbatch_size = 64\n\n# Download and load the training data\ntrainset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n                                          shuffle=True)\n\n# Download and load the test data\ntestset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n                                         shuffle=True)\n```\n:::\n\n\n* `torchvision.dataset.MNIST` is a convenience class which inherits from\n  `torch.utils.data.Dataset` (see [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset))\n   that wraps a particular dataset and overwrites a `__getitem__()` method which\n   retrieves a data sample given an \n   index or a key.\n\n* If we give the argument `train=True`, it returns the training set, while the \n  argument `train=False` returns the test set.\n\n* `torch.utils.data.DataLoader()` takes a dataset as in the previous line and\n  returns a python _iterable_ which lets you loop through the data.\n\n* We give `DataLoader` the _batch size_, and it will return a batch of data samples\n  on each iteration.\n\n* By passing `shuffle=True`, we are telling the data loader to shuffle the batches\n  after every epoch.\n\n::: {#52d54f83 .cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"No. of training images: {len(trainset)}\")\nprint(f\"No. of test images: {len(testset)}\")\nprint(\"The dataset classes are:\")\nprint(trainset.classes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo. of training images: 60000\nNo. of test images: 10000\nThe dataset classes are:\n['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 1. Load and Scale MNIST\n:::\n\nWe can see the data loader, `trainloader` in action in the code below to\nget a batch and visualize it along with the labels.\n\nEverytime we rerun the cell we will get a different batch.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#d5d4ddc7 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-9-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#88aee496 .cell execution_count=10}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [trainset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7 - seven</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7 - seven</td>\n      <td>3 - three</td>\n      <td>1 - one</td>\n      <td>9 - nine</td>\n      <td>7 - seven</td>\n      <td>0 - zero</td>\n      <td>3 - three</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>1 - one</td>\n      <td>4 - four</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>3 - three</td>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>2 - two</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>0 - zero</td>\n      <td>3 - three</td>\n      <td>1 - one</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2 - two</td>\n      <td>2 - two</td>\n      <td>3 - three</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1 - one</td>\n      <td>1 - one</td>\n      <td>6 - six</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n      <td>1 - one</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1 - one</td>\n      <td>3 - three</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>5 - five</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n# Convolutional Neural Networks\n\n## Problems with Fully-Connected Networks\n\n* Size\n    * 224x224 RGB image = 150,528 dimensions\n    * Hidden layers generally larger than inputs\n    * One hidden layer = $150,520\\times 150,528\\approx 22$ billion weights\n* Nearby pixels statistically related\n    * Fully connected networks don't exploit spatial correlation\n\n## Convolutional Neural Network (CNN)\n\n- **Definition**:\n  - A type of deep learning model designed for processing structured grid data, such as images.\n  - Utilizes convolutional layers to automatically and adaptively learn spatial hierarchies of features.\n\n--- \n\n- **Key Components**:\n  - **Convolutional Layers**: Apply filters to input data to create feature maps.\n  - **Pooling Layers**: Reduce the dimensionality of feature maps while retaining important information.\n  - **Fully Connected Layers**: Perform classification based on the features extracted by convolutional and pooling layers.\n\n---\n\n- **Advantages**:\n  - **Parameter Sharing**: Reduces the number of parameters, making the network more efficient.\n  - **Translation Invariance**: Recognizes patterns regardless of their position in the input.\n\n## Convolutional Network Applications\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-img-class.svg){width=\"75%\" fig-align=\"center\"}\n\n* Multi-class classification problem ( >2 possible classes)\n* Convolutional network with classification output\n\n::: {.content-visible when-profile=\"slides\"}\n## Convolutional Network Applications\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-obj-det.png){width=\"75%\" fig-align=\"center\"}\n\n* Localize and classify objects in an image\n* Convolutional network with classification _and_ regression output\n\n::: {.content-visible when-profile=\"slides\"}\n## Convolutional Network Applications\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-img-seg.png){width=\"75%\" fig-align=\"center\"}\n\n* Classify each pixel in an image to 2 or more classes\n* Convolutional encoder-decoder network with a classification values for each pixel\n\n\n\n## Classification Invariant to Shift\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-shift-img-class.png){width=\"75%\" fig-align=\"center\"}\n\n* Let's say we want to do classification on these two images.\n* If you look carefully, one image is shifted w.r.t. the other.\n* An FCN would have to learn a new set of weights for each shift.\n\n\n## Image Segmentation Invariant to Shift\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-shift-seg.png){.r-stretch width=\"65%\" fig-align=\"center\"}\n\n* Same thing for image segmentation.\n* An FCN would have to learn a new set of weights for each shift.\n\n:::: {.fragment}\nSolution: Convolutional Neural Networks\n\n* Parameters only look at local data regions\n* Shares parameters across image or signal\n::::\n\n## 1-D Convolution\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n::: {.content-visible when-profile=\"web\"}\n![](figs/NN-figs/L25-conv07.png){width=\"85%\" fig-align=\"center\"}\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n![](figs/NN-figs/L25-conv04.png){height=\"300px\" fig-align=\"center\"}\n:::\n\nIn CNNs, we define a set of weights that are moved across\nthe input data.\n\nHere is an example with 3 weights and input of length 6.\n\nIn Figure (a), we calculate \n\n$$\nz_2 = \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution\n![](figs/NN-figs/L25-conv05.png){height=\"300px\" fig-align=\"center\"}\n:::\n\n\nTo calculate $z_3$, we shift the weights over 1 place (figure (b)) and then\nweight and sum the inputs. We can generalize the equation slightly to\n\n$$\nz_i = \\omega_1 x_{i - 1} + \\omega_2 x_i + \\omega_3 x_{i+1}.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Edge Cases\n![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n:::\n\nWhat do we do about $z_1$?\n\nWe calculate $z_1$ by _padding_ our input data. In figure (c), we\nsimply add (pad with) $0$. This allows us to calculate $z_1$.\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Edge Cases\n![](figs/NN-figs/L25-conv07.png){width=\"85%\" fig-align=\"center\"}\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n\nAlternatively, we can just reduce the size of the output, by only calculating where\nwe have _valid_ input data, as in figure (d).\n\nFor 1-D data, this reduces the output size by 1 at the beginning and end of the\ndata. This means that for a length-3 filter, the size of the output is reduced by 2.\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n:::\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n\nThere are a few design choices one can make with convolution layers, such as:\n\n1. __filter length__, e.g., size 3 in figures (a) and (b) and 5 in (c).\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n:::\n\n2. __stride__, the shift of the weights to calculate the next output. Common values are\n    1. _stride 1_ as we saw in the previous examples and in figures (c) and (d),\n    2. _stride 2_, which effectively halves the size of the output as in figures (a) and (b).\n\n::: {.content-visible when-profile=\"slides\"}\n## 1-D Convolution -- Parameters\n![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n:::\n\n3. __dilation__, the spacing between elements in the filter. There is an example of dilation=2 in the filter  in figure (d)\n\n## 2D Convolution\n\nFor images and video frames we use a two-dimensional convolution\n(called `conv2d` in PyTorch) which is an extension of the 1-D\nconvolution.\nFrom [cs231n](https://cs231n.github.io/convolutional-networks/).\n\nLet's look at a 2D convolution layer: $7 \\times 7 \\times 3 \\rightarrow 3 \\times 3 \\times 2$\n\n<!-- Image Credit \"https://cs231n.github.io/convolutional-networks/\"-->\n\n<iframe src=\"figs/NN-figs/conv-demo/index.html\" width=\"100%\" height=\"800px\"></iframe>\n\n<!-- ![](figs/NN-figs/L25-conv-2d.png){width=\"75%\" fig-align=\"center\"} -->\n\n\n## Define a CNN in PyTorch\n\nWe will do the following steps in order:\n\n1. Load and scale the MNIST training and test datasets using\n   ``torchvision`` (already done)\n2. Define a Convolutional Neural Network architecture\n3. Define a loss function\n4. Train the network on the training data\n5. Test the network on the test data\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nDefine and instantiate a CNN for MNIST.\n\n::: {#65407095 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n# network for MNIST\nimport torch\nfrom torch import nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.fc2(x)\n        output = nn.functional.log_softmax(x, dim=1)\n        return output\n\nnet = Net()\n```\n:::\n\n\nThe `Conv2d` layer is defined as:\n\n```python\nCLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n                      stride=1, padding_mode='valid', ...)\n```\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nWe can see the layers and shapes of the data as it passes through the network.\n\n\n| Layer   | Kernel Size | Stride | Input Shape | Input Channels | Output Channels | Output Shape |\n| ------- | ----------- | ------ | ----------- | -------------- | --------------- | ------------ |\n| Conv2D/ReLU  | (3x3)       | 1      |  28x28      |    1           |    32           |  26x26       |\n| Conv2D/ReLU  | (3x3)       | 1      |  26x26      |    32          |    64           |  24x24       |\n| Max_pool2d | (2x2)    | 2      |  24x24      |    64          |    64           |  12x12       |\n| Flatten |             |        |  12x12      |    64          |    1            |  9216x1      |\n| FC/ReLU |             |        |  9216x1     |    1           |    1            |  128x1       |\n| FC Linear |           |        |  128x1      |    1           |    1            |  10x1        |\n| Soft Max |            |        |  10x1      |    1           |    1            |  10x1        |\n\n::: {.content-visible when-profile=\"slides\"}\n## Define a CNN in PyTorch\n:::\n\nHere's a common way to visualize a CNN architecture.\n\n<!-- Image Credit \"https://alexlenail.me/NN-SVG/AlexNet.html\"-->\n\n![](figs/NN-figs/L25-mnist-cnn2.svg){width=\"75%\" fig-align=\"center\"}\n\n[NN-SVG](https://alexlenail.me/NN-SVG/AlexNet.html)\n\n## 3. Define a Loss function and optimizer\n\nWe'll use a Classification Cross-Entropy loss and SGD with momentum.\n\n::: {#510d4360 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n:::\n\n\n## Cross Entropy Loss\n\n* Popular loss function for multi-class classification that measures the \n  _dissimilarity_ between the predicted class log probability $\\log(p_i)$ and\n  the true class $y_i$.\n\n$$\n- \\sum_i y_i \\log(p_i).\n$$\n\nSee this\n[link](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) \nfor more information.\n\n## Momentum\n\nMomentum is a useful technique in optimization. It accelerates\ngradients vectors in the right directions, which can lead to faster convergence. \n\nIt is inspired by physical laws of motion. The optimizer uses 'momentum' to push\nover hilly terrains and valleys to find the global minimum.\n\nIn gradient descent, the weight update rule with momentum is given by:\n\n$$ \nm_{t+1} = \\beta m_t + \\eta \\nabla J(w),\n$$\n\n$$\nw_{t+1} = w_t - m_{t+1},\n$$\n\nwhere\n\n* $m_t$ is the momentum (which drives the update at iteration $t$), \n* $\\beta \\in [0, 1)$, typically 0.9, controls the degree to which the gradient is smoothed over time, and \n* $\\eta$ is the learning rate.\n\nSee _Understanding Deep Learning_, Section 6.3 to learn more.\n\n## 4. Train the network\n\n::: {#8b8d5e33 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\nprint(f\"[Epoch #, Iteration #] loss\")\n\n# loop over the dataset multiple times\n# change this value to 2\nfor epoch in range(1):  \n    \n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch #, Iteration #] loss\n[1,   100] loss: 0.108\n[1,   200] loss: 0.060\n[1,   300] loss: 0.026\n[1,   400] loss: 0.023\n[1,   500] loss: 0.020\n[1,   600] loss: 0.018\n[1,   700] loss: 0.017\n[1,   800] loss: 0.016\n[1,   900] loss: 0.015\nFinished Training\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nDisplay some of the images from the test set with the ground truth labels.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#20c8d6cf .cell execution_count=14}\n``` {.python .cell-code}\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-14-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#0bf6d746 .cell execution_count=15}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border and smaller font size\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1 - one</td>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n      <td>2 - two</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>2 - two</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>6 - six</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>2 - two</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>3 - three</td>\n      <td>5 - five</td>\n      <td>3 - three</td>\n      <td>1 - one</td>\n      <td>6 - six</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>3 - three</td>\n      <td>0 - zero</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nLet's run inference (forward pass) on the model to get numeric outputs.\n\n::: {#25e1ffc7 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\noutputs = net(images)\n```\n:::\n\n\nGet the index of the element with highest value and print the label \nassociated with that index.\n\n::: {#054c8cac .cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\n_, predicted = torch.max(outputs, 1)\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nWe can display the predicted labels for the images.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#4f095932 .cell execution_count=18}\n``` {.python .cell-code}\n# print images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-18-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#1fbb80e6 .cell execution_count=19}\n``` {.python .cell-code}\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[predicted[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Add CSS to shrink the size of the table\nhtml = f\"\"\"\n<style>\n    table {{\n        font-size: 14px;\n    }}\n</style>\n{html}\n\"\"\"\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    table {\n        font-size: 14px;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1 - one</td>\n      <td>6 - six</td>\n      <td>8 - eight</td>\n      <td>2 - two</td>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n      <td>2 - two</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8 - eight</td>\n      <td>0 - zero</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>2 - two</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>3 - three</td>\n      <td>2 - two</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>6 - six</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>8 - eight</td>\n      <td>5 - five</td>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>3 - three</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6 - six</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>3 - three</td>\n      <td>5 - five</td>\n      <td>3 - three</td>\n      <td>1 - one</td>\n      <td>6 - six</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>3 - three</td>\n      <td>2 - two</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n:::\n::::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nEvaluate over the entire test set.\n\n::: {#568b6fbf .cell execution_count=20}\n``` {.python .cell-code code-fold=\"false\"}\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the network on the 10000 test images: 92 %\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## 4. Train the network\n:::\n\nEvaluate the performance per class.\n\n::: {#00bd5be5 .cell execution_count=21}\n``` {.python .cell-code}\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in testset.classes}\ntotal_pred = {classname: 0 for classname in testset.classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[testset.classes[label]] += 1\n            total_pred[testset.classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy for class: 0 - zero is 97.9 %\nAccuracy for class: 1 - one is 97.3 %\nAccuracy for class: 2 - two is 91.8 %\nAccuracy for class: 3 - three is 94.1 %\nAccuracy for class: 4 - four is 88.5 %\nAccuracy for class: 5 - five is 84.9 %\nAccuracy for class: 6 - six is 94.1 %\nAccuracy for class: 7 - seven is 94.1 %\nAccuracy for class: 8 - eight is 91.2 %\nAccuracy for class: 9 - nine is 90.3 %\n```\n:::\n:::\n\n\n## To Dig Deeper\n\nTry working with common CNN network architectures. \n\nFor example see [_Understanding Deep Learning_](https://udlbook.github.io/udlbook/)\nsection 10.5 or \n[PyTorch models and pre-trained weights](https://pytorch.org/vision/stable/models.html).\n\n## Recap\n\nWe covered the following topics:\n\n* Convolutional Neural Networks\n* 1-D and 2-D convolutions\n* Common CNN architectures\n* Training a CNN in PyTorch\n\n",
>>>>>>> main
    "supporting": [
      "25-NN-III-CNNs_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}