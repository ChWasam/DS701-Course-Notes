{
  "hash": "c8b20da77704ff46febd223c80076bd6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Time Series Analysis'\njupyter: python3\nbibliography: references.bib\n---\n\n\n\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/26-TimeSeries.ipynb)\n\n\n## Lecture Outline\n\n### Lecture Outline: Time Series Analysis\n\n#### **1. Introduction to Time Series (10 minutes)**\n   - Definition and examples of time series data\n   - Importance and applications in various fields (e.g., finance, economics, meteorology)\n   - Distinction between time series and other types of data\n\n#### **2. Components of Time Series (10 minutes)**\n   - Trend: Long-term movement in the data\n   - Seasonality: Regular pattern repeating over a known, fixed period\n   - Cyclic: Long-term oscillations not of a fixed period\n   - Irregular/Noise: Random variation\n\n#### **3. Time Series Decomposition (10 minutes)**\n   - Additive vs. multiplicative models\n   - Decomposition techniques (e.g., classical decomposition, STL)\n   - Practical example using Python (brief code demonstration)\n\n#### **4. Stationarity and Differencing (10 minutes)**\n   - Definition of stationarity\n   - Importance of stationarity in time series analysis\n   - Techniques to achieve stationarity (e.g., differencing, transformation)\n   - Demonstration of stationarity tests (e.g., ADF test)\n\n#### **5. Time Series Models (15 minutes)**\n   - Autoregressive (AR) models\n   - Moving Average (MA) models\n   - ARIMA (Autoregressive Integrated Moving Average) models\n   - Seasonal ARIMA (SARIMA) models\n   - Brief introduction to advanced models (e.g., GARCH, VAR)\n\n#### **6. Model Evaluation and Forecasting (10 minutes)**\n   - Criteria for model selection (e.g., AIC, BIC)\n   - Cross-validation techniques for time series\n   - Forecasting and confidence intervals\n   - Practical example of forecasting using Python\n\n#### **7. Case Study and Discussion (10 minutes)**\n   - Real-world case study: Application of time series analysis\n   - Group discussion on potential projects or applications\n   - Q&A session\n\n#### **8. Conclusion and Further Reading (5 minutes)**\n   - Summary of key points\n   - Recommended resources for further study (books, online courses, research papers)\n   - Introduction to software tools for time series analysis (e.g., Python libraries, R packages)\n\n# 1. Introduction to Time Series (10 minutes)\n\n## Definition and examples of time series data\n\n::: {#bae7f159 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-2-output-1.png){width=668 height=283}\n:::\n:::\n\n\n- A **time series** is a series of data points or observations recorded at different\n  or regular time intervals, e.g. hourly, daily, monthly, quarterly, yearly, etc.\n- **Time Series Analysis** is the process of analyzing time series data to extract\n  meaningful statistics and other characteristics of the data such as trends,\n  cycles, and seasonal patterns.\n- **Time-Series Forecasting** is the process of using statistical models to predict\n  future values based on past values.\n\n## Applications\n\n\n\n::: {#a83af147 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-4-output-1.png){width=672 height=376}\n:::\n:::\n\n\n- **Finance**: Time series analysis is used for stock price prediction, risk management, and economic forecasting.\n- **Economics**: It helps in understanding economic indicators, GDP growth, and inflation rates.\n- **Meteorology**: Time series data is crucial for weather forecasting, climate change studies, and analyzing seasonal patterns.\n- **Healthcare**: Time series analysis is used for analyzing patient data, monitoring disease outbreaks, and predicting patient outcomes.\n- **Marketing**: Time series analysis is used for analyzing sales data, customer behavior, and predicting future trends.\n- **Manufacturing**: Time series analysis is used for analyzing production data, monitoring equipment performance, and predicting maintenance needs.\n\n## Components of a Time Series\n\n- **Trend**: Long-term movement in the data. \"Are car model sales going up or down?\"\n- **Seasonality**: Regular pattern repeating over a known, fixed period. \"Do sales of ice cream increase during the summer?\"\n- **Cyclic**: Long-term oscillations not of a fixed period. \"Are there long term business cycles in the sales of a product?\"\n- **Irregular/Noise**: Random variation. \"Are there random variations in the daily temperature that are not explained by the trend or seasonality?\"\n\n## Distinction between time series and other types of data\n\n- **Time Series Data**: Data points collected or recorded at specific time intervals. Examples include stock prices, weather data, and sales figures.\n- **Cross-Sectional Data**: Data collected at a single point in time, representing a snapshot. Examples include survey results and census data.\n- **Panel Data**: A combination of time series and cross-sectional data, where multiple subjects are observed over time. Examples include longitudinal studies and repeated measures data.\n\nTODO: Add example of panel data... maybe from Tableau??\n\n\n# Time and Date Manipulation\n\nMany time series data sets are indexed by date or time. The python `datetime`\nlibrary and the `pandas` library provide a powerful set of tools for manipulating\ntime series data.\n\nThe [Time Series](https://wesmckinney.com/book/time-series) chapter of the book\n[Python for Data Analysis, 3rd Ed.](https://wesmckinney.com/book/time-series)\nprovides a good overview of these tools.\n\n::: {#d749c383 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nnow = datetime.now()\nprint(f\"Date and time when this cell was executed: {now}\")\nprint(f\"Year: {now.year}, month: {now.month}, day: {now.day}\")\n\ndelta = now - datetime(2024, 1, 1)\nprint(f\"Since beginning of 2024 till when this cell was run there were {delta.days} days and {delta.seconds} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDate and time when this cell was executed: 2024-11-18 22:25:56.197682\nYear: 2024, month: 11, day: 18\nSince beginning of 2024 till when this cell was run there were 322 days and 80756 seconds.\n```\n:::\n:::\n\n\nYou can also convert between strings and datetime.\n\n::: {#0d60bfee .cell execution_count=5}\n``` {.python .cell-code}\n# string to datetime\ndate_string = \"2024-01-01\"\ndate_object = datetime.strptime(date_string, \"%Y-%m-%d\")\nprint(date_object)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024-01-01 00:00:00\n```\n:::\n:::\n\n\nYou can also format datetime objects as strings.\n\n::: {#dcf215da .cell execution_count=6}\n``` {.python .cell-code}\n# datetime to string\nnow_str = now.strftime(\"%Y-%m-%d\")\nprint(now_str)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024-11-18\n```\n:::\n:::\n\n\nSee Table 11.2 in the [book](https://wesmckinney.com/book/time-series) for a list of formatting codes.\n\nLet's explore some of the pandas time series tools.\n\nCreate a time series with a datetime index.\n\n::: {#9ee42d94 .cell execution_count=7}\n``` {.python .cell-code}\nlonger_ts = pd.Series(np.random.standard_normal(1000),\n                      index=pd.date_range(\"2022-01-01\", periods=1000))\nprint(type(longer_ts))\nlonger_ts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.series.Series'>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n2022-01-01   -0.465048\n2022-01-02    0.558187\n2022-01-03    0.374651\n2022-01-04    1.372832\n2022-01-05    1.845964\n                ...   \n2024-09-22    1.077284\n2024-09-23   -0.157822\n2024-09-24   -0.327039\n2024-09-25    1.535035\n2024-09-26   -0.910202\nFreq: D, Length: 1000, dtype: float64\n```\n:::\n:::\n\n\nWe can access just the samples from 2023 with simply:\n\n::: {#7c805a9e .cell execution_count=8}\n``` {.python .cell-code}\nlonger_ts[\"2023\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n2023-01-01   -1.226897\n2023-01-02    0.432450\n2023-01-03   -0.252300\n2023-01-04    0.362482\n2023-01-05   -0.252721\n                ...   \n2023-12-27    0.703350\n2023-12-28   -0.128298\n2023-12-29   -0.979629\n2023-12-30   -0.366233\n2023-12-31    0.407304\nFreq: D, Length: 365, dtype: float64\n```\n:::\n:::\n\n\nOr the month of September 2023:\n\n::: {#2959ac30 .cell execution_count=9}\n``` {.python .cell-code}\nlonger_ts[\"2023-09\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n2023-09-01    1.739672\n2023-09-02    2.202768\n2023-09-03   -0.345346\n2023-09-04   -0.659202\n2023-09-05    1.484263\n2023-09-06    0.368870\n2023-09-07    1.314203\n2023-09-08   -0.889409\n2023-09-09    0.051595\n2023-09-10   -0.437353\n2023-09-11    1.340871\n2023-09-12    0.395255\n2023-09-13    0.299571\n2023-09-14   -0.661219\n2023-09-15    0.033138\n2023-09-16   -0.365710\n2023-09-17   -1.505680\n2023-09-18   -0.294069\n2023-09-19    1.111559\n2023-09-20    1.885654\n2023-09-21   -0.273504\n2023-09-22    0.916227\n2023-09-23    0.107470\n2023-09-24   -0.488229\n2023-09-25   -1.081169\n2023-09-26   -1.391784\n2023-09-27   -1.340716\n2023-09-28    1.078150\n2023-09-29   -0.809658\n2023-09-30    1.664673\nFreq: D, dtype: float64\n```\n:::\n:::\n\n\nOr slice by date range:\n\n::: {#476c30f9 .cell execution_count=10}\n``` {.python .cell-code}\nlonger_ts[\"2023-03-01\":\"2023-03-10\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n2023-03-01    0.410358\n2023-03-02    0.154026\n2023-03-03   -0.220611\n2023-03-04   -0.600096\n2023-03-05    0.646864\n2023-03-06    1.183607\n2023-03-07    1.713072\n2023-03-08    0.459701\n2023-03-09   -0.787671\n2023-03-10   -0.297903\nFreq: D, dtype: float64\n```\n:::\n:::\n\n\nor:\n\n::: {#676449a2 .cell execution_count=11}\n``` {.python .cell-code}\nlonger_ts[\"2023-09-15\":]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n2023-09-15    0.033138\n2023-09-16   -0.365710\n2023-09-17   -1.505680\n2023-09-18   -0.294069\n2023-09-19    1.111559\n                ...   \n2024-09-22    1.077284\n2024-09-23   -0.157822\n2024-09-24   -0.327039\n2024-09-25    1.535035\n2024-09-26   -0.910202\nFreq: D, Length: 378, dtype: float64\n```\n:::\n:::\n\n\nThere are many more time series tools available that let you do things like:\n\n- Shifting and setting frequencies of date ranges\n- Time zone handling\n- Time series resampling\n- Time series rolling and expanding windows\n\n## Moving Window Functions\n\nLet's dive into the moving window functions.\n\n::: {#1107dff4 .cell execution_count=12}\n``` {.python .cell-code}\nimport pandas as pd\nimport yfinance as yf\n\n# Load 10 years of AAPL stock prices into a dataframe\naapl_data = yf.download('AAPL', start='2012-01-01', end='2022-01-01')\nprint(aapl_data.head())\n\naapl_close_px = aapl_data['Close']\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[*********************100%%**********************]  1 of 1 completed\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2012-01-03  14.621429  14.732143  14.607143  14.686786  12.388998  302220800\n2012-01-04  14.642857  14.810000  14.617143  14.765714  12.455574  260022000\n2012-01-05  14.819643  14.948214  14.738214  14.929643  12.593858  271269600\n2012-01-06  14.991786  15.098214  14.972143  15.085714  12.725513  318292800\n2012-01-09  15.196429  15.276786  15.048214  15.061786  12.705328  394024400\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n:::\n\n\n::: {#8b0aace1 .cell execution_count=13}\n``` {.python .cell-code}\n# Plot the closing prices\nplt = aapl_close_px.plot(label='AAPL')\naapl_close_px.rolling(window=250).mean().plot(label='250d MA')\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-14-output-1.png){width=575 height=401}\n:::\n:::\n\n\n# Visualization\n\n## Time Series Plot\n\nWe're going to use a dataset of air passengers per month from 1949 to 1960.\n\n::: {#17bf1668 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Number of Passengers</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1949-01-01</th>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1949-02-01</th>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>1949-03-01</th>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>1949-04-01</th>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>1949-05-01</th>\n      <td>121</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3b149988 .cell execution_count=15}\n``` {.python .cell-code}\nts = air_passengers['Number of Passengers']\nts.plot(ylabel='Number of Passengers', title='Air Passengers 1949-1960', figsize=(8, 4))\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-16-output-1.png){width=667 height=376}\n:::\n:::\n\n\nClearly there are some trends and seasonality in the data.\n\nOne way to accentuate that is to use a two-sided plot.\n\n::: {#31eb4219 .cell execution_count=16}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nx = air_passengers.index.values\ny1 = air_passengers['Number of Passengers'].values\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nplt.fill_between(x, y1=y1, y2=-y1, color='tab:blue', alpha=0.2)\nplt.ylim(-800, 800)\nplt.title('Air Passengers (Two-Sided View)')\nplt.hlines(y=0, xmin=x[0], xmax=x[-1], color='black', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-17-output-1.png){width=660 height=357}\n:::\n:::\n\n\nSince there is a clear seasonal pattern in the data, we can plot the data by month.\n\n::: {#68b113f6 .cell execution_count=17}\n``` {.python .cell-code}\n# Seasonal plot of air_passengers\nimport seaborn as sns\n\n# Extract month and year from the index\nair_passengers['Month'] = air_passengers.index.month\nair_passengers['Year'] = air_passengers.index.year\n\n# Create a seasonal plot\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=air_passengers, x='Month', y='Number of Passengers', hue='Year', palette='tab10')\nplt.title('Seasonal Plot of Air Passengers')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Month')\nplt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-18-output-1.png){width=786 height=376}\n:::\n:::\n\n\nThe seasonality looks like it is increasing over time, but relatively speaking that\nmight not be the case. We can normalize the data by the first month of each year.\n\n::: {#e3220bdb .cell execution_count=18}\n``` {.python .cell-code}\n# Normalize the number of passengers by the first month of each year\nair_passengers['Normalized_Passengers'] = air_passengers.groupby('Year')['Number of Passengers'].transform(lambda x: x / x.iloc[0])\n\n# Create a seasonal plot with normalized values\nplt.figure(figsize=(8, 4))\nsns.lineplot(data=air_passengers, x='Month', y='Normalized_Passengers', hue='Year', palette='tab10')\nplt.title('Seasonal Plot of Air Passengers (Normalized by First Month of Each Year)')\nplt.ylabel('Normalized Number of Passengers')\nplt.xlabel('Month')\nplt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-19-output-1.png){width=782 height=376}\n:::\n:::\n\n\nNow the seasonality looks more similar across years, but perhaps it is still\nincreasing a bit over time.\n\n::: {#0c4efda4 .cell execution_count=19}\n``` {.python .cell-code}\n# Year-wise box plot of air passengers\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=air_passengers, x='Year', y='Number of Passengers', palette='tab10')\nplt.title('Year-wise Box Plot of Air Passengers')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Year')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_37105/3550910787.py:3: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=air_passengers, x='Year', y='Number of Passengers', palette='tab10')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-20-output-2.png){width=667 height=376}\n:::\n:::\n\n\nYou can see the trends, median, and interquartile range of the data by year.\n\n::: {#b10cc0f6 .cell execution_count=20}\n``` {.python .cell-code}\n# Draw seasonal subseries plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a subseries plot for each month\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=air_passengers, x='Month', y='Number of Passengers', palette='tab10')\nplt.title('Seasonal Subseries Plot of Air Passengers')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Month')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_37105/4047514819.py:7: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=air_passengers, x='Month', y='Number of Passengers', palette='tab10')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-21-output-2.png){width=667 height=376}\n:::\n:::\n\n\n::: {#ebf42714 .cell execution_count=21}\n``` {.python .cell-code}\n# Create seasonal subseries plot of monthly passengers\nfig, axes = plt.subplots(1, 12, figsize=(15, 10), sharey=True)\nfig.suptitle('Seasonal Subseries Plot of Monthly Air Passengers')\n\n# Iterate over each month and create a subplot\nfor i, ax in enumerate(axes.flatten(), start=1):\n    monthly_data = air_passengers[air_passengers['Month'] == i]\n    ax.plot(monthly_data['Year'], monthly_data['Number of Passengers'], marker='o')\n    ax.set_title(f'Month: {i}')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Number of Passengers')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-22-output-1.png){width=1431 height=917}\n:::\n:::\n\n\n::: {#c3d621cb .cell execution_count=22}\n``` {.python .cell-code}\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n# Draw the autocorrelation plot of air passengers\nplt.figure(figsize=(8, 4))\nplot_acf(air_passengers['Number of Passengers'], lags=48, )\nplt.title('Autocorrelation Plot of Air Passengers')\nplt.xlabel('Lags')\nplt.ylabel('Autocorrelation')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x384 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-23-output-2.png){width=608 height=449}\n:::\n:::\n\n\nThe blue shaded area is the 95% confidence interval. The autocorrelation is\nsignificantly different when it is outside the confidence interval. Although we can see peaks at lags 12, 24, 36, and 48, they are not statistically significant.\n\n# 2. Components of Time Series (10 minutes)\n\nLet's now try to decompose the time series into its components.\n\n## Trend: Long-term movement in the data\n\n- **Trend**: Represents the long-term progression of the series. It can be increasing, decreasing, or constant over time.\n- **Examples**: \n  - An upward trend in stock prices over several years.\n  - A downward trend in the sales of a product as it becomes obsolete.\n\n## Seasonality: Regular pattern repeating over a known, fixed period\n\n- **Seasonality**: Refers to regular, predictable changes that recur every calendar year. It is a pattern that repeats over a known, fixed period.\n- **Examples**: \n  - Increased retail sales during the holiday season.\n  - Higher ice cream sales during the summer months.\n  - Regular fluctuations in electricity consumption due to seasonal temperature changes.\n\n## Cyclic: Long-term oscillations not of a fixed period\n\n- **Cyclic**: Refers to long-term oscillations or fluctuations in the data that are not of a fixed period. These cycles can vary in length and are often influenced by economic or business conditions.\n- **Examples**: \n  - Business cycles with periods of expansion and contraction.\n  - Agricultural cycles influenced by factors such as weather and market conditions.\n\n## Irregular/Noise: Random variation\n\n- **Irregular/Noise**: Refers to the random variation in the data that cannot be attributed to trend, seasonality, or cyclic patterns. It is the residual variation after accounting for other components.\n- **Examples**: \n  - Sudden spikes or drops in stock prices due to unexpected news.\n  - Random fluctuations in daily temperature readings.\n  - Unpredictable changes in sales figures due to unforeseen events.\n\n# 3. Time Series Decomposition (10 minutes)\n\n## Additive vs. multiplicative models\n\n- **Additive Model**: Assumes that the components of the time series (trend, seasonality, and noise) are added together. The model can be represented as:\n  - $Y(t) = T(t) + S(t) + e(t)$\n  - Where $Y(t)$ is the observed value, $T(t)$ is the trend component, $S(t)$ is the seasonal component, and $e(t)$ is the noise or error term.\n- **Multiplicative Model**: Assumes that the components of the time series are multiplied together. The model can be represented as:\n  - $Y(t) = T(t) \\times S(t) \\times e(t)$\n  - Where $Y(t)$ is the observed value, $T(t)$ is the trend component, $S(t)$ is the seasonal component, and $e(t)$ is the noise or error term.\n- **Examples**:\n  - Additive: Monthly sales data where the seasonal effect is constant over time.\n  - Multiplicative: Monthly sales data where the seasonal effect increases or decreases proportionally with the trend.\n\n## Decomposition techniques (e.g., classical decomposition, STL)\n\n- **Classical Decomposition**: This technique involves breaking down a time series into its trend, seasonal, and irregular components. It can be applied using either an additive or multiplicative model.\n  - **Steps**:\n    1. Estimate the trend component by applying a moving average.\n    2. Remove the trend component to get the detrended series.\n    3. Estimate the seasonal component from the detrended series.\n    4. Remove the seasonal component to get the irregular component.\n\nLet's take each step by step. First we'll estimate the trend component by\napplying a moving average.\n\n::: {#42b47e1b .cell execution_count=23}\n``` {.python .cell-code}\nts.plot(figsize=(8, 4), label='Monthly')\nts.rolling(window=12, center=True).mean().plot(label='12m MA')\nplt.ylabel('Number of Passengers')\nplt.title('Air Passengers 1949-1960')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-24-output-1.png){width=667 height=376}\n:::\n:::\n\n\nNow we can remove the trend component to get the detrended series.\n\n::: {#03c1408f .cell execution_count=24}\n``` {.python .cell-code}\ndetrended_ts = ts - ts.rolling(window=12, center=True).mean()\ndetrended_ts.plot(figsize=(8, 4), label='Detrended')\nplt.ylabel('Number of Passengers')\nplt.title('Air Passengers 1949-1960')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-25-output-1.png){width=670 height=376}\n:::\n:::\n\n\nNow we can estimate the seasonal component by taking the mean of the detrended\nseries for each month.\n\n::: {#89405cb0 .cell execution_count=25}\n``` {.python .cell-code}\nseasonal_ts = detrended_ts.groupby(detrended_ts.index.month).mean()\nseasonal_ts.plot(figsize=(8, 4), label='Seasonal')\nplt.ylabel('Number of Passengers')\nplt.title('Air Passengers 1949-1960')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-26-output-1.png){width=670 height=376}\n:::\n:::\n\n\n::: {#e76d16f8 .cell execution_count=26}\n``` {.python .cell-code}\n# Box plot of the detrended series for each month\nimport seaborn as sns\n\n# Create a DataFrame with the detrended series and the corresponding month\ndetrended_df = pd.DataFrame({'Detrended': detrended_ts, 'Month': detrended_ts.index.month})\n\n# Plot the box plot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Month', y='Detrended', data=detrended_df)\nplt.xlabel('Month')\nplt.ylabel('Detrended Number of Passengers')\nplt.title('Box Plot of Detrended Air Passengers by Month')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-27-output-1.png){width=819 height=523}\n:::\n:::\n\n\n## STL (Seasonal and Trend decomposition using Loess)\n- **STL (Seasonal and Trend decomposition using Loess)**: This is a more flexible and robust method for decomposing time series data. It uses locally weighted regression (Loess) to estimate the trend and seasonal components.\n  - **Advantages**:\n    - Handles any type of seasonality (e.g., weekly, monthly).\n    - Can deal with missing values and outliers.\n    - Provides smooth and adaptable trend and seasonal components.\n\n## Practical example using Python (brief code demonstration)\n\n- **Practical Example**: Let's demonstrate a practical example of time series decomposition using Python.\n- **Steps**:\n  1. Import necessary libraries.\n  2. Load a sample time series dataset.\n  3. Apply a decomposition technique (e.g., classical decomposition or STL).\n  4. Visualize the decomposed components (trend, seasonal, and residual).\n\n- **Code**:\n\n::: {#716a5969 .cell execution_count=27}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Load a sample time series dataset\ndata = pd.read_csv('air_passengers_1949_1960.csv', index_col='Date', parse_dates=True)\nts = data['Number of Passengers']\n```\n:::\n\n\n```{.python}\nts.plot(figsize=(8, 4))\nplt.ylabel('Number of Passengers')\nplt.title('Air Passengers 1949-1960')\nplt.show()\n```\n\n::: {#730583be .cell execution_count=28}\n``` {.python .cell-code}\n# Apply classical decomposition\ndecomposition = seasonal_decompose(ts, model='additive')\n\n# Plot the decomposed components\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(8, 8))\ndecomposition.observed.plot(ax=ax1)\nax1.set_ylabel('Observed')\nax1.set_title('Air Passengers 1949-1960')\ndecomposition.trend.plot(ax=ax2)\nax2.set_ylabel('Trend')\ndecomposition.seasonal.plot(ax=ax3)\nax3.set_ylabel('Seasonal')\ndecomposition.resid.plot(ax=ax4)\nax4.set_ylabel('Residual')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-29-output-1.png){width=758 height=758}\n:::\n:::\n\n\nLet's try the multiplicative model:\n\n::: {#665bfeee .cell execution_count=29}\n``` {.python .cell-code}\n# Apply classical decomposition\ndecomposition = seasonal_decompose(ts, model='multiplicative')\n\n# Plot the decomposed components\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8))\ndecomposition.observed.plot(ax=ax1)\nax1.set_ylabel('Observed')\ndecomposition.trend.plot(ax=ax2)\nax2.set_ylabel('Trend')\ndecomposition.seasonal.plot(ax=ax3)\nax3.set_ylabel('Seasonal')\ndecomposition.resid.plot(ax=ax4)\nax4.set_ylabel('Residual')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-30-output-1.png){width=1142 height=758}\n:::\n:::\n\n\nThe STL decomposition is more flexible and can handle the seasonality in the data\nbetter.\n\nLet's try the STL decomposition:\n\n::: {#769e0a41 .cell execution_count=30}\n``` {.python .cell-code}\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(ts, period=12, robust=True)\nresult = stl.fit()\n# result.plot()\n\n# Plot the decomposed components\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(8, 6))\nresult.observed.plot(ax=ax1)\nax1.set_ylabel('Observed')\nax1.set_title('Air Passengers 1949-1960 (STL)')\nresult.trend.plot(ax=ax2)\nax2.set_ylabel('Trend')\nresult.seasonal.plot(ax=ax3)\nax3.set_ylabel('Seasonal')\nresult.resid.plot(ax=ax4)\nax4.set_ylabel('Residual')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-31-output-1.png){width=758 height=566}\n:::\n:::\n\n\nFrom [@hyndman2021forecasting]\n\nSTL is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while Loess is a method for estimating nonlinear relationships. The STL method was developed by [@cleveland1990stl].\n\nSTL has several advantages over the classical, SEATS and X11 decomposition methods:\n\n* Unlike SEATS and X11, STL will handle any type of seasonality, not only monthly and quarterly data.\n* The seasonal component is allowed to change over time, and the rate of change can be controlled by the user.\n* The smoothness of the trend-cycle can also be controlled by the user.\n* It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component.\n\nOn the other hand, STL has some disadvantages. In particular, it does not handle trading day or calendar variation automatically, and it only provides facilities for additive decompositions.\n\n# 4. Stationarity and Differencing (10 minutes)\n\n## Definition of stationarity\n\n- **Definition of Stationarity**: A time series is said to be stationary if its statistical properties such as mean, variance, and autocorrelation are constant over time.\n- **Types of Stationarity**:\n  1. **Strict Stationarity**: The entire distribution of the time series remains unchanged over time.\n  2. **Weak Stationarity**: Only the first two moments (mean and variance) are constant over time, and the covariance between two time points depends only on the time lag between them.\n\n## Importance of stationarity in time series analysis\n\n- **Importance of Stationarity**: Stationarity is crucial in time series analysis because many statistical methods and models assume that the time series is stationary. Non-stationary data can lead to misleading results and poor forecasts.\n- **Why Stationarity Matters**:\n  1. **Model Assumptions**: Many time series models, such as ARIMA, require the data to be stationary to make accurate predictions.\n  2. **Statistical Properties**: Stationary time series have consistent statistical properties over time, making it easier to analyze and interpret.\n  3. **Forecasting**: Stationary data improves the reliability and accuracy of forecasts.\n\n## Techniques to achieve stationarity (e.g., differencing, transformation)\n\n- **Techniques to Achieve Stationarity**:\n  1. **Differencing**: Subtracting the previous observation from the current observation to remove trends and seasonality.\n  2. **Transformation**: Applying mathematical transformations such as logarithms or square roots to stabilize the variance.\n  3. **Decomposition**: Separating the time series into trend, seasonal, and residual components to analyze and adjust each part individually.\n  4. **Smoothing**: Using techniques like moving averages to smooth out short-term fluctuations and highlight longer-term trends.\n\n## Demonstration of stationarity tests (e.g., ADF test)\n\n- **ADF Test (Augmented Dickey-Fuller Test)**: A statistical test used to determine if a time series is stationary. It tests the null hypothesis that a unit root is present in the time series.\n- **KPSS Test (Kwiatkowski-Phillips-Schmidt-Shin Test)**: Another test for stationarity that tests the null hypothesis that the time series is stationary around a deterministic trend.\n- **PP Test (Phillips-Perron Test)**: Similar to the ADF test but uses a non-parametric method to account for serial correlation in the error terms.\n\n# 5. Time Series Models (15 minutes)\n\n## Autoregressive (AR) models\n\n- **Autoregressive (AR) Models**: A type of time series model where the current\n  value of the series is based on its previous values. \n  \n  The model is defined as:\n\n  $$\n  y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t\n  $$\n\nwhere:\n\n  - $y_t$ is the current value,\n  - $c$ is a constant,\n  - $\\phi_1, \\phi_2, \\ldots, \\phi_p$ are the coefficients,\n  - $\\epsilon_t$ is the white noise error term.\n\n**Key Points**:\n\n  1. **Lag Order (p)**: The number of lagged observations included in the model.\n  2. **Stationarity**: The series should be stationary for the AR model to be effective.\n  3. **Parameter Estimation**: Methods like Yule-Walker equations or Maximum Likelihood Estimation (MLE) are used to estimate the parameters.\n\n## Moving Average (MA) models\n\n- **Moving Average (MA) Models**: A type of time series model where the current value of the series is based on past forecast errors. The model is defined as:\n\n$$\ny_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\n$$\n\nwhere:\n\n  - $y_t$ is the current value,\n  - $\\mu$ is the mean of the series,\n  - $\\epsilon_t$ is the white noise error term,\n  - $\\theta_1, \\theta_2, \\ldots, \\theta_q$ are the coefficients.\n\n**Key Points**:\n\n  1. **Lag Order (q)**: The number of lagged forecast errors included in the model.\n  2. **Stationarity**: The series should be stationary for the MA model to be effective.\n  3. **Parameter Estimation**: Methods like Maximum Likelihood Estimation (MLE) are used to estimate the parameters.\n\n## ARIMA (Autoregressive Integrated Moving Average) models\n\n- **ARIMA Models**: A type of time series model that combines Autoregressive (AR)\n  and Moving Average (MA) models with differencing to make the series stationary.\n  The model is defined as:\n\n$$\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\n$$\n\nwhere:\n\n  - $y_t$ is the current value,\n  - $c$ is a constant,\n  - $\\phi_1, \\phi_2, \\ldots, \\phi_p$ are the AR coefficients,\n  - $\\epsilon_t$ is the white noise error term,\n  - $\\theta_1, \\theta_2, \\ldots, \\theta_q$ are the MA coefficients.\n\n**Key Points**:\n\n  1. **Order (p, d, q)**: The parameters of the ARIMA model where $p$ is the number of lag observations, $d$ is the degree of differencing, and $q$ is the size of the moving average window.\n  2. **Stationarity**: Differencing is used to make the series stationary.\n  3. **Parameter Estimation**: Methods like Maximum Likelihood Estimation (MLE) are used to estimate the parameters.\n\n\n## Seasonal ARIMA (SARIMA) models\n\n- **Seasonal ARIMA (SARIMA) Models**: A type of time series model that extends ARIMA to support seasonality. The model is defined as:\n\n$$\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q} + \\Phi_1 Y_{t-s} + \\Phi_2 Y_{t-2s} + \\ldots + \\Phi_P Y_{t-Ps} + \\Theta_1 E_{t-s} + \\Theta_2 E_{t-2s} + \\ldots + \\Theta_Q E_{t-Qs}\n$$\n\nwhere:\n\n  - $y_t$ is the current value,\n  - $c$ is a constant,\n  - $\\phi_1, \\phi_2, \\ldots, \\phi_p$ are the AR coefficients,\n  - $\\epsilon_t$ is the white noise error term,\n  - $\\theta_1, \\theta_2, \\ldots, \\theta_q$ are the MA coefficients,\n  - $\\Phi_1, \\Phi_2, \\ldots, \\Phi_P$ are the seasonal AR coefficients,\n  - $Y_{t-s}, Y_{t-2s}, \\ldots, Y_{t-Ps}$ are the seasonal lagged observations,\n  - $\\Theta_1, \\Theta_2, \\ldots, \\Theta_Q$ are the seasonal MA coefficients,\n  - $E_{t-s}, E_{t-2s}, \\ldots, E_{t-Qs}$ are the seasonal lagged forecast errors.\n\n**Key Points**:\n\n  1. **Order (p, d, q) x (P, D, Q, s)**: The parameters of the SARIMA model where $p, d, q$ are the non-seasonal parameters, $P, D, Q$ are the seasonal parameters, and $s$ is the length of the seasonal cycle.\n  2. **Stationarity**: Differencing is used to make the series stationary.\n  3. **Parameter Estimation**: Methods like Maximum Likelihood Estimation (MLE) are used to estimate the parameters.\n\n## Brief introduction to advanced models (e.g., GARCH, VAR)\n\n**GARCH (Generalized Autoregressive Conditional Heteroskedasticity) Models**: Used to model time series data with changing volatility over time. The model is defined as:\n\n$$\n\\sigma_t^2 = \\alpha_0 + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2\n$$\n\nwhere:\n\n  - $\\sigma_t^2$ is the conditional variance,\n  - $\\alpha_0, \\alpha_1, \\beta_1$ are coefficients,\n  - $\\epsilon_{t-1}$ is the lagged error term.\n\n- **VAR (Vector Autoregression) Models**: Used for multivariate time series data. The model is defined as:\n\n$$\ny_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \\ldots + A_p y_{t-p} + \\epsilon_t\n$$\n\n  where:\n\n  - $y_t$ is a vector of time series variables,\n  - $c$ is a vector of constants,\n  - $A_1, A_2, \\ldots, A_p$ are coefficient matrices,\n  - $\\epsilon_t$ is a vector of white noise error terms.\n\n**Key Points**:\n\n1. **GARCH Models**: Useful for modeling time series data with time-varying volatility.\n2. **VAR Models**: Useful for capturing the linear interdependencies among multiple time series.\n\n# 6. Model Evaluation and Forecasting (10 minutes)\n\n## Criteria for model selection (e.g., AIC, BIC)\n\n- **AIC (Akaike Information Criterion)**: A measure of the relative quality of a\n  statistical model for a given set of data. It is defined as:\n\n$$\n\\text{AIC} = 2k - 2\\ln(L)\n$$\n\n  where:\n\n  - $k$ is the number of parameters in the model,\n  - $L$ is the maximum value of the likelihood function for the model.\n\n**BIC (Bayesian Information Criterion)**: Similar to AIC but includes a penalty term for the number of parameters in the model. It is defined as:\n\n$$\n\\text{BIC} = k\\ln(n) - 2\\ln(L)\n$$\n\n  where:\n\n  - $k$ is the number of parameters in the model,\n  - $n$ is the number of data points,\n  - $L$ is the maximum value of the likelihood function for the model.\n\n**Key Points**:\n\n  1. **Model Comparison**: Both AIC and BIC are used to compare different models; the model with the lower AIC or BIC is preferred.\n  2. **Penalty for Complexity**: BIC imposes a larger penalty for models with more parameters compared to AIC.\n  3. **Trade-off**: There is a trade-off between goodness of fit and model complexity.\n\n## Cross-validation techniques for time series\n\n- **Time Series Cross-Validation**: Unlike traditional cross-validation, time series cross-validation respects the temporal order of the data. Common techniques include:\n  - **Rolling Forecast Origin**: The training set is expanded with each iteration, and the model is re-evaluated.\n  - **Time Series Split**: The data is split into multiple training and test sets, ensuring that the training set always precedes the test set.\n  - **Blocked Cross-Validation**: The data is divided into blocks, and each block is used as a test set while the preceding blocks are used for training.\n\n## Key Points:\n1. **Respect Temporal Order**: Ensure that the training set always precedes the test set to avoid data leakage.\n2. **Multiple Techniques**: Various techniques like rolling forecast origin, time series split, and blocked cross-validation can be used.\n\n## Forecasting and confidence intervals\n\n- **Forecasting**: The process of making predictions about future values based on historical data. Common methods include:\n  - **ARIMA (AutoRegressive Integrated Moving Average)**: A popular time series forecasting method that combines autoregression, differencing, and moving average components.\n  - **Exponential Smoothing**: A technique that applies decreasing weights to past observations, giving more importance to recent data.\n  - **Prophet**: A forecasting tool developed by Facebook that handles seasonality, holidays, and missing data.\n- **Confidence Intervals**: A range of values that is likely to contain the true value of the forecast. It provides an estimate of the uncertainty associated with the forecast.\n  - **Calculation**: Confidence intervals are typically calculated using the standard error of the forecast and a critical value from the t-distribution or normal distribution.\n  - **Interpretation**: A 95% confidence interval means that there is a 95% chance that the true value will fall within the interval.\n  \n## Key Points:\n\n1. **Forecasting Methods**: Various methods like ARIMA, exponential smoothing, and Prophet can be used for forecasting.\n2. **Uncertainty Estimation**: Confidence intervals provide an estimate of the uncertainty associated with the forecast.\n\n## Practical example of forecasting using Python\n\n- **Practical Example**: Let's walk through a practical example of forecasting using Python.\n  - **Data Preparation**: Load and preprocess the time series data.\n  - **Model Selection**: Choose an appropriate forecasting model (e.g., ARIMA, Prophet).\n  - **Model Training**: Train the model on the historical data.\n  - **Forecasting**: Generate forecasts for future values.\n  - **Evaluation**: Assess the accuracy of the forecasts using appropriate metrics.\n  - **Visualization**: Plot the historical data, forecasts, and confidence intervals.\n\n# 7. Case Study and Discussion (10 minutes)\n\n## Real-world case study: Application of time series analysis\n\n- **Case Study**: Let's explore a real-world case study where time series analysis is applied.\n  - **Industry**: Choose an industry (e.g., finance, healthcare, retail).\n  - **Problem Statement**: Define the problem that needs to be addressed using time series analysis.\n  - **Data Collection**: Describe the data collection process and the type of data used.\n  - **Model Selection**: Select appropriate time series models for the analysis.\n  - **Analysis**: Perform the time series analysis and interpret the results.\n  - **Outcome**: Discuss the outcomes and how the analysis helped in decision-making.\n  \n## Group discussion on potential projects or applications\n\n- **Group Discussion**: Let's engage in a group discussion to brainstorm potential projects or applications of time series analysis.\n  - **Project Ideas**: Share and discuss various project ideas that can benefit from time series analysis.\n  - **Application Areas**: Identify different application areas such as finance, healthcare, retail, and more.\n  - **Challenges**: Discuss the potential challenges and limitations of applying time series analysis in these projects.\n  - **Collaboration**: Explore opportunities for collaboration and knowledge sharing within the group.\n\n\n## Q&A session\n\n# 8. Conclusion and Further Reading (5 minutes)\n\n## Summary of key points\n\n## Recommended resources for further study (books, online courses, research papers)\n\n- **Books**: \n  - \"Time Series Analysis and Its Applications\" by Robert H. Shumway and David S. Stoffer\n  - \"Forecasting: Principles and Practice\" by Rob J Hyndman and George Athanasopoulos\n- **Online Courses**: \n  - \"Time Series Analysis and Forecasting\" on Coursera\n  - \"Practical Time Series Analysis\" on Udacity\n- **Research Papers**: \n  - \"A comprehensive review on time series forecasting using deep learning\" by Wen et al.\n  - \"An Empirical Comparison of Machine Learning Models for Time Series Forecasting\" by Makridakis et al.\n\n## Introduction to software tools for time series analysis (e.g., Python libraries, R packages)\n\n- **Python Libraries**: \n  - **Pandas**: For data manipulation and analysis.\n  - **NumPy**: For numerical computations.\n  - **Statsmodels**: For statistical modeling and time series analysis.\n  - **scikit-learn**: For machine learning and predictive modeling.\n  - **TensorFlow/Keras**: For deep learning models.\n  - **Prophet**: For forecasting time series data. [web](https://facebook.github.io/prophet/) [@taylor2017forecasting]\n- **R Packages**: \n  - **forecast**: For time series forecasting. [web](https://pkg.robjhyndman.com/forecast/)\n  - **tseries**: For time series analysis and computational finance.\n  - **zoo**: For working with regular and irregular time series data.\n  - **xts**: For extensible time series.\n  - **prophet**: For forecasting time series data. [web](https://facebook.github.io/prophet/) [@taylor2017forecasting]\n\n\n# Recap and References\n\n\n## Recap\n\n\n## References\n\n- [Wikipedia: Time Series Analysis](https://en.wikipedia.org/wiki/Time_series)\n- [Time Series Analysis](https://otexts.com/fpp3/)\n\n- [Complete Guide on Time Series Analysis in Python](https://www.kaggle.com/code/prashant111/complete-guide-on-time-series-analysis-in-python/notebook)\n- [Time Series Data Visualization in Python](https://www.geeksforgeeks.org/time-series-data-visualization-in-python/)\n\n",
    "supporting": [
      "26-TimeSeries_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}