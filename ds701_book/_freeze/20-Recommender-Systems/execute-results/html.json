{
  "hash": "e9a6a06f3ee59caa55161d6ee30d4a75",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Recommender Systems\njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/20-Recommender-Systems.ipynb)\n\n::: {#556a9ed8 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"remove-input\"]' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mp\nimport sklearn\nfrom IPython.display import Image, HTML\n\nimport laUtilities as ut\n\n%matplotlib inline\n```\n:::\n\n\nToday, we look at a topic that has become enormously important in society: recommender systems.\n\nWe will\n* Define recommender systems\n* Review the challenges they pose\n* Discuss two classic methods:\n    * Collaborative Filtering\n    * Matrix Factorization\n\n\n\n\n```{margin}\nThis section draws heavily on \n* These [slides](http://alex.smola.org/teaching/berkeley2012/slides/8_Recommender.pdf) by Alex Smola\n* [Matrix Factorization Techniques for Recommender Systems,](https://ieeexplore.ieee.org/document/5197422) by Yehuda Koren, Robert Bell, and Chris Volinsky, and\n* [Collaborative Filtering with Temporal Dynamics,](https://dl.acm.org/doi/10.1145/1557019.1557072) by Yehuda Koren\n```\n\n\n\n\n## What are Recommender Systems?\n\nThe concept of recommender systems emerged in the late 1990s / early 2000s as social life moved online:\n* online purchasing and commerce\n* online discussions and ratings\n* social information sharing\n\nIn these systems content was exploding and users were having a hard time finding things they were interested in.\n\nUsers wanted recommendations.\n\nOver time, the problem has only gotten worse:\n\n<center>\n    \n<img src=\"figs/L20-netflix-options.png\" alt=\"Figure\" width=\"100%\">\n    \n</center>\n\n<center>\n    \n<img src=\"figs/L20-amazon-options.png\" alt=\"Figure\" width=\"100%\">\n    \n</center>\n\nAn enormous need has emerged for systems to help sort through products, services, and content items.\n\nThis often goes by the term __personalization.__\n\nSome examples:\n    \n* Movie recommendation (Netflix, YouTube)\n* Related product recommendation (Amazon)\n* Web page ranking (Google)\n* Social content filtering (Facebook, Twitter)\n* Services (Airbnb, Uber, TripAdvisor)\n* News content recommendation (Apple News)\n* Priority inbox & spam filtering (Google)\n* Online dating (OK Cupid)\n\nA more formal view:\n    \n* User - requests content\n* Objects - that can be displayed\n* Context - device, location, time\n* Interface - browser, mobile\n\n<center>\n    \n<img src=\"figs/L20-recsys-abstractly.png\" alt=\"Figure\" width=\"45%\">\n    \n</center>\n\n## Inferring Preferences\n\nUnfortunately, users generally have a hard time __explaining__ what types of content they prefer.   Some early systems worked by interviewing users to ask what they liked.  Those systems did not work very well.\n\n\n\n\n```{margin}\nA very interesting article about the earliest personalization systems is [User Modeling via Stereotypes](https://www.cs.utexas.edu/users/ear/CogSci.pdf) by Elaine Rich, dating from 1979.\n```\n\n\n\n\nInstead, modern systems work by capturing user's opinions about __specific__ items.\n\nThis can be done actively:\n* When a user is asked to **rate** a movie, product, or experience,\n\nOr it can be done passively:\n* By noting which items a user **chooses** to purchase (for example).\n\n<center>\n    \n<img src=\"figs/L20-example-data.png\" alt=\"Figure\" width=\"55%\">\n    \n</center>\n\n## Challenges\n\n* The biggest issue is __scalability__: typical data for this problem is huge.\n  * Millions of objects\n  * 100s of millions of users\n* Changing user base\n* Changing inventory (movies, stories, goods)\n* Available features\n* Imbalanced dataset\n    * User activity / item reviews are power law distributed    \n\n\n\n\n```{margin}\nThis data is a subset of the data presented in: \"From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews,\" by J. McAuley and J. Leskovec. WWW, 2013\n```\n\n\n::: {#6a7c77a3 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=4}\n``` {.python .cell-code}\ndf = pd.read_csv('train.csv')\n```\n:::\n\n\n\n__Reviews are Sparse.__\n\nExample: A commonly used dataset for testing consists of Amazon movie reviews:\n* 1,697,533 reviews\n* 123,960 users\n* 50,052 movies\n\nNotice that there are 6,204,445,920 __potential__ reviews, but we only have 1,697,533 __actual__ reviews.\n\nOnly 0.02% of the reviews are available -- 99.98% of the reviews are missing.\n\n::: {#3f701cfe .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nprint(f'There are on average {n_reviews/n_movies:0.1f} reviews per movie' +\n     f' and {n_reviews/n_users:0.1f} reviews per user')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are on average 33.9 reviews per movie and 13.7 reviews per user\n```\n:::\n:::\n\n\n__Sparseness is skewed.__\n\nAlthough on average a movie receives 34 reviews, __almost all movies have even fewer reviews.__\n\n::: {#7fed9802 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"remove-input\"]' execution_count=7}\n``` {.python .cell-code}\nreviews_per_movie = df.groupby('ProductId').count()['Id'].values\nfrac_below_mean = np.sum(reviews_per_movie < (n_reviews/n_movies))/len(reviews_per_movie)\nplt.plot(sorted(reviews_per_movie), '.-')\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(n_reviews/n_movies, xmin, xmax, 'r', lw = 3)\nplt.ylabel('Number of Ratings', fontsize = 14)\nplt.xlabel('Movie', fontsize = 14)\nplt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per Movie\\n' +\n          f'{frac_below_mean:0.0%} of Movies Below Average', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](20-Recommender-Systems_files/figure-html/cell-7-output-1.png){width=607 height=508}\n:::\n:::\n\n\nLikewise, although the average user writes 14 reviews, almost all users write even fewer reviews.\n\n::: {#1229687e .cell hide_input='true' tags='[\"remnove-input\"]' execution_count=8}\n``` {.python .cell-code}\nreviews_per_user = df.groupby('UserId').count()['Id'].values\nfrac_below_mean = np.sum(reviews_per_user < (n_reviews/n_users))/len(reviews_per_user)\nplt.plot(sorted(reviews_per_user), '.-')\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(n_reviews/n_users, xmin, xmax, 'r', lw = 3)\nplt.ylabel('Number of Ratings', fontsize = 14)\nplt.xlabel('User', fontsize = 14)\nplt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per User\\n' +\n          f'{frac_below_mean:0.0%} of Users Below Average', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](20-Recommender-Systems_files/figure-html/cell-8-output-1.png){width=607 height=508}\n:::\n:::\n\n\nA typical objective function is root mean square error (RMSE)\n\n$$ \\text{RMSE} = \\sqrt{1/|S| \\sum_{(i,u)\\in S} (\\hat{r}_{ui} - r_{ui})^2} $$\n\nwhere $ r_{ui} $ is the rating that user $u$ gives to item $i$, and $S$ is the set of all ratings.\n\nOK, now we know the problem and the data available.   How can we address the problem?\n\nThe earliest method developed is called __collaborative filtering.__\n\n## Collaborative Filtering\n\nThe central idea of collaborative filtering is that the set of known recommendations can be considered to be a __bipartite graph.__\n\n<center>\n    \n<img src=\"figs/L20-bipartite.png\" alt=\"Figure\" width=\"35%\">\n    \n</center>\n\nThe nodes of the bipartite graph are __users__ and __items__.   \n\nEach edge corresponds to a known rating $r_{ui}.$\n\nThen recommendations are formed by traversing or processing the bipartite graph.\n\n<center>\n    \n<img src=\"figs/L20-cf-basic-idea.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nThere are at least two ways this graph can be used.\n\nTo form a rating for item $(u, i)$: \n    \n1. Using user-user similarity:\n      * look at users that have similar item preferences to user $u$\n      * look at how those users rated item $i$\n      \nGood for many users, fewer items\n\n    \n2. Using item-item similarity:\n      * look at other items that have been liked by similar users as item $i$\n      * look at how user $u$ rated those items\n      \nGood for many items, fewer users\n\n### Item-item CF\n\nLet's look at the item-item CF approach in detail.\n\nThe questions are:\n* How do we judge \"similarity\" of items?\n* How do we form a predicted rating?\n\nHere is another view of the ratings graph, this time as a matrix that includes missing entries:\n\n<center>\n    \n<img src=\"figs/L20-u-u-cf-1.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nLet's say we want to predict the value of this unknown rating:\n\n<center>\n    \n<img src=\"figs/L20-u-u-cf-2.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nWe'll consider two other items, namely items 3 and 6 (for example).\n\nNote that we are only interested in items that this user has rated.\n\n<center>\n    \n<img src=\"figs/L20-u-u-cf-3.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nWe will discuss strategies for assessing similarity shortly. \n\nHow did we choose these two items?   \n\nWe used __$k$-nearest neighbors__.   Here $k$ = 2.\n\nFor now, let's just say we determine the similarities as:\n\n$$ s_{13} = 0.2 $$\n\n$$ s_{16} = 0.3 $$\n\n<center>\n    \n<img src=\"figs/L20-u-u-cf-3.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nThese similarity scores tell us how much weight to put on the rating of the other items.\n\nSo we can form a prediction of $\\hat{r}_{15}$ as:\n    \n$$ \\hat{r}_{15} = \\frac{s_{13} \\cdot r_{35} + s_{16} \\cdot r_{65}}{s_{13} + s_{16}} = \\frac{0.2 \\cdot 2 + 0.3 \\cdot 3}{0.2 + 0.3} = 2.6 $$\n\n<center>\n    \n<img src=\"figs/L20-u-u-cf-4.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\n### Similarity\n\nHow should we assess similarity of items?\n\nA reasonable approach is to consider items similar if their ratings are __correlated.__\n\nSo we can use the Pearson correlation coefficient $r$.\n\nHowever, note that two items will not have ratings in the same positions.\n\n<center>\n    \n<img src=\"figs/L20-corr-support.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nSo we want to compute correlation only over the users who rated both the items.\n\nIn some cases we will need to work with binary $r_{ui}$s.  \n\nFor example, purchase histories on an e-commerce site, or clicks on an ad.\n\nIn this case, an appropriate replacement for Pearson $r$ is the Jaccard similarity coefficient.\n\n(See the lecture on similarity measures.)\n\n### Improving CF\n\nOne problem with the story so far arises due to __bias__.\n* Some items are significantly higher or lower rated\n* Some users rate substantially higher or lower in general\n\nThese properties interfere with similarity assessment.  \n\nBias correction is crucial for CF recommender systems.\n\nWe need to include\n* Per-user offset\n* Per-item offset\n* Global offset\n\nHence we need to form a per-item bias of:\n    \n$$ b_{ui} = \\mu + \\alpha_u + \\beta_i $$\n\nwhere $\\alpha_u$ is the per-user offset of user $u$ and $\\beta_i$ is the per-item offset of item $i$.\n\nHow can we estimate the $\\alpha$s, the $\\beta$s, and the $\\mu$?\n\nLet's assume for a minute that we had a fully-dense matrix of ratings $R$.\n\n$R$ has items on the rows and users on the columns.\n\nThen what we want to estimate is\n\n$$\\min_{\\alpha,\\beta,\\mu} \\Vert R - \\mathbf{1}\\alpha^T + \\beta\\mathbf{1}^T + \\mu1\\Vert^2 + \\lambda(\\Vert\\alpha\\Vert^2 + \\Vert\\beta\\Vert^2) $$\n\nHere, $\\mathbf{1}$ represents appropriately sized vectors of ones, and $1$ is a matrix of ones.\n\nWhile this is not a simple ordinary least squares problem, there is a strategy for solving it.\n\nAssume we hold $\\beta\\mathbf{1}^T + \\mu1$ constant.  \n\nThen the remaining problem is \n\n$$\\min_{\\alpha} \\Vert R - \\mathbf{1}\\alpha^T \\Vert^2 + \\lambda \\Vert\\alpha\\Vert^2 $$\n\nwhich (for each column of $R$) is a standard least squares problem (which we solve via Ridge regression).\n\nThis sort of problem is called __jointly convex__.   \n\nThe strategy for solving is:\n    \n1. Hold $\\alpha$ and $\\beta$ constant, solve for $\\mu$.\n2. Hold $\\alpha$ and $\\mu$ constant, solve for $\\beta$.\n3. Hold $\\beta$ and $\\mu$ constant, solve for $\\alpha$.\n\nEach of the three steps will reduce the overall error.   So we iterate over them until convergence.\n\nThe last issue is that the matrix $R$ is not dense - in reality we only have a small subset of its entries.\n\nWe simply need to adapt the least-squares solution to only consider the entries in $R$ that we know.\n\nAs a result, the actual calculation is:\n\nStep 1:\n\n$$ \\mu = \\frac{\\sum_{(u, i) \\in R} (r_{ui} - \\alpha_u - \\beta_i)}{|R|} $$\n\nStep 2: \n\n$$ \\alpha_u = \\frac{\\sum_{i \\in R(u)}(r_{ui} - \\mu - \\beta_i)}{\\lambda + |R(u)|} $$\n\nStep 3:\n    \n$$ \\beta_i = \\frac{\\sum_{u \\in R(i)}(r_{ui} - \\mu - \\alpha_u)}{\\lambda + |R(i)|} $$\n\nStep 4: If not converged, go to Step 1.\n\nNow that we have the biases learned, we can do a better job of estimating correlation:\n\n$$ \\hat{\\rho}_{ij} = \\frac{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})(r_{uj}-b_{uj})} \n{\\sqrt{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})^2\\sum_{u\\in U(i,j)}(r_{uj}-b_{uj})^2}} $$\n\nwhere \n* $b_{ui} = \\mu + \\alpha_u + \\beta_i$, and\n* $U(i,j)$ are the users who have rated both $i$ and $j$.\n\nAnd using biases we can also do a better job of estimating ratings:\n\n$$ \\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{j \\in n_k(i, u)} s_{ij}(r_{uj} - b_{uj})}{\\sum_{j \\in n_k(i, u)} s_{ij}} $$\n\nwhere \n* $b_{ui} = \\mu + \\alpha_u + \\beta_i$, and\n* $n_k(i, u)$ are the $k$ nearest neighbors to $i$ that were rated by user $u$.\n\n### Assessing CF\n\nThis completes the high level view of CF.\n\nWorking with user-user similarities is analogous.\n\nStrengths:\n* Essentially no training.\n    * The reliance on $k$-nearest neighbors helps in this respect.\n* Easy to update with new users, items, and ratings\n* Can be explained to user: \n    * \"We recommend _Minority Report_ because you liked _Blade Runner_ and _Total Recall._\"\n\nWeaknesses:\n* Accuracy can be a problem\n* Scalability can be a problem (think $k$-NN)\n\n## Matrix Factorization\n\nNote that standard CF forces us to consider similarity among items, __or__ among users, but does not take into account __both.__\n\nCan we use both kinds of similarity simultaneously?\n\nWe can't use both the rows and columns of the ratings matrix $R$ at the same time -- the user and item vectors live in different vector spaces.\n\nWhat we could try to do is find a __single__ vector space in which we represent __both__ users __and__ items, along with a similarity function, such that:\n* users who have similar item ratings are similar in the vector space\n* items who have similar user ratings are similar in the vector space\n* when a given user highly rates a given item, that user and item are similar in the vector space.\n\n<center>\n    \n<img src=\"figs/L10-Movie-Latent-Space.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nKoren et al, IEEE Computer, 2009 \n\nWe saw this idea previously, in an SVD lecture.\n\nThis new vector space is called a __latent__ space,\n\nand the user and item representations are called __latent vectors.__\n\nNow, however, we are working with a matrix which is only __partially observed.__\n\nThat is, we only know __some__ of the entries in the ratings matrix.\n\nNonetheless, we can imagine a situation like this:\n    \n<center>\n    \n<img src=\"figs/L20-mf-1.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nNow we want the product of the two matrices on the right to be as close as possible __to the known values__ of the ratings matrix.\n\nWhat this setup implies is that our similarity function is the __inner product.__\n\nWhich means that to predict an unknown rating, we take the __inner product of latent vectors:__\n\n<center>\n    \n<img src=\"figs/L20-mf-2.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nNow $(-2 \\cdot -0.5)+(0.3 \\cdot 0.6)+(2.5 \\cdot 0.5) = 2.43$, so:\n\n<center>\n    \n<img src=\"figs/L20-mf-3.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\n### Solving Matrix Factorization\n\nNotice that in this case we've decided that the factorization should be rank 3, ie, low-rank.\n\nSo we want something like an SVD.\n\n(Recall that SVD gives us the most-accurate-possible low-rank factorization of a matrix).\n\nHowever, we can't use the SVD algorithm directly, because we don't know all the entries in $R$. \n\n(Indeed, the unseen entries in $R$ as exactly what we want to predict.)\n\n<center>\n    \n<img src=\"figs/L20-mf-1.png\" alt=\"Figure\" width=\"60%\">\n    \n</center>\n\nHere is what we want to solve: \n    \n$$ \\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) $$\n\nwhere $R$ is $m\\times n$, $U$ is the $m\\times k$ items matrix and $V$ is the $n\\times k$ users matrix.\n\nThe $(\\cdot)_S$ notation means that we are only considering the matrix entries that correspond to known reviews (the set $S$).\n\nNote that as usual, we add $\\ell_2$ penalization to avoid overfitting (Ridge regression).\n\nOnce again, this problem is __jointly convex.__   \n\nIn particular, it we hold either $U$ or $V$ constant, then the result is a simple ridge regression.\n\nSo one commonly used algorithm for this problem is called __alternating least squares:__\n    \n1. Hold $U$ constant, and solve for $V$\n2. Hold $V$ constant, and solve for $U$\n3. If not converged, go to Step 1.\n\nThe only thing I've left out at this point is how to deal with the missing entries of $R$.  \n\nIt's not hard, but the details aren't that interesting, so I will give you code instead!\n\n### ALS in Practice\n\nThe entire Amazon reviews dataset is too large to work with easily, and it is too sparse. \n\nHence, we will take the densest rows and columns of the matrix.\n\n::: {#fbd08a2a .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=9}\n``` {.python .cell-code}\n# The densest columns: products with more than 50 reviews\npids = df.groupby('ProductId').count()['Id']\nhi_pids = pids[pids > 50].index\n\n# reviews that are for these products\nhi_pid_rec = [r in hi_pids for r in df['ProductId']]\n\n# the densest rows: users with more than 50 reviews\nuids = df.groupby('UserId').count()['Id']\nhi_uids = uids[uids > 50].index\n\n# reviews that are from these users\nhi_uid_rec = [r in hi_uids for r in df['UserId']]\n\n# reviews that are from those users and for those movies\ngoodrec = [a and b for a, b in zip(hi_uid_rec, hi_pid_rec)]\n```\n:::\n\n\nNow we create a matrix from these reviews.\n\nMissing entries will be filled with NaNs.\n\n::: {#3cd35f22 .cell execution_count=10}\n``` {.python .cell-code}\ndense_df = df.loc[goodrec]\ngood_df = dense_df.loc[~df['Score'].isnull()]\nR = good_df.pivot_table(columns = 'ProductId', index = 'UserId', values = 'Score')\n```\n:::\n\n\n::: {#b388e2f7 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=11}\n``` {.python .cell-code}\nR\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ProductId</th>\n      <th>0005019281</th>\n      <th>0005119367</th>\n      <th>0307142485</th>\n      <th>0307142493</th>\n      <th>0307514161</th>\n      <th>0310263662</th>\n      <th>0310274281</th>\n      <th>0718000315</th>\n      <th>0764001035</th>\n      <th>0764003828</th>\n      <th>...</th>\n      <th>B00IKM5OCO</th>\n      <th>B00IWULQQ2</th>\n      <th>B00J4LMHMK</th>\n      <th>B00J5JSV1W</th>\n      <th>B00JA3RPAG</th>\n      <th>B00JAQJMJ0</th>\n      <th>B00JBBJJ24</th>\n      <th>B00JKPHUE0</th>\n      <th>B00K2CHVJ4</th>\n      <th>B00L4IDS4W</th>\n    </tr>\n    <tr>\n      <th>UserId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A02755422E9NI29TCQ5W3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A100JCBNALJFAW</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A10175AMUHOQC4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A103KNDW8GN92L</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A106016KSI0YQ</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>AZUBX0AYYNTFF</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>AZXGPM8EKSHE9</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>AZXHK8IO25FL6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>AZXR5HB99P936</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>AZZ4GD20C58ND</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>3677 rows × 7244 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#bbdb33d7 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=12}\n``` {.python .cell-code}\nimport MF as MF\n\n# I am pulling these hyperparameters out of the air;\n# That's not the right way to do it!\nRS = MF.als_MF(rank = 20, lambda_ = 1)\n```\n:::\n\n\n::: {#5f36eb2e .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=13}\n``` {.python .cell-code}\n%time pred, error = RS.fit_model(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 10min 24s, sys: 8min 39s, total: 19min 3s\nWall time: 2min 6s\n```\n:::\n:::\n\n\n::: {#350d019e .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"remove-input\"]' execution_count=14}\n``` {.python .cell-code}\nprint(f'RMSE on visible entries (training data): {np.sqrt(error/R.count().sum()):0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE on visible entries (training data): 0.344\n```\n:::\n:::\n\n\n::: {#a0f407f2 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=15}\n``` {.python .cell-code}\npred\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ProductId</th>\n      <th>0005019281</th>\n      <th>0005119367</th>\n      <th>0307142485</th>\n      <th>0307142493</th>\n      <th>0307514161</th>\n      <th>0310263662</th>\n      <th>0310274281</th>\n      <th>0718000315</th>\n      <th>0764001035</th>\n      <th>0764003828</th>\n      <th>...</th>\n      <th>B00IKM5OCO</th>\n      <th>B00IWULQQ2</th>\n      <th>B00J4LMHMK</th>\n      <th>B00J5JSV1W</th>\n      <th>B00JA3RPAG</th>\n      <th>B00JAQJMJ0</th>\n      <th>B00JBBJJ24</th>\n      <th>B00JKPHUE0</th>\n      <th>B00K2CHVJ4</th>\n      <th>B00L4IDS4W</th>\n    </tr>\n    <tr>\n      <th>UserId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A02755422E9NI29TCQ5W3</th>\n      <td>5.214071</td>\n      <td>5.337271</td>\n      <td>5.056362</td>\n      <td>4.087416</td>\n      <td>5.407318</td>\n      <td>5.297832</td>\n      <td>5.071851</td>\n      <td>4.281667</td>\n      <td>4.749674</td>\n      <td>4.973165</td>\n      <td>...</td>\n      <td>3.650950</td>\n      <td>5.014452</td>\n      <td>4.495824</td>\n      <td>3.159308</td>\n      <td>4.592631</td>\n      <td>4.737402</td>\n      <td>4.844146</td>\n      <td>4.857948</td>\n      <td>3.970947</td>\n      <td>4.807057</td>\n    </tr>\n    <tr>\n      <th>A100JCBNALJFAW</th>\n      <td>4.417716</td>\n      <td>3.817517</td>\n      <td>3.224885</td>\n      <td>2.357864</td>\n      <td>5.030919</td>\n      <td>2.621849</td>\n      <td>3.948541</td>\n      <td>3.346081</td>\n      <td>3.728381</td>\n      <td>2.551108</td>\n      <td>...</td>\n      <td>3.829715</td>\n      <td>3.925485</td>\n      <td>3.231605</td>\n      <td>3.145845</td>\n      <td>4.113032</td>\n      <td>4.462769</td>\n      <td>2.925964</td>\n      <td>2.396574</td>\n      <td>2.887802</td>\n      <td>3.132729</td>\n    </tr>\n    <tr>\n      <th>A10175AMUHOQC4</th>\n      <td>5.293009</td>\n      <td>4.997519</td>\n      <td>5.887083</td>\n      <td>5.469588</td>\n      <td>4.961311</td>\n      <td>4.931874</td>\n      <td>4.692830</td>\n      <td>3.581315</td>\n      <td>4.272003</td>\n      <td>4.683869</td>\n      <td>...</td>\n      <td>4.230151</td>\n      <td>4.732049</td>\n      <td>5.258265</td>\n      <td>2.890735</td>\n      <td>4.361825</td>\n      <td>5.417988</td>\n      <td>6.094052</td>\n      <td>4.464990</td>\n      <td>4.183745</td>\n      <td>4.107988</td>\n    </tr>\n    <tr>\n      <th>A103KNDW8GN92L</th>\n      <td>4.507828</td>\n      <td>4.936013</td>\n      <td>4.201449</td>\n      <td>6.079262</td>\n      <td>4.308772</td>\n      <td>3.876277</td>\n      <td>4.287324</td>\n      <td>4.428760</td>\n      <td>4.306389</td>\n      <td>5.911843</td>\n      <td>...</td>\n      <td>3.694592</td>\n      <td>4.522997</td>\n      <td>5.072133</td>\n      <td>2.288274</td>\n      <td>3.707725</td>\n      <td>4.664537</td>\n      <td>4.901905</td>\n      <td>4.725056</td>\n      <td>4.371643</td>\n      <td>4.125607</td>\n    </tr>\n    <tr>\n      <th>A106016KSI0YQ</th>\n      <td>3.845861</td>\n      <td>4.375542</td>\n      <td>2.865917</td>\n      <td>4.754560</td>\n      <td>4.615124</td>\n      <td>3.731579</td>\n      <td>4.204170</td>\n      <td>3.166638</td>\n      <td>3.870211</td>\n      <td>4.389759</td>\n      <td>...</td>\n      <td>4.158656</td>\n      <td>3.886184</td>\n      <td>2.773142</td>\n      <td>2.501356</td>\n      <td>2.541995</td>\n      <td>3.564672</td>\n      <td>3.943119</td>\n      <td>2.447221</td>\n      <td>2.340907</td>\n      <td>1.915349</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>AZUBX0AYYNTFF</th>\n      <td>3.674328</td>\n      <td>4.317386</td>\n      <td>3.246379</td>\n      <td>4.588257</td>\n      <td>4.433596</td>\n      <td>3.773535</td>\n      <td>3.810215</td>\n      <td>3.088035</td>\n      <td>2.628883</td>\n      <td>3.896187</td>\n      <td>...</td>\n      <td>2.849555</td>\n      <td>3.482059</td>\n      <td>2.862210</td>\n      <td>1.798721</td>\n      <td>2.512885</td>\n      <td>4.537965</td>\n      <td>3.276120</td>\n      <td>3.582396</td>\n      <td>2.254772</td>\n      <td>2.610681</td>\n    </tr>\n    <tr>\n      <th>AZXGPM8EKSHE9</th>\n      <td>2.622713</td>\n      <td>3.290833</td>\n      <td>0.960872</td>\n      <td>3.759832</td>\n      <td>3.664305</td>\n      <td>3.286624</td>\n      <td>3.864391</td>\n      <td>3.333899</td>\n      <td>3.396193</td>\n      <td>1.919726</td>\n      <td>...</td>\n      <td>2.114964</td>\n      <td>3.191576</td>\n      <td>0.844707</td>\n      <td>3.084528</td>\n      <td>2.390036</td>\n      <td>1.277799</td>\n      <td>2.933082</td>\n      <td>3.213754</td>\n      <td>2.616831</td>\n      <td>3.778216</td>\n    </tr>\n    <tr>\n      <th>AZXHK8IO25FL6</th>\n      <td>3.402754</td>\n      <td>3.820739</td>\n      <td>3.240812</td>\n      <td>4.015029</td>\n      <td>4.723189</td>\n      <td>3.087372</td>\n      <td>3.540523</td>\n      <td>3.307519</td>\n      <td>3.449936</td>\n      <td>3.623652</td>\n      <td>...</td>\n      <td>2.483712</td>\n      <td>3.556408</td>\n      <td>3.140434</td>\n      <td>2.104098</td>\n      <td>2.626208</td>\n      <td>3.651925</td>\n      <td>2.806459</td>\n      <td>3.601581</td>\n      <td>2.816939</td>\n      <td>4.530367</td>\n    </tr>\n    <tr>\n      <th>AZXR5HB99P936</th>\n      <td>3.984719</td>\n      <td>4.140453</td>\n      <td>3.389305</td>\n      <td>3.158184</td>\n      <td>4.870186</td>\n      <td>3.802074</td>\n      <td>4.233936</td>\n      <td>3.369978</td>\n      <td>3.732052</td>\n      <td>2.903827</td>\n      <td>...</td>\n      <td>3.111108</td>\n      <td>3.986335</td>\n      <td>2.725485</td>\n      <td>2.691206</td>\n      <td>3.094194</td>\n      <td>3.127388</td>\n      <td>3.503217</td>\n      <td>3.013696</td>\n      <td>2.777004</td>\n      <td>3.620779</td>\n    </tr>\n    <tr>\n      <th>AZZ4GD20C58ND</th>\n      <td>4.953150</td>\n      <td>4.785057</td>\n      <td>4.262979</td>\n      <td>4.772219</td>\n      <td>5.616228</td>\n      <td>3.955708</td>\n      <td>4.520935</td>\n      <td>3.794724</td>\n      <td>3.724401</td>\n      <td>4.179466</td>\n      <td>...</td>\n      <td>3.779693</td>\n      <td>4.348419</td>\n      <td>4.600293</td>\n      <td>2.805938</td>\n      <td>3.572500</td>\n      <td>5.020926</td>\n      <td>4.539176</td>\n      <td>3.574643</td>\n      <td>3.113098</td>\n      <td>3.377049</td>\n    </tr>\n  </tbody>\n</table>\n<p>3677 rows × 7244 columns</p>\n</div>\n```\n:::\n:::\n\n\n\nJust for comparison's sake, let's check the performance of $k$-NN on this dataset.\n\nAgain, this is only on the training data -- so overly optimistic for sure.\n\nAnd note that this is a subset of the full dataset -- the subset that is \"easiest\" to predict due to density.\n\n::: {#c49d9917 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error\nX_train = good_df.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\ny_train = good_df['Score']\n# Using k-NN on features HelpfulnessNumerator, HelpfulnessDenominator, Score, Time\nmodel = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n%time y_hat = model.predict(X_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 3.13 s, sys: 14.7 ms, total: 3.15 s\nWall time: 3.17 s\n```\n:::\n:::\n\n\n::: {#2a5bd34b .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"remove-input\"]' execution_count=18}\n``` {.python .cell-code}\nprint(f'RMSE on visible entries (test set): {mean_squared_error(y_train, y_hat, squared = False):0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE on visible entries (test set): 0.649\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/tomg/Source/courses/tools4ds/DS701-Course-Notes/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n```\n:::\n:::\n\n\n### Assessing MF\n\nMatrix Factorization per se is a good idea.    However, many of the improvements we've discussed for CF apply to MF as well.\n\nTo illustrate, we'll look at some of the successive improvements used by the team that won the Netflix prize (\"BellKor's Pragmatic Chaos\").\n\nWhen the prize was announced, the Netflix supplied solution achieved an RMSE of 0.951.\n\nBy the end of the competition (about 3 years), the winning team's solution achieved RMSE of 0.856.\n\nLet's restate our MF objective in a way that will make things clearer:\n\n$$ \\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - u_u^Tv_i)^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) $$\n\n__1. Adding Biases__\n\n$$ \\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u + \\beta_i + u_u^Tv_i)^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) $$\n\n<center>\n    \n<img src=\"figs/L20-netflix-1.png\" alt=\"Figure\" width=\"70%\">\n    \n</center>\n\n__2. Who Rated What?__\n\nIn reality, ratings are not provided __at random.__\n\nTake note of which users rated the same movies (ala CF) and use this information.\n\n<center>\n    \n<img src=\"figs/L20-netflix-2.png\" alt=\"Figure\" width=\"70%\">\n    \n</center>\n\n<center>\n    \n<img src=\"figs/L20-netflix-3.png\" alt=\"Figure\" width=\"70%\">\n    \n</center>\n\n__3. Ratings Change Over Time__\n\nOlder movies tend to get higher ratings!\n\n<center>\n    \n<img src=\"figs/L20-netflix-4.png\" alt=\"Figure\" width=\"70%\">\n    \n</center>\n\n$$ \\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u(t) + \\beta_i(t) + u_u^Tv_i(t))^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) $$\n\n<center>\n    \n<img src=\"figs/L20-netflix-5.png\" alt=\"Figure\" width=\"70%\">\n    \n</center>\n\nTo estimate these billions of parameters, we cannot use alternating least squares or any linear algebraic method.\n\nWe need to use gradient descent (which we will cover in a future lecture).\n\n## Assessing Recommender Systems\n\nThere are a number of concerns with the widespread use of recommender systems and personalization in society.\n\nFirst, recommender systems are accused of creating __filter bubbles.__ \n\nA filter bubble is the tendency for recommender systems to limit the variety of information presented to the user.\n\nThe concern is that a user's past expression of interests will guide the algorithm in continuing to provide \"more of the same.\"\n\nThis is believed to increase polarization in society, and to reinforce confirmation bias.\n\nSecond, recommender systems in modern usage are often tuned to __maximize engagement.__\n\nIn other words, the objective function of the system is not to present the user's most favored content, but rather the content that will be most likely to keep the user on the site.\n\nThe incentive to maximize engagement arises on sites that are supported by advertising revenue.   \n\nMore engagement time means more revenue for the site.\n\nHowever, many studies have shown that sites that strive to __maximize engagement__ do so in large part by guiding users toward __extreme content:__\n* content that is shocking, \n* or feeds conspiracy theories, \n* or presents extreme views on popular topics.\n\nGiven this tendency of modern recommender systems, \nfor a third party to create \"clickbait\" content such as this, one of the easiest ways is to present false claims.\n\nMethods for addressing these issues are being very actively studied at present.\n\nWays of addressing these issues can be:\n* via technology\n* via public policy\n\n\n\n\n```{note}\nYou can read about some of the work done in my group on this topic:\n* [How YouTube Leads Privacy-Seeking Users Away from Reliable Information,](http://www.cs.bu.edu/faculty/crovella/paper-archive/youtube-fairumap20.pdf) \nLarissa Spinelli and Mark Crovella,\n_Proceedings of the Workshop on Fairness in User Modeling, Adaptation, and Personalization (FairUMAP)_, 2020.   \n* [Closed-Loop Opinion Formation,](http://www.cs.bu.edu/faculty/crovella/paper-archive/netsci17-filterbubble.pdf) Larissa Spinelli and Mark Crovella \n_Proceedings of the 9th International ACM Web Science Conference (WebSci)_, 2017.\n* [Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems,](http://www.cs.bu.edu/faculty/crovella/paper-archive/wsdm19-antidote-data.pdf)\nBashir Rastegarpanah, Krishna P. Gummadi and Mark Crovella\n_Proceedings of WSDM_, 2019.\n```\n\n",
    "supporting": [
      "20-Recommender-Systems_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}