{
  "hash": "bc64eec90147798b0e82d0df519e5bc0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Recommender Systems\njupyter: python3\nbibliography: references.bib\n---\n\n# Introduction\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/20-Recommender-Systems.ipynb)\n\n\n\nToday, we look at a topic that has become enormously important in society: recommender systems.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\nWe will\n\n* Define recommender systems\n* Review the challenges they pose\n* Discuss two classic methods:\n    * Collaborative Filtering\n    * Matrix Factorization\n* And one modern method:\n    * Deep Learning Recommender Model (DLRM)\n\n:::\n::: {.column width=\"50%\"}\n\n\nThis section draws heavily on\n\n* These [slides](http://alex.smola.org/teaching/berkeley2012/slides/8_Recommender.pdf) by Alex Smola\n* _Matrix Factorization Techniques for Recommender Systems_, [@koren2009matrix]\n* _Collaborative Filtering with Temporal Dynamics_, [@koren2009collaborative]\n* _Deep Learning Recommender Model for Personalization and Recommendation Systems_, [@naumov2019deep]\n\n:::\n::::\n\n\n## What are Recommender Systems?\n\nThe concept of recommender systems emerged in the late 1990s / early 2000s as social life moved online:\n\n* online purchasing and commerce\n* online discussions and ratings\n* social information sharing\n\nIn these systems the amount of content was exploding and users were having a hard time finding things they were interested in.\n\n> Users wanted recommendations.\n\n---\n\nOver time, the problem has only gotten worse:\n\n![](figs/L20-netflix-options.png){fig-align=\"center\"}\n\n---\n\n![](figs/L20-amazon-options.png){fig-align=\"center\"}\n\n---\n\nAn enormous need has emerged for systems to help sort through products, services, and content items.\n\nThis often goes by the term __personalization.__\n\nSome examples:\n    \n* Movie recommendation (Netflix ~6.5K movies and shows, YouTube ~14B videos)\n* Related product recommendation (Amazon ~600M products)\n* Web page ranking (Google >>100B pages)\n* Social content filtering (Facebook, Twitter)\n* Services (Airbnb, Uber, TripAdvisor)\n* News content recommendation (Apple News)\n* Priority inbox & spam filtering (Gmail)\n* Online dating (Match.com)\n\n---\n\nA more formal view:\n    \n* User - requests content\n* Objects - instances of content\n* Context - device, location, time, history\n* Interface - browser, mobile\n\n![](figs/L20-recsys-abstractly.png){fig-align=\"center\"}\n\n## Inferring Preferences\n\nUnfortunately, users generally have a hard time __explaining__ what types of\ncontent they prefer.   \n\nSome early systems worked by interviewing users to ask what they liked.  Those\nsystems did not work very well.\n\n::: aside\nA very interesting article about the earliest personalization systems is [User Modeling via Stereotypes](https://www.cs.utexas.edu/users/ear/CogSci.pdf) by Elaine Rich, dating from 1979.\n:::\n\n---\n\nInstead, modern systems work by capturing user's opinions about __specific__ items.\n\nThis can be done actively:\n\n* When a user is asked to **rate** a movie, product, or experience,\n\nor it can be done passively:\n\n* By noting which items a user **chooses** to purchase (for example).\n\n![](figs/L20-example-data.png){width=\"55%\" fig-align=\"center\"}\n\n\n## Challenges\n\n* The biggest issue is __scalability__: typical data for this problem is huge.\n  * Millions of objects\n  * 100s of millions of users\n* Changing user base\n* Changing inventory (movies, stories, goods)\n* Available features\n* Imbalanced dataset\n    * User activity / item reviews are power law distributed    \n\n::: aside\nThis data is a subset of the data presented in: \"From amateurs to connoisseurs:\nmodeling the evolution of user expertise through online reviews,\" by J. McAuley\nand J. Leskovec. WWW, 2013\n:::\n\n## Example\n\nLet's look at a dataset for testing recommender systems consisting of Amazon movie reviews:\n\nWe'll download a compressed pickle file containing the data if it is not already present.\n\n::: {#0274d422 .cell execution_count=2}\n``` {.python .cell-code}\n# This is a 647 MB file, delete it after use\nimport gdown\nurl = \"https://drive.google.com/uc?id=14GakA7oOjbQp7nxcGApI86WlP3GrYTZI\"\npickle_output = \"train.pkl.gz\"\n\nimport os.path\nif not os.path.exists(pickle_output):\n    gdown.download(url, pickle_output)\n```\n:::\n\n\nWe'll load the data into a pandas DataFrame.\n\n::: {#09fe11e5 .cell execution_count=3}\n``` {.python .cell-code}\nimport gzip\nimport time\n\nstart_time = time.time()\nwith gzip.open(pickle_output, 'rb') as f:\n    df = pd.read_pickle(f)\nelapsed_time = time.time() - start_time\nprint(f\"Elapsed read time: {elapsed_time:.2f} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nElapsed read time: 2.86 seconds\n```\n:::\n:::\n\n\nRun `df.info()` to see the column names and data types.\n\n::: {#64619c83 .cell execution_count=4}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1697533 entries, 0 to 1697532\nData columns (total 9 columns):\n #   Column                  Dtype  \n---  ------                  -----  \n 0   Id                      int64  \n 1   ProductId               object \n 2   UserId                  object \n 3   HelpfulnessNumerator    int64  \n 4   HelpfulnessDenominator  int64  \n 5   Score                   float64\n 6   Time                    int64  \n 7   Summary                 object \n 8   Text                    object \ndtypes: float64(1), int64(4), object(4)\nmemory usage: 116.6+ MB\n```\n:::\n:::\n\n\n---\n\nNow we can count the number of users and movies:\n\n::: {#49702416 .cell execution_count=5}\n``` {.python .cell-code}\nfrom IPython.display import display, Markdown\n\nn_users = df[\"UserId\"].unique().shape[0]\nn_movies = df[\"ProductId\"].unique().shape[0]\nn_reviews = len(df)\ndisplay(Markdown(f'There are:\\n'))\ndisplay(Markdown(f'* {n_reviews:,} reviews\\n* {n_movies:,} movies\\n* {n_users:,} users'))\n\ndisplay(Markdown(f'There are {n_users * n_movies:,} potential reviews, meaning sparsity of {(n_reviews/(n_users * n_movies)):0.4%}'))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nThere are:\n\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n* 1,697,533 reviews\n* 50,052 movies\n* 123,960 users\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nThere are 6,204,445,920 potential reviews, meaning sparsity of 0.0274%\n:::\n:::\n\n\nwhere\n\n$$\n\\text{sparsity} \n= \\frac{\\text{\\# of reviews}}{\\text{\\# of users} \\times \\text{\\# of movies}}\n= \\frac{\\text{\\# of reviews}}{\\text{\\# of potential reviews}}\n$$\n\n## Reviews are Sparse\n\nOnly 0.02% of the reviews are available -- 99.98% of the reviews are missing.\n\n::: {#6659888f .cell execution_count=6}\n``` {.python .cell-code}\ndisplay(Markdown(f'There are on average {n_reviews/n_movies:0.1f} reviews per movie' +\n     f' and {n_reviews/n_users:0.1f} reviews per user'))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nThere are on average 33.9 reviews per movie and 13.7 reviews per user\n:::\n:::\n\n\n## Sparseness is Skewed\n\nAlthough on average a movie receives 34 reviews, __almost all movies have even\nfewer reviews.__\n\n::: {#7155db06 .cell execution_count=7}\n``` {.python .cell-code}\nreviews_per_movie = df.groupby('ProductId').count()['Id'].values\nfrac_below_mean = np.sum(reviews_per_movie < (n_reviews/n_movies))/len(reviews_per_movie)\nplt.plot(sorted(reviews_per_movie, reverse=True), '.-')\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(n_reviews/n_movies, xmin, xmax, 'r', lw = 3)\nplt.ylabel('Number of Ratings', fontsize = 14)\nplt.xlabel('Movie', fontsize = 14)\nplt.legend(['Number of Ratings', 'Average Number of Ratings'], fontsize = 14)\nplt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per Movie\\n' +\n          f'{frac_below_mean:0.0%} of Movies Below Average', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](20-Recommender-Systems_files/figure-html/cell-8-output-1.png){width=607 height=508}\n:::\n:::\n\n\n---\n\nLikewise, although the average user writes 14 reviews, almost all users write even fewer reviews.\n\n::: {#19578c72 .cell execution_count=8}\n``` {.python .cell-code}\nreviews_per_user = df.groupby('UserId').count()['Id'].values\nfrac_below_mean = np.sum(reviews_per_user < (n_reviews/n_users))/len(reviews_per_user)\nplt.plot(sorted(reviews_per_user, reverse=True), '.-')\nxmin, xmax, ymin, ymax = plt.axis()\nplt.hlines(n_reviews/n_users, xmin, xmax, 'r', lw = 3)\nplt.ylabel('Number of Ratings', fontsize = 14)\nplt.xlabel('User', fontsize = 14)\nplt.legend(['Number of Ratings', 'Average Number of Ratings'], fontsize = 14)\nplt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per User\\n' +\n          f'{frac_below_mean:0.0%} of Users Below Average', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](20-Recommender-Systems_files/figure-html/cell-9-output-1.png){width=607 height=508}\n:::\n:::\n\n\n## Objective Function\n\nUltimately, our goal is to predict the rating that a user would give to an item.\n\nFor that, we need to define a loss or objective function.\n\nA typical objective function is root mean square error (RMSE)\n\n$$ \n\\text{RMSE} = \\sqrt{\\frac{1}{|S|} \\sum_{(i,u)\\in S} (\\hat{r}_{ui} - r_{ui})^2},\n$$\n\nwhere \n\n* $r_{ui}$ is the rating that user $u$ gives to item $i$, and \n* $S$ is the subset of items that have ratings.\n\nOK, now we know the problem and the data available.   How can we address the problem?\n\nThe earliest method developed is called __collaborative filtering.__\n\n# Colaborative Filtering\n\n## Collaborative Filtering\n\nThe central idea of collaborative filtering is that the set of known \nrecommendations can be considered to be a __bipartite graph.__\n\n![](figs/L20-bipartite.png){width=\"35%\" fig-align=\"center\"}\n\nThe nodes of the bipartite graph are __users__ ($U$) and __items__ ($V$).   \n\nEach edge corresponds to a known rating $r_{ui}.$\n\n---\n\nThen recommendations are formed by traversing or processing the bipartite graph.\n\n![](figs/L20-cf-basic-idea.png){width=\"60%\" fig-align=\"center\"}\n\nThere are at least two ways this graph can be used. \n\n---\n\nTwo ways to form a rating for item $(u, i)$: \n\n:::: {.fragment}\n1. Using **user-user similarity**:\n      * look at users that have similar item preferences to user $u$\n      * look at how those users rated item $i$\n::::  \n\n:::: {.fragment}\n⟹ <span style=\"background-color: yellow;\">Good for many users, fewer items.</span><br>\n(e.g., NetFix had ~280M subscribers, ~6.5K movies/shows)\n::::\n\n:::: {.fragment}\n2. Using **item-item similarity**:\n      * look at other items that have been liked by similar users as item $i$\n      * look at how user $u$ rated those items\n::::\n      \n:::: {.fragment}\n⟹ <span style=\"background-color: yellow;\">Good for many items, fewer users</span><br>\n(e.g. Amazon had ~300M accounts, ~600M products)\n::::\n\n## Item-Item CF\n\nFor item-item similarity, we'll look at **item-item Collaborative Filtering (CF).**\n\nThe questions are:\n\n::: {.incremental}\n* How do we judge \"similarity\" of items?\n* How do we form a predicted rating?\n:::\n\n---\n\nHere is another view of the ratings graph, this time as a matrix that includes missing entries:\n\n![](figs/L20-u-u-cf-1.png){width=\"60%\" fig-align=\"center\"}\n\n---\n\nLet's say we want to predict the value of this unknown rating:\n\n![](figs/L20-u-u-cf-2.png){width=\"60%\" fig-align=\"center\"}\n\n---\n\nWe'll consider two other items, namely items 3 and 6 (for example).\n\nNote that we are only interested in items that this user has rated.\n\n![](figs/L20-u-u-cf-3.png){width=\"60%\" fig-align=\"center\"}\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/L20-u-u-cf-3.png){width=\"100%\" fig-align=\"center\"}\n:::\n::: {.column width=\"50%\"}\nWe will discuss strategies for assessing similarity shortly. \n\nHow did we choose these two items?   \n\nWe used __$k$-nearest neighbors__.   Here $k$ = 2.\n\nFor now, let's just say we determine the similarities as:\n\n$$\ns_{13} = 0.2 \n$$\n\n$$\ns_{16} = 0.3 \n$$\n\n:::\n::::\n\n---\n\nThese similarity scores tell us how much weight to put on the rating of the other items.\n\n![](figs/L20-u-u-cf-4.png){width=\"50%\" fig-align=\"center\"}\n\nSo we can form a prediction of $\\hat{r}_{15}$ as:\n  \n$$\n\\hat{r}_{15} = \\frac{s_{13} \\cdot r_{35} + s_{16} \\cdot r_{65}}{s_{13} + s_{16}} = \\frac{0.2 \\cdot 2 + 0.3 \\cdot 3}{0.2 + 0.3} = 2.6 \n$$\n\n## Similarity\n\nHow should we assess similarity of items?\n\nA reasonable approach is to consider items similar if their ratings are\n__correlated__, for example using the Pearson correlation coefficient\n$r$[^pearson].\n\n[^pearson]: See the [Course Notes](#pearson-correlation-coefficient) for the definition of Pearson correlation coefficient.\n\nHowever, note that two items will not have ratings in the same positions.\n\n![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n\nSo we want to compute correlation only over the users who rated both the items.\n\n## Example\n\n::: {.content-visible when-profile=\"slides\"}\n![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n:::\n\nLet's compute the Pearson correlation coefficient for the above two items:\n\n::: {#6ba127fb .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nfrom IPython.display import display, Markdown\n\nratings_item_i = [1, np.nan, np.nan, 5, 5, 3, np.nan, np.nan, np.nan, 4, 2, np.nan, np.nan, np.nan, np.nan, 4, np.nan, 5, 4, 1, np.nan]\nratings_item_j = [np.nan, np.nan, 4, 2, 5, np.nan, np.nan, 1, 2, 5, np.nan, np.nan, 2, np.nan, np.nan, 3, np.nan, np.nan, np.nan, 5, 4]\n\ndisplay(Markdown(f'Ratings for item $i$:\\n\\n{ratings_item_i}'))\ndisplay(Markdown(f'Ratings for item $j$:\\n\\n{ratings_item_j}'))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nRatings for item $i$:\n\n[1, nan, nan, 5, 5, 3, nan, nan, nan, 4, 2, nan, nan, nan, nan, 4, nan, 5, 4, 1, nan]\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nRatings for item $j$:\n\n[nan, nan, 4, 2, 5, nan, nan, 1, 2, 5, nan, nan, 2, nan, nan, 3, nan, nan, nan, 5, 4]\n:::\n:::\n\n\n---\n\n::: {.content-visible when-profile=\"slides\"}\n![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n:::\n\nLet's drop the non-common ratings:\n\n::: {#fb38dfd8 .cell execution_count=10}\n``` {.python .cell-code}\n# Create new lists where only numbers are kept that are not np.nan in both lists\nfiltered_ratings_item_i = [rating_i for rating_i, rating_j in zip(ratings_item_i, ratings_item_j) if not np.isnan(rating_i) and not np.isnan(rating_j)]\nfiltered_ratings_item_j = [rating_j for rating_i, rating_j in zip(ratings_item_i, ratings_item_j) if not np.isnan(rating_i) and not np.isnan(rating_j)]\n\ndisplay(Markdown(f'Common ratings for item $i$: {filtered_ratings_item_i}'))\ndisplay(Markdown(f'Common ratings for item $j$: {filtered_ratings_item_j}'))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nCommon ratings for item $i$: [5, 5, 4, 4, 1]\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nCommon ratings for item $j$: [2, 5, 5, 3, 5]\n:::\n:::\n\n\nNow we can compute the Pearson correlation coefficient:\n\n::: {#2c96b1b5 .cell execution_count=11}\n``` {.python .cell-code}\ndisplay(Markdown(f'Pearson correlation coefficient: {np.corrcoef(filtered_ratings_item_i, filtered_ratings_item_j)[0, 1]:0.2f}'))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\nPearson correlation coefficient: -0.43\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n## Pearson Correlation Coefficient\n\nThe Pearson correlation coefficient, often denoted as $r$, is a measure of the\nlinear correlation between two variables $X$ and $Y$. It quantifies the degree\nto which a linear relationship exists between the variables. The value of $r$\nranges from -1 to 1, where:\n\n- $r = 1$ indicates a perfect positive linear relationship,\n- $r = -1$ indicates a perfect negative linear relationship,\n- $r = 0$ indicates no linear relationship.\n\nThe formula for the Pearson correlation coefficient is:\n\n$$\nr = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\n$$\n\nWhere:\n- $X_i$ and $Y_i$ are the individual sample points,\n- $\\bar{X}$ and $\\bar{Y}$ are the means of the $X$ and $Y$ samples, respectively.\n\nThe Pearson correlation coefficient is sensitive to outliers and assumes that the\nrelationship between the variables is linear and that the data is normally distributed.\n:::\n\n## Similarity for Binary Data\n\nIn some cases we will need to work with binary $r_{ui}$.  \n\nFor example, purchase histories on an e-commerce site, or clicks on an ad.\n\nIn this case, an appropriate replacement for Pearson $r$ is the \n**Jaccard similarity coefficient** or **Intersection over Union**.\n\n$$\nJ_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n$$\n\nSee the lecture on [similarity measures](05-Distances-Timeseries.qmd#norms).\n\n## Improving CF in the presence of bias\n\nOne problem with the story so far arises due to __bias__.\n\n* Some items are significantly higher or lower rated\n* Some users rate substantially higher or lower in general\n\nThese properties interfere with similarity assessment.  \n\nBias correction is crucial for CF recommender systems.\n\nWe need to include\n\n* Per-user offset\n* Per-item offset\n* Global offset\n\n## Representing biases\n\nHence we need to form a per-item bias of:\n    \n$$ \nb_{ui} = \\mu + \\alpha_u + \\beta_i \n$$\n\nwhere \n\n* $\\mu$ is the global average rating across all items and users.\n* $\\alpha_u$ is the offset of user $u$ and \n* $\\beta_i$ is the offset of item $i$.\n\nIf we gather all these elements together we can form: \n\n* $\\boldsymbol{\\alpha}$ an $n\\times 1$ vector of per-user offsets, and\n* $\\boldsymbol{\\beta}$ an $m\\times 1$ vector of per-item offsets.\n\n\n## Estimating biases\n\n::: {.content-visible when-profile=\"slides\"}\n$$ \nb_{ui} = \\mu + \\alpha_u + \\beta_i \n$$\n:::\n\nHow can we estimate the parameters $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\beta}$, and $\\mu$?\n\nLet's assume for a minute that we had a fully-dense matrix of ratings $R$.\n\nEach of the $m$ rows of $R$ represents an item and each of the $n$ columns a user.\n\nThen what we want to estimate is\n\n$$\n\\min_{\\alpha,\\beta,\\mu} \n\\Vert R - \\mathbf{1}\\alpha^T + \\beta\\mathbf{1}^T + \\mu1\\Vert^2 \n+ \\lambda(\\Vert\\alpha\\Vert^2 + \\Vert\\beta\\Vert^2).\n$$\n\nHere, bold-faced $\\mathbf{1}$ represents appropriately sized vectors of ones,\nand non-boldfaced $1$ is an $m\\times n$ matrix of ones.\n\n---\n\n::: {.content-visible when-profile=\"slides\"}\n$$\n\\min_{\\alpha,\\beta,\\mu} \n\\Vert R - \\mathbf{1}\\alpha^T + \\beta\\mathbf{1}^T + \\mu1\\Vert^2 \n+ \\lambda(\\Vert\\alpha\\Vert^2 + \\Vert\\beta\\Vert^2) \n$$\n:::\n\nWhile this is not a simple ordinary least squares problem, there is a strategy for solving it.\n\nAssume we hold $\\beta\\mathbf{1}^T + \\mu1$ constant.  \n\nThen the remaining problem is \n\n$$\n\\min_{\\alpha} \\Vert R - \\mathbf{1}\\alpha^T \\Vert^2 + \\lambda \\Vert\\alpha\\Vert^2,\n$$\n\nwhich (for each column of $R$) is a standard regularized least squares problem\nsolved via [Ridge regression](./19-Regression-III-More-Linear.qmd#ridge-regression).\n\n---\n\nThis sort of problem is called __jointly convex__ in that it is convex in each of the variables $\\alpha$, $\\beta$, and $\\mu$.\n\nThe strategy for solving is:\n    \n1. Hold $\\alpha$ and $\\beta$ constant, solve for $\\mu$.\n2. Hold $\\alpha$ and $\\mu$ constant, solve for $\\beta$.\n3. Hold $\\beta$ and $\\mu$ constant, solve for $\\alpha$.\n\nEach of the three steps will reduce the overall error.  As a result, we iterate over them until convergence.\n\n---\n\nThe last issue is that the matrix $R$ is not dense - in reality we only have a small subset of its entries.\n\nWe simply need to adapt the least-squares solution to only consider the entries in $R$ that we know.\n\n---\n\nAs a result, the actual calculation is:\n\nStep 1:\n\n$$\n\\mu = \\frac{\\sum_{(u, i) \\in R} (r_{ui} - \\alpha_u - \\beta_i)}{|R|} \n$$\n\nStep 2: \n\n$$ \n\\alpha_u = \\frac{\\sum_{i \\in R(u)}(r_{ui} - \\mu - \\beta_i)}{\\lambda + |R(u)|} \n$$\n\nStep 3:\n    \n$$ \n\\beta_i = \\frac{\\sum_{u \\in R(i)}(r_{ui} - \\mu - \\alpha_u)}{\\lambda + |R(i)|} \n$$\n\nStep 4: If not converged, go to Step 1.\n\nHere $i \\in R(u)$ means the set of items rated by user $u$ and $u \\in R(i)$ means\nthe set of users who have rated item $i$ and $|R(u)|$ is the number of ratings.\n\n---\n\nNow that we have learned the biases, we can do a better job of estimating correlation:\n\n$$ \n\\hat{\\rho}_{ij} = \\frac{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})(r_{uj}-b_{uj})} \n{\\sqrt{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})^2\\sum_{u\\in U(i,j)}(r_{uj}-b_{uj})^2}},\n$$\n\nwhere \n\n* $b_{ui} = \\mu + \\alpha_u + \\beta_i$, and\n* $U(i,j)$ are the users who have rated both $i$ and $j$.\n\n---\n\nAnd using biases we can also do a better job of estimating ratings:\n\n$$ \n\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{j \\in n_k(i, u)} s_{ij}(r_{uj} - b_{uj})}{\\sum_{j \\in n_k(i, u)} s_{ij}},\n$$\n\nwhere \n\n* $b_{ui} = \\mu + \\alpha_u + \\beta_i$,\n* $n_k(i, u)$ are the $k$ nearest neighbors to $i$ that were rated by user $u$ and\n* $s_{ij}$ is the similarity between items $i$ and $j$, estimated as above.\n\n## Assessing CF\n\nThis completes the high level view of CF.\n\nWorking with user-user similarities is analogous.\n\nStrengths:\n\n* Essentially no training.\n    * The reliance on $k$-nearest neighbors helps in this respect.\n* Easy to update with new users, items, and ratings.\n* Explainable: \n    * \"We recommend _Minority Report_ because you liked _Blade Runner_ and _Total Recall._\"\n\nWeaknesses:\n\n* Accuracy can be a problem -- resulting in poor recommendations\n* Scalability can be a problem -- compute grows (think $k$-NN)\n\n# Matrix Factorization\n\n## Matrix Factorization\n\nNote that standard CF forces us to consider similarity among items, __or__ among\nusers, but does not take into account __both.__\n\nCan we use both kinds of similarity simultaneously?\n\nWe can't use both the rows and columns of the ratings matrix $R$ at the same\ntime -- the user and item vectors live in different vector spaces.\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/L10-Movie-Latent-Space.png){width=\"100%\" fig-align=\"center\" #fig-movie-latent-space}\n\n[@koren2009matrix]\n:::\n::: {.column width=\"50%\"}\nWhat we could try to do is find a __single__ vector space in which we represent\n__both__ users __and__ items, along with a similarity function, such that:\n\n* users who have similar item ratings are similar in the vector space\n* items who have similar user ratings are similar in the vector space\n* when a given user highly rates a given item, that user and item are similar in the vector space.\n:::\n::::\n\n:::: {.fragment}\nWe saw this idea previously, in an SVD lecture.\n\nThis new vector space is called a __latent__ space, and the user and item\nrepresentations are called __latent vectors.__\n\nThis notion of a shared latent space is also central to deep learning \nrecommender approaches [@naumov2019deep] we will look at later.\n::::\n\n---\n\nNow, however, we are working with a matrix which is only __partially observed.__\nThat is, we only know __some__ of the entries in the ratings matrix.\n\nNonetheless, we can imagine a situation like this:\n\n![](figs/L20-mf-1.png){.lightbox width=\"50%\" fig-align=\"center\"}\n\nwhere we decompose the ratings matrix $R$ into two matrices.\n\nWe want the product of the two matrices to be as close as possible \n__to the known values__ of the ratings matrix.\n\n---\n\nWhat this setup implies is that our similarity function is the __inner product.__\n\nWhich means that to predict an unknown rating, we take the __inner product of \nlatent vectors:__\n\n![](figs/L20-mf-2.png){width=\"60%\" fig-align=\"center\"}\n\nTaking, for example, the 2nd row of \"items\" and the 5th row of \"users\"...\n\n---\n\nWe have\n\n$$\n(-0.5 \\cdot -2)+(0.6 \\cdot 0.3)+(0.5 \\cdot 2.4) = 2.43,\n$$\n\nso:\n\n![](figs/L20-mf-3.png){width=\"60%\" fig-align=\"center\"}\n\n## Solving Matrix Factorization\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n![](figs/L20-mf-1.png){.lightbox width=\"100%\" fig-align=\"center\"}\n:::\n::: {.column width=\"60%\"}\nNotice that in this case we've decided that the factorization should be rank 3,\ni.e., low-rank.\n\nSo we want something like an SVD.\n\n(Recall that SVD gives us the most-accurate-possible low-rank factorization of a matrix).\n:::\n::::\n\nHowever, we can't use the SVD algorithm directly, because we don't know all the entries in $R$. \n\n> Indeed, the unseen entries in $R$ are exactly what we want to predict.\n\n---\n\nHere is what we want to solve: \n    \n$$\n\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n$$\n\nwhere:\n\n* $R$ is $m\\times n$, \n* $U$ is the $m\\times k$ items matrix,\n* $V$ is the $n\\times k$ users matrix and\n* $k$ is the rank of the factorization and dimensionality of the latent space.\n\nThe $(\\cdot)_S$ notation means that we are only considering the _subset_ of\nmatrix entries that correspond to known reviews (the set $S$).\n\nNote that as usual, we add $\\ell_2$ penalization to avoid overfitting\n([Ridge regression](./19-Regression-III-More-Linear.qmd#ridge-regression)).\n\n---\n\n::: {.content-visible when-profile=\"slides\"}\n$$\n\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n$$\n:::\n\nOnce again, this problem is __jointly convex__ in that it is convex in each of the variables $U$ and $V$.\n\nIn particular, if we hold either $U$ or $V$ constant, then the result is a simple\nridge regression.\n\n---\n\n::: {.content-visible when-profile=\"slides\"}\n$$\n\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n$$\n:::\n\n\nSo one commonly used algorithm for this problem is called __alternating least squares (ALS):__\n    \n1. Hold $U$ constant, and solve for $V$\n2. Hold $V$ constant, and solve for $U$\n3. If not converged, go to Step 1.\n\nThe only thing we've left out at this point is how to deal with the missing entries of $R$.  \n\nIt's not hard, but the details aren't that interesting, so we'll give you code instead!\n\n## ALS in Practice\n\nThe entire Amazon reviews dataset is too large to work with easily, and it is too sparse. \n\nHence, we will take the densest rows and columns of the matrix.\n\n::: {#3dad1920 .cell execution_count=12}\n``` {.python .cell-code}\n# The densest columns: products with more than 50 reviews\npids = df.groupby('ProductId').count()['Id']\nhi_pids = pids[pids > 50].index\n\n# reviews that are for these products\nhi_pid_rec = [r in hi_pids for r in df['ProductId']]\n\n# the densest rows: users with more than 50 reviews\nuids = df.groupby('UserId').count()['Id']\nhi_uids = uids[uids > 50].index\n\n# reviews that are from these users\nhi_uid_rec = [r in hi_uids for r in df['UserId']]\n\n# The result is a list of booleans equal to the number of rewviews\n# that are from those dense users and movies\ngoodrec = [a and b for a, b in zip(hi_uid_rec, hi_pid_rec)]\n```\n:::\n\n\nNow we create a $\\textnormal{UserID} \\times \\textnormal{ProductID}$ matrix from these reviews.\n\nMissing entries will be filled with NaNs.\n\n::: {#d6245f9f .cell execution_count=13}\n``` {.python .cell-code}\ndense_df = df.loc[goodrec]\ngood_df = dense_df.loc[~df['Score'].isnull()]\nR = good_df.pivot_table(columns = 'ProductId', index = 'UserId', values = 'Score')\n```\n:::\n\n\n---\n\nAnd we can look at a small part of the matrix:\n\n::: {#f356f9f1 .cell execution_count=14}\n``` {.python .cell-code}\nR.iloc[900:910, 1000:1010]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ProductId</th>\n      <th>1417024917</th>\n      <th>1417030321</th>\n      <th>1417030976</th>\n      <th>1417054069</th>\n      <th>1417065818</th>\n      <th>1417068329</th>\n      <th>1417070471</th>\n      <th>1419802356</th>\n      <th>1419807587</th>\n      <th>1419815644</th>\n    </tr>\n    <tr>\n      <th>UserId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A1WLZYEOIL1HLT</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WNJVA59HLMO5</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WR12AC35R3K6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WSFHRBY2ZD1R</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WUMTJOASEL5F</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WVA7V02PQOY6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1WX42M589VAMQ</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1X054KUYG5V</th>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1X12F0KC8GY6M</th>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>A1X15AQVSCKKRG</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nWe'll use code from the [Antidote Data Framework](https://github.com/rastegarpanah/antidote-data-framework) to do the matrix factorization and ALS. We have local copies \n[recommender_MF.py](recommender_MF.py), [recommender_als.py](recommender_als.py)\nand [recommender_lmafit.py](recommender_lmafit.py) in our repository.\n\n::: {#f391e434 .cell execution_count=15}\n``` {.python .cell-code}\n# Import local python package MF.py\nimport recommender_MF as MF\n\n# Instantiate the model\n# We are pulling these hyperparameters out of the air -- that's not the right way to do it!\nRS = MF.als_MF(rank = 20, lambda_ = 1)\n```\n:::\n\n\n::: {#12cc682f .cell execution_count=16}\n``` {.python .cell-code}\n%time pred, error = RS.fit_model(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 31.3 s, sys: 1.58 s, total: 32.9 s\nWall time: 32.8 s\n```\n:::\n:::\n\n\n::: {#0928a8eb .cell execution_count=17}\n``` {.python .cell-code}\nprint(f'RMSE on visible entries (training data): {np.sqrt(error/R.count().sum()):0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE on visible entries (training data): 0.343\n```\n:::\n:::\n\n\nAnd we can look at the predicted ratings matrix and see that it is a dense matrix:\n\n::: {#13f9b639 .cell execution_count=18}\n``` {.python .cell-code}\npred\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ProductId</th>\n      <th>0005019281</th>\n      <th>0005119367</th>\n      <th>0307142485</th>\n      <th>0307142493</th>\n      <th>0307514161</th>\n      <th>0310263662</th>\n      <th>0310274281</th>\n      <th>0718000315</th>\n      <th>0764001035</th>\n      <th>0764003828</th>\n      <th>...</th>\n      <th>B00IKM5OCO</th>\n      <th>B00IWULQQ2</th>\n      <th>B00J4LMHMK</th>\n      <th>B00J5JSV1W</th>\n      <th>B00JA3RPAG</th>\n      <th>B00JAQJMJ0</th>\n      <th>B00JBBJJ24</th>\n      <th>B00JKPHUE0</th>\n      <th>B00K2CHVJ4</th>\n      <th>B00L4IDS4W</th>\n    </tr>\n    <tr>\n      <th>UserId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A02755422E9NI29TCQ5W3</th>\n      <td>4.901128</td>\n      <td>5.101293</td>\n      <td>5.124230</td>\n      <td>5.167632</td>\n      <td>5.264014</td>\n      <td>5.509682</td>\n      <td>5.019623</td>\n      <td>4.630935</td>\n      <td>4.324269</td>\n      <td>5.410814</td>\n      <td>...</td>\n      <td>3.931071</td>\n      <td>5.075952</td>\n      <td>5.371094</td>\n      <td>2.029338</td>\n      <td>4.793952</td>\n      <td>5.751275</td>\n      <td>5.055573</td>\n      <td>4.245653</td>\n      <td>3.910609</td>\n      <td>5.015350</td>\n    </tr>\n    <tr>\n      <th>A100JCBNALJFAW</th>\n      <td>4.031142</td>\n      <td>4.462234</td>\n      <td>3.075861</td>\n      <td>3.804829</td>\n      <td>4.471864</td>\n      <td>3.809544</td>\n      <td>4.381435</td>\n      <td>2.979852</td>\n      <td>3.261133</td>\n      <td>3.253135</td>\n      <td>...</td>\n      <td>3.127049</td>\n      <td>4.169933</td>\n      <td>1.678929</td>\n      <td>2.055047</td>\n      <td>2.772564</td>\n      <td>3.706830</td>\n      <td>3.107731</td>\n      <td>3.654849</td>\n      <td>1.916827</td>\n      <td>2.760760</td>\n    </tr>\n    <tr>\n      <th>A10175AMUHOQC4</th>\n      <td>4.647358</td>\n      <td>4.640926</td>\n      <td>4.548000</td>\n      <td>5.478397</td>\n      <td>5.087498</td>\n      <td>4.276774</td>\n      <td>4.729792</td>\n      <td>4.577080</td>\n      <td>5.469803</td>\n      <td>5.659700</td>\n      <td>...</td>\n      <td>4.183990</td>\n      <td>5.158620</td>\n      <td>5.547430</td>\n      <td>2.022941</td>\n      <td>3.922144</td>\n      <td>5.513525</td>\n      <td>4.721612</td>\n      <td>4.606729</td>\n      <td>3.861235</td>\n      <td>3.855086</td>\n    </tr>\n    <tr>\n      <th>A103KNDW8GN92L</th>\n      <td>4.601196</td>\n      <td>4.808000</td>\n      <td>4.028050</td>\n      <td>4.047650</td>\n      <td>3.320891</td>\n      <td>3.779980</td>\n      <td>4.328358</td>\n      <td>3.558697</td>\n      <td>3.458177</td>\n      <td>6.051367</td>\n      <td>...</td>\n      <td>3.377724</td>\n      <td>4.782034</td>\n      <td>4.561805</td>\n      <td>1.705925</td>\n      <td>4.585235</td>\n      <td>3.074747</td>\n      <td>3.597283</td>\n      <td>2.487920</td>\n      <td>3.741830</td>\n      <td>5.180065</td>\n    </tr>\n    <tr>\n      <th>A106016KSI0YQ</th>\n      <td>3.940547</td>\n      <td>3.758841</td>\n      <td>4.443953</td>\n      <td>4.960247</td>\n      <td>3.905060</td>\n      <td>3.533902</td>\n      <td>3.875516</td>\n      <td>3.851135</td>\n      <td>3.154589</td>\n      <td>4.576446</td>\n      <td>...</td>\n      <td>3.684078</td>\n      <td>4.510247</td>\n      <td>3.135226</td>\n      <td>1.913635</td>\n      <td>3.372981</td>\n      <td>3.755371</td>\n      <td>3.776432</td>\n      <td>3.224742</td>\n      <td>2.587647</td>\n      <td>2.744543</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>AZUBX0AYYNTFF</th>\n      <td>3.623108</td>\n      <td>4.195799</td>\n      <td>3.259184</td>\n      <td>4.362600</td>\n      <td>4.275729</td>\n      <td>3.316542</td>\n      <td>3.962883</td>\n      <td>3.308387</td>\n      <td>3.276024</td>\n      <td>4.812006</td>\n      <td>...</td>\n      <td>3.394854</td>\n      <td>3.926255</td>\n      <td>2.921070</td>\n      <td>1.338711</td>\n      <td>2.495747</td>\n      <td>4.264773</td>\n      <td>3.650783</td>\n      <td>2.810611</td>\n      <td>2.131493</td>\n      <td>3.223099</td>\n    </tr>\n    <tr>\n      <th>AZXGPM8EKSHE9</th>\n      <td>2.801357</td>\n      <td>3.712006</td>\n      <td>2.946317</td>\n      <td>3.989053</td>\n      <td>3.037111</td>\n      <td>4.899051</td>\n      <td>3.366558</td>\n      <td>3.124647</td>\n      <td>3.368960</td>\n      <td>4.354287</td>\n      <td>...</td>\n      <td>2.191798</td>\n      <td>3.152695</td>\n      <td>4.557602</td>\n      <td>0.832827</td>\n      <td>4.441145</td>\n      <td>3.403507</td>\n      <td>3.395606</td>\n      <td>3.397682</td>\n      <td>2.572013</td>\n      <td>3.865427</td>\n    </tr>\n    <tr>\n      <th>AZXHK8IO25FL6</th>\n      <td>3.206185</td>\n      <td>3.427266</td>\n      <td>3.533580</td>\n      <td>3.978738</td>\n      <td>3.569193</td>\n      <td>1.279516</td>\n      <td>3.312234</td>\n      <td>2.314085</td>\n      <td>3.451623</td>\n      <td>3.280554</td>\n      <td>...</td>\n      <td>2.218830</td>\n      <td>3.583831</td>\n      <td>1.944980</td>\n      <td>0.815382</td>\n      <td>3.028860</td>\n      <td>3.740974</td>\n      <td>1.404925</td>\n      <td>2.415937</td>\n      <td>-0.518910</td>\n      <td>1.556146</td>\n    </tr>\n    <tr>\n      <th>AZXR5HB99P936</th>\n      <td>4.110834</td>\n      <td>4.150344</td>\n      <td>3.031901</td>\n      <td>3.782407</td>\n      <td>4.470900</td>\n      <td>3.631913</td>\n      <td>4.119725</td>\n      <td>3.534826</td>\n      <td>3.758144</td>\n      <td>3.211057</td>\n      <td>...</td>\n      <td>3.240027</td>\n      <td>4.153761</td>\n      <td>2.854006</td>\n      <td>2.568058</td>\n      <td>2.882833</td>\n      <td>3.732649</td>\n      <td>3.244162</td>\n      <td>3.111499</td>\n      <td>2.400912</td>\n      <td>2.676788</td>\n    </tr>\n    <tr>\n      <th>AZZ4GD20C58ND</th>\n      <td>4.674770</td>\n      <td>4.905721</td>\n      <td>3.781878</td>\n      <td>3.892328</td>\n      <td>4.420746</td>\n      <td>4.790422</td>\n      <td>4.576695</td>\n      <td>3.470988</td>\n      <td>4.198702</td>\n      <td>3.515578</td>\n      <td>...</td>\n      <td>3.000951</td>\n      <td>4.495355</td>\n      <td>3.190370</td>\n      <td>2.837386</td>\n      <td>4.104762</td>\n      <td>3.374785</td>\n      <td>3.384070</td>\n      <td>4.392084</td>\n      <td>2.654402</td>\n      <td>3.320654</td>\n    </tr>\n  </tbody>\n</table>\n<p>3677 rows × 7244 columns</p>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: {#80c75a4f .cell execution_count=19}\n``` {.python .cell-code}\n## todo: hold out test data, compute oos error\n\n# We create a mask of the known entries, then calculate the indices of the known\n# entries, then split that data into training and test sets.\n\n# Create a mask for the known entries\nRN = ~R.isnull()\n\n# Get the indices of the known entries\nvisible = np.where(RN)\n\n# Split the data into training and test sets\nimport sklearn.model_selection as model_selection\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(visible[0], visible[1], test_size = 0.1)\n```\n:::\n\n\nJust for comparison's sake, let's check the performance of $k$-NN on this dataset.\n\nAgain, this is only on the training data -- so overly optimistic for sure.\n\nAnd note that this is a subset of the full dataset -- the subset that is \"easiest\" to predict due to density.\n\n::: {#3eac8097 .cell execution_count=20}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error\n\n# Drop the columns that are not features\nX_train = good_df.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n\n# The target is the score\ny_train = good_df['Score']\n\n# Using k-NN on features HelpfulnessNumerator, HelpfulnessDenominator, Score, Time\nmodel = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n%time y_hat = model.predict(X_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 2.4 s, sys: 44.9 ms, total: 2.45 s\nWall time: 2.42 s\n```\n:::\n:::\n\n\n::: {#29e2c027 .cell execution_count=21}\n``` {.python .cell-code}\nprint(f'RMSE on visible entries (test set): {np.sqrt(mean_squared_error(y_train, y_hat)):.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE on visible entries (test set): 0.649\n```\n:::\n:::\n\n\n## Assessing Matrix Factorization\n\nMatrix Factorization per se is a good idea.    \nHowever, many of the improvements we've discussed for CF apply to MF as well.\n\nTo illustrate, we'll look at some of the successive improvements used by the\nteam that won the Netflix prize (\"BellKor's Pragmatic Chaos\").\n\nWhen the prize was announced, the Netflix supplied solution achieved an RMSE of 0.951.\n\nBy the end of the competition (about 3 years), the winning team's solution achieved RMSE of 0.856.\n\nLet's restate our MF objective in a way that will make things clearer:\n\n$$\n\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - u_u^Tv_i)^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n$$\n\nwhere we have written out the vector $\\ell_2$ norm as the summation.\n\n## 1. Adding Biases\n\nIf we add biases:\n$$ \n\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u + \\beta_i + u_u^Tv_i)^2 \n+ \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) \n$$\n\nwe see improvements in accuracy:\n\n![Matrix factorization models’ accuracy. The plots show the root-mean-square error of each of four individual factor models (lower is better). Accuracy improves when the factor model’s dimensionality (denoted by numbers on the charts) increases. In addition, the more refined factor models, whose descriptions involve more distinct sets of parameters, are more accurate.](figs/L20-netflix-1.png){width=\"70%\" fig-align=\"center\" #fig-mf}\n\n## 2. Who Rated What?\n\nIn reality, ratings are not provided __at random.__\n\nTake note of which users rated the same movies (ala CF) and use this information.\n\n![](figs/L20-netflix-2.png){width=\"70%\" fig-align=\"center\"}\n\n![](figs/L20-netflix-3.png){width=\"70%\" fig-align=\"center\"}\n\n## 3. Ratings Change Over Time\n\nOlder movies tend to get higher ratings!\n\n![](figs/L20-netflix-4.png){width=\"70%\" fig-align=\"center\"}\n\n---\n\nIf we add time-varying biases:\n\n$$\n\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u(t) + \\beta_i(t) + u_u^Tv_i(t))^2 \n+ \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) \n$$\n\nwe see further improvements in accuracy:\n\n![](figs/L20-netflix-5.png){width=\"70%\" fig-align=\"center\"}\n\nTo estimate these billions of parameters, we cannot use alternating least squares or any linear algebraic method.\n\nWe need to use gradient descent (which we covered previously).\n\n# Deep Learning for Recommender Systems\n\n## Deep Learning for Recommender Systems\n\nBesides the Collaborative Filtering and Matrix Factorization models, other popular\napproaches to building recommender systems use Deep Learning.\n\nWe'll look at the Deep Learning Recommender Model (DLRM) proposed by Facebook in\n2019 [@naumov2019deep] with [GitHub repository](https://github.com/facebookresearch/dlrm).\n\n## DLRM Architecture\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- Components (@fig-dlrm-model):\n  1. **Embeddings**: Dense representations for categorical data.\n  2. **Bottom MLP**: Transforms dense continuous features.\n  3. **Feature Interaction**: Dot-product of embeddings and dense features.\n  4. **Top MLP**: Processes interactions and outputs probabilities.\n\n:::\n::: {.column width=\"50%\"}\n\n![DLRM Architecture](figs/RecSys-figs/dlrm-model.png){.lightbox width=80% fig-align=\"center\" #fig-dlrm-model}\n\n:::\n::::\n\nLet's look at each of these components in turn.\n\n## Embeddings\n\n**Embeddings**: Map categorical inputs to latent factor space.\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n- A learned embedding matrix $W \\in \\mathbb{R}^{m \\times d}$ for each category of input\n- One-hot vector $e_i$ with $i\\text{-th}$ entry 1 and rest are 0s\n- Embedding of $e_i$ is $i\\text{-th}$ row of $W$, i.e., $w_i^T = e_i^T W$\n\nWe can also use weighted combination of multiple items with a multi-hot vector\nof weights $a^T = [0, ..., a_{i_1}, ..., a_{i_k}, ..., 0]$.\n\nThe embedding of this multi-hot vector is then $a^T W$.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model01.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n---\n\nPyTorch has a convenient way to do this using `EmbeddingBag`, which besides summing\ncan combine embeddings via mean or max pooling.\n\nHere's an example with 5 embeddings of dimension 3:\n\n::: {#35f87f9c .cell execution_count=22}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# Example embedding matrix: 5 embeddings, each of dimension 3\nembedding_matrix = nn.EmbeddingBag(num_embeddings=5, embedding_dim=3, mode='mean')\n\n# Input: Indices into the embedding matrix\ninput_indices = torch.tensor([1, 2, 3, 4])  # Flat list of indices\noffsets = torch.tensor([0, 2])  # Start new bag at position 0 and 2 in input_indices\n\n# Forward pass\noutput = embedding_matrix(input_indices, offsets)\n\nprint(\"Embedding Matrix:\\n\", embedding_matrix.weight)\nprint(\"Output:\\n\", output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding Matrix:\n Parameter containing:\ntensor([[ 0.9878,  0.7353,  0.5594],\n        [ 0.5943,  0.0403,  0.6281],\n        [ 1.1241,  1.1920, -1.2281],\n        [-1.7778,  1.5219, -0.6562],\n        [-0.6574, -2.2826, -0.4094]], requires_grad=True)\nOutput:\n tensor([[ 0.8592,  0.6161, -0.3000],\n        [-1.2176, -0.3803, -0.5328]], grad_fn=<EmbeddingBagBackward0>)\n```\n:::\n:::\n\n\n## Dense Features\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe advantage of the DLRM architecture is that it can take continuous features\nas input such as the user's age, time of day, etc.\n\nThere is a bottom MLP that transforms these dense features into a latent space of\nthe same dimension $d$.\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model02.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Optional Sparse Feature MLPs\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nOptionally, one can add MLPs to transform the sparse features as well.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model03.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Feature Interactions\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe 2nd order interactions are modeled via dot-products of all pairs from the\ncollections of embedding vectors and processed dense features.\n\nThe results of the dot-product interactions are concatenated with the processed\ndense vectors.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model04.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Top MLP\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe concatenated vector is then passed to a final MLP and then to a sigmoid\nfunction to produce the final prediction (e.g., probability score of recommendation)\n\nThis entire model is trained end-to-end using standard deep learning techniques.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model05.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Training Results\n\n![DLRM Training Results](figs/RecSys-figs/dlrm-training-results.png){width=\"70%\" fig-align=\"center\" #fig-dlrm-training-results}\n\n@fig-dlrm-training-results shows the training (solid) and validation (dashed)\naccuracies of DLRM on the [Criteo Ad Kaggle dataset](https://www.kaggle.com/competitions/criteo-display-ad-challenge/overview).\n\nAccuracy is compared with Deep and Cross network (DCN) [@wang2017deep].\n\n## Other Modern Approaches\n\nThere are many other modern approaches to recommender systems for example:\n\n1. **Graph-Based Recommender Systems**:\n   - Leverage graph structures to capture relationships between users and items.\n   - Use techniques like Graph Neural Networks (GNNs) to enhance recommendation accuracy.\n\n2. **Context-Aware Recommender Systems**:\n   - Incorporate contextual information such as time, location, and user mood to provide more personalized recommendations.\n   - Contextual data can be integrated using various machine learning models.\n\n3. **Hybrid Recommender Systems**:\n   - Combine multiple recommendation techniques, such as collaborative filtering and content-based filtering, to improve performance.\n   - Aim to leverage the strengths of different methods while mitigating their weaknesses.\n\n4. **Reinforcement Learning-Based Recommender Systems**:\n   - Use reinforcement learning to optimize long-term user engagement and satisfaction.\n   - Models learn to make sequential recommendations by interacting with users and receiving feedback.\n\nThese approaches often leverage advancements in machine learning and data processing to provide more accurate and personalized recommendations.\n\nSee [@ricci2022recommender] for a comprehensive overview of recommender systems.\n\n# Assessing Recommender Systems\n\n## Assessing Recommender Systems\n\nThere are a number of concerns with the widespread use of recommender systems and personalization in society.\n\nFirst, recommender systems are accused of creating __filter bubbles.__ \n\nA filter bubble is the tendency for recommender systems to limit the variety of information presented to the user.\n\nThe concern is that a user's past expression of interests will guide the algorithm in continuing to provide \"more of the same.\"\n\nThis is believed to increase polarization in society, and to reinforce confirmation bias.\n\n---\n\nSecond, recommender systems in modern usage are often tuned to __maximize engagement.__\n\nIn other words, the objective function of the system is not to present the user's most favored content, but rather the content that will be most likely to keep the user on the site.\n\nThe incentive to maximize engagement arises on sites that are supported by advertising revenue.   \n\nMore engagement time means more revenue for the site.\n\n---\n\nHowever, many studies have shown that sites that strive to __maximize \nengagement__ do so in large part by guiding users toward __extreme content:__\n\n* content that is shocking, \n* or feeds conspiracy theories, \n* or presents extreme views on popular topics.\n\nGiven this tendency of modern recommender systems, \nfor a third party to create \"clickbait\" content such as this, one of the easiest\nways is to present false claims.\n\nMethods for addressing these issues are being very actively studied at present.\n\nWays of addressing these issues can be:\n\n* via technology\n* via public policy\n\n# Recap and References\n\n## BU CS/CDS Research\n\nYou can read about some of the work done in Professor Mark Crovella's group on\nthis topic:\n\n* _How YouTube Leads Privacy-Seeking Users Away from Reliable Information_, [@spinelli2020youtube] \n* _Closed-Loop Opinion Formation_, [@spinelli2017closed] \n* _Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems_, [@rastegarpanah2019fighting] \n\n## Recap\n\n* Introduction to recommender systems and their importance in modern society.\n* Explanation of collaborative filtering (CF) and its two main approaches: user-user similarity and item-item similarity.\n* Discussion on the challenges of recommender systems, including scalability and data sparsity.\n* Introduction to matrix factorization (MF) as an improvement over CF, using latent vectors and alternating least squares (ALS) for optimization.\n* Practical implementation of ALS for matrix factorization on a subset of Amazon movie reviews.\n* Review of Deep Learning Recommender Model (DLRM) architecture and its components.\n* Discussion on the societal impact of recommender systems, including filter bubbles and engagement maximization.\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "20-Recommender-Systems_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}