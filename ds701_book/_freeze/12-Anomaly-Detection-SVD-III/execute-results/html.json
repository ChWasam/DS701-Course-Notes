{
  "hash": "88ca3e8c771ba2ec9c2c97cd737af699",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Anomaly Detection (and SVD-III)\njupyter: python3\n---\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/12-Anomaly-Detection-SVD-III.ipynb)\n\n::: {#510921fb .cell hide_input='true' quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"skip\"}}' execution_count=1}\n``` {.python .cell-code}\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n# import libraries\nimport numpy as np\nimport matplotlib as mp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport slideUtilities as sl\nimport laUtilities as ut\nfrom importlib import reload\nfrom datetime import datetime\nfrom IPython.display import Image\nfrom IPython.display import display_html\nfrom IPython.display import display\nfrom IPython.display import Math\nfrom IPython.display import Latex\nfrom IPython.display import HTML;\n```\n:::\n\n\nToday we'll discuss an important topic related to unsupervised learning: __anomaly detection.__\n\nAnomalies are objects that are different from most other objects.   \n\nAnomalies are also called \"outliers\".\n\nFurthermore, we usually expect that anomalies are different in a __qualitative__ sense as well.\n\n> An outlier is an observation that differs so much from other observations as to arouse suspicion that it was generated by a different mechanism\n\n-- Douglas Hawkins\n\nWhy might we be interested in anomalies?\n\n* __Fraud Detection__ - stolen credit cards\n* __Intrusion Detection__ - attacks on computer systems\n* __Public Health__ - occurrence of disease in a particular area\n* __Medicine__ - a set of symptoms may indicate a disease\n\nAnomaly detection presents a number of challenges.\n\nIt is an __unsupervised__ method -- so validation is hard\n* It is hard to know that your set of anomalies is correct\n* It is hard to know how many anomalies there are in the data\n\nThe main assumption made in anomaly detection:\n\n__There are many more \"normal\" observations than \"abnormal\" (anomalies) in the data.__\n\nMethodologically, anomaly detection proceeds as follows:\n\n1. Build a profile of \"normal\" data objects\n    * These can be patterns, summary statistics, or more complicated models\n2. Use the \"normal\" profile to detect anomalies\n    * These are observations whose characteristics differ significantly from the normal profile.\n\n## Approaches To Anomaly Detection\n\nThe idea that \"normal behavior is what is most frequently observed\" is the basis for most anomaly detection methods.\n\nIt suggests a number of approaches.\n\n__Model-Based Methods.__\n\nHere, we assume that a model for the data will describe most of the data.   Data points that are not well described by the model are potentially anomalies.\n\n1) Use the data to estimate the parameters of a probability distribution.   For example, one might estimate a normal distribution from the data.\n\n* Then an object that is very __unlikely__ under the model may be an anomaly.\n\n2) Model the data as a set of clusters (cluster the data).\n\n* Then an object that does not strongly belong to any cluster may be an anomaly.\n\n3) Model the data using a regression.\n* Then an object that is far from its predicted value may be an anomaly.\n\n__Other Methods.__\n\nIf you cannot build a model of the data, you can still:\n\n1. Define an anomaly as one that is distant from all (or most) other objects.\n2. Define an anomaly as one that is in an unusually-low-density region.\n\n## Model-Based Detection: 1-D Gaussian\n\nTo start, we will examine a very simple case: anomaly detection in 1-D.\n\n::: {#e92162f0 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=2}\n``` {.python .cell-code}\nn_samples = 100\n\n# construct random samples from a normal distribution\ndata = np.random.randn(n_samples, 1)\n\n# add an outlier\ndata = np.concatenate([data, np.array([[4]])])\n\n# plot\nplt.plot(data, np.zeros(n_samples+1), '.')\n_ = plt.xlim([-4.5, 4.5])\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-3-output-1.png){width=813 height=411}\n:::\n:::\n\n\n::: {#f71d8a78 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=3}\n``` {.python .cell-code}\n# construct MLE estimates of mean and standard deviation\nm_est = np.mean(data)\nstd_est = np.std(data)\n```\n:::\n\n\n::: {#f1850778 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=4}\n``` {.python .cell-code}\n# create a normal model for the data using estimated parameters\nimport scipy.stats\nnorm = scipy.stats.distributions.norm(m_est,std_est)\n```\n:::\n\n\n::: {#0571c009 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=5}\n``` {.python .cell-code}\nplt.plot(data, np.zeros(n_samples+1), '.')\nplt.xlim([-4.5, 4.5])\nplt.ylim([-0.1,0.4])\nx = np.linspace(norm.ppf(0.01),\n                 norm.ppf(0.99), 100)\nplt.plot(x, norm.pdf(x), '-')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-6-output-1.png){width=805 height=416}\n:::\n:::\n\n\nLet's calculate the probability desnity at each data point under this model.\n\n::: {#62ef4349 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=6}\n``` {.python .cell-code}\nprob_data = [norm.pdf(x) for x in data]\nprob_data\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[array([0.28826542]),\n array([0.30774426]),\n array([0.28749629]),\n array([0.20001721]),\n array([0.17930111]),\n array([0.35774878]),\n array([0.35184951]),\n array([0.3625853]),\n array([0.34840118]),\n array([0.22681693]),\n array([0.06182424]),\n array([0.35874217]),\n array([0.3182342]),\n array([0.35004261]),\n array([0.15373227]),\n array([0.36297966]),\n array([0.35531822]),\n array([0.02874462]),\n array([0.04161106]),\n array([0.34483461]),\n array([0.36306581]),\n array([0.27892885]),\n array([0.29758485]),\n array([0.30439859]),\n array([0.2444764]),\n array([0.14755215]),\n array([0.12141304]),\n array([0.07800224]),\n array([0.36011573]),\n array([0.36040118]),\n array([0.11785892]),\n array([0.31365198]),\n array([0.36007627]),\n array([0.3296946]),\n array([0.29165218]),\n array([0.32149531]),\n array([0.06758685]),\n array([0.2350988]),\n array([0.35855]),\n array([0.21947989]),\n array([0.03960655]),\n array([0.35868991]),\n array([0.23802359]),\n array([0.21600715]),\n array([0.3423092]),\n array([0.35991475]),\n array([0.36170246]),\n array([0.35044354]),\n array([0.20476901]),\n array([0.24555221]),\n array([0.36103545]),\n array([0.35959375]),\n array([0.36310331]),\n array([0.36223406]),\n array([0.24530102]),\n array([0.23515218]),\n array([0.34638763]),\n array([0.05210554]),\n array([0.36051625]),\n array([0.34851491]),\n array([0.35260205]),\n array([0.34466398]),\n array([0.2644555]),\n array([0.32703342]),\n array([0.10208113]),\n array([0.3574945]),\n array([0.36328208]),\n array([0.36155117]),\n array([0.12422421]),\n array([0.33562945]),\n array([0.16698207]),\n array([0.35256231]),\n array([0.31443772]),\n array([0.34687896]),\n array([0.286794]),\n array([0.31069272]),\n array([0.039811]),\n array([0.32620001]),\n array([0.1489611]),\n array([0.2804921]),\n array([0.21290599]),\n array([0.00825428]),\n array([0.19528854]),\n array([0.35171182]),\n array([0.25057473]),\n array([0.20019781]),\n array([0.3493479]),\n array([0.29391799]),\n array([0.34726531]),\n array([0.0353764]),\n array([0.14104907]),\n array([0.23758961]),\n array([0.36171378]),\n array([0.27124508]),\n array([0.35303884]),\n array([0.32193002]),\n array([0.32477496]),\n array([0.36199649]),\n array([0.3624224]),\n array([0.35558877]),\n array([0.0008975])]\n```\n:::\n:::\n\n\nTo make this easier to analyze, let's work with the (negative) log probability of the data:\n\n::: {#96c0cdcf .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=7}\n``` {.python .cell-code}\nimport math\nlog_prob_data = [-math.log(x) for x in prob_data]\nplt.plot(data, log_prob_data, 'o')\n_ = plt.xlim([-4.5,4.5])\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-8-output-1.png){width=781 height=411}\n:::\n:::\n\n\nThis plot makes clear that there is one point that is **very** unlikely under our model.  We would be justified in identifying this as an anomaly.\n\nIn particular, the probability of seeing a point this extreme or worse under this model is 0.0002337.  \n\n::: {#c1556b38 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=8}\n``` {.python .cell-code}\n1 - norm.cdf(data[-1])\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0.00026504])\n```\n:::\n:::\n\n\nWe would not expect to see a sample this extreme unless we sampled the distribution 1/0.0002337 = 4278 times. \n\nHowever we have only 101 data points.\n\n__An important point.__\n\nNotice that we estimated the mean and standard deviation of our model using __all__ the data -- including the point that we later decided was an anomaly.\n\nOf course the correct parameter estimation should __not__ have included the anomalous data point, if it truly \"was generated by a different mechanism.\"\n\nOn one hand, our estimation approach gives us an approximation to the true distribution. \n\nThis approximation may be justified under the assumption that \"most of the data points are not anomalies\".\n\nAnd since we don't know the anomalies in advance (of course) we cannot simply remove them before we estimate the parameters.\n\nThere are more sophisticated approaches to try to address this problem -- we won't discuss them, but you should be aware that it is possible to do better than what we did here.\n\n## Extending to Multiple Dimensions\n\nFor data with multiple features we would like to take the same approach. \n\nWe would like to identify points as outliers if they have low probability under a __multivariate__ Gaussian model.\n\n::: {#216dc797 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=9}\n``` {.python .cell-code}\nn_samples = 1000\ncov = np.array([[1., 0.75],[0.75, 3]])\nmean = np.array([0.,0])\napt1 = np.array([2,8])\napt2 = np.array([-4,4])\ndata = np.random.multivariate_normal(mean, cov, n_samples)\ndata = np.concatenate([data,np.array([apt1,apt2])])\n```\n:::\n\n\n::: {#68c8d1ca .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=10}\n``` {.python .cell-code}\nplt.plot(data[:,0],data[:,1],'o',markersize=3)\nplt.axis('square')\nplt.xlim([-10,10])\n_ = plt.ylim([-10,10])\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-11-output-1.png){width=454 height=416}\n:::\n:::\n\n\n::: {#b83665b6 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=11}\n``` {.python .cell-code}\ntheta = np.linspace(0,2*math.pi,500)\ncoords = np.array([np.sin(theta), np.cos(theta)])\ncov = np.array([[1., 0.75],[0.75, 3]])\nlam, evec = np.linalg.eig(cov)\nc = evec @ np.diag(np.sqrt(lam))\ncoords = 3 * c @ coords\nplt.plot(data[:-2,0],data[:-2,1],'o',markersize=3,color='b')\nplt.plot(data[-2:,0],data[-2:,1],'o',markersize=3,color='r')\nplt.plot(coords[0], coords[1], '-', color='g')\nplt.axis('square')\nplt.xlim([-10,10])\nplt.ylim([-10,10])\n_ = plt.title(r'Contour of constant probability density {:0.4f}'.format(norm.pdf(3)))\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-12-output-1.png){width=454 height=431}\n:::\n:::\n\n\nConsider the following candidates for outliers: (2,8) and (-4,4).\n\nThese are marked in red.\n\n::: {#c63ed5a7 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=12}\n``` {.python .cell-code}\nprint('{} is {:0.3f} from the cluster center.'.format(apt1,np.linalg.norm(apt1)))\nprint('{} is {:0.3f} from the cluster center.'.format(apt2,np.linalg.norm(apt2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 8] is 8.246 from the cluster center.\n[-4  4] is 5.657 from the cluster center.\n```\n:::\n:::\n\n\nWhich one is more of an outlier?\n\nWe have to take into account the __probability__ of the point under the (presumed) multivariate Gaussian distribution.\n\nUsing standard (MLE) methods (on all the data), we estimate the Gaussian distribution to have mean \n\n$$ \\mu = \\mat{{c}0\\\\0}$$\n\nand covariance\n\n$$\\Sigma = \\mat{{cc}1&0.75\\\\0.75&3}$$\n\nA convenient way to express the probability is via its negative log:\n\n$$ - \\log(P[\\vx]) - C = (\\vx - \\mu)^T \\Sigma^{-1} (\\vx - \\mu)$$\n\n($C$ is a constant that does not depend on $\\vx$.)\n\nThe expression on the right is called the __Mahalanobis distance.__  \n\nIt is essentially the distance to the center of the Gaussian, scaled to take into account the __shape__ of the Gaussian.\n\n::: {#31d2252e .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=13}\n``` {.python .cell-code}\np = np.array([apt1]).T\nmp = p.T @ np.linalg.inv(cov) @ p\nprint('{} has Mahalanobis distance {:0.3f}.'.format(apt1,mp[0,0]))\np = np.array([apt2]).T\nmp = p.T @ np.linalg.inv(cov) @ p\nprint('{} has Mahalanobis distance {:0.3f}.'.format(apt2,mp[0,0]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 8] has Mahalanobis distance 21.333.\n[-4  4] has Mahalanobis distance 36.103.\n```\n:::\n:::\n\n\nHence we can conclude that (-4, 4) is more of an outlier than (2, 8):\n\n* Although it is closer to the cluster center in Euclidean distance,\n* It is further from the cluster center in Mahalanobis distance.\n\n## High Dimensional Data\n\nWhen our data objects have hundreds (or millions) of features, we can no longer build a distributional model.   \n\nFor a dataset with 1,000 features, we need to build a 1,000 $\\times$ 1,000 covariance matrix -- which has a million elements.\n\nWe have a problem of high dimensionality -- and a natural approach is to consider dimensionality reduction.\n\nSVD to the rescue again!\n\nAs we've seen, dimensionality reduction will work if the dataset shows low effective rank.\n\nLet's consider how we might do anomaly detection when data has low effective rank.\n\nSince \n* the principle of anomaly detection is that most objects are normal, and  \n* the data has low effective rank, \n\none way to look for anomalies is to find objects that are __not__ well described by the low rank model.\n\n\nLet's start with our usual (toy) model in 2-D.\n\n(Keeping in mind that this is to build intuition for the real problem, which is in high dimension.)\n\n::: {#b20409b7 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=14}\n``` {.python .cell-code}\nn_samples = 500\n# Create correlated multivariate Gaussian samples\nC = np.array([[0.1, 0.6], [2., .6]])\nnp.random.seed(1)\nX = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])\n# Mean center\nXc = X - np.mean(X,axis=0)\n# Create an anomalous data point\napt = np.array([5,5])\nXc = np.concatenate([Xc, np.array([apt])],axis=0)\n# SVD of all data\nu, s, vt = np.linalg.svd(Xc,full_matrices=False)\northog_dir = np.array([-vt[0,1], vt[0,0]])\n# project points onto subspace\nscopy = s.copy()\nscopy[1] = 0.\nreducedX = u @ np.diag(scopy) @ vt\napt_proj = reducedX[-1,:]\n```\n:::\n\n\n::: {#d19a7879 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=15}\n``` {.python .cell-code}\n# plot\nax = ut.plotSetup(-10,10,-10,10,(8,8))\nut.centerAxes(ax)\nplt.axis('equal')\nplt.scatter(Xc[:,0],Xc[:,1], color='r')\nplt.scatter(reducedX[:,0], reducedX[:,1])\nendpoints = np.array([[-10],[10]]) @ vt[[0],:]\n_ = plt.plot(endpoints[:,0], endpoints[:,1], 'g-')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-16-output-1.png){width=614 height=611}\n:::\n:::\n\n\nIs there a point here that is not well described by the low-rank (rank-1) model?\n\nHow would we quantify this fact?\n\n::: {#53a9c214 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=16}\n``` {.python .cell-code}\n# plot\nax = ut.plotSetup(-10,10,-10,10,(8,8))\nut.centerAxes(ax)\nplt.axis('equal')\nplt.scatter(Xc[:,0],Xc[:,1], color='r')\nplt.scatter(reducedX[:,0], reducedX[:,1])\nplt.plot([apt[0],apt_proj[0]],[apt[1],apt_proj[1]],'r:')\nendpoints = np.array([[-10],[10]]) @ vt[[0],:]\n_ = plt.plot(endpoints[:,0], endpoints[:,1], 'g-')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-17-output-1.png){width=614 height=611}\n:::\n:::\n\n\nWhat is the distance of the anomalous point from the subspace?\n\nIt is simply the **length of the difference** between the point and its projection in the subspace.\n\n::: {#f2613fe6 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=17}\n``` {.python .cell-code}\n# plot\nax = ut.plotSetup(-10,10,-10,10,(8,8))\nut.centerAxes(ax)\nplt.axis('equal')\nplt.scatter(Xc[:,0],Xc[:,1], color='r')\nplt.scatter(reducedX[:,0], reducedX[:,1])\nplt.plot([apt[0],apt_proj[0]],[apt[1],apt_proj[1]],'r:')\nendpoints = np.array([[-10],[10]]) @ vt[[0],:]\nalpha = 1.9\nplt.plot(alpha*orthog_dir[0]+endpoints[:,0], alpha*orthog_dir[1]+endpoints[:,1], 'g:')\nplt.plot(endpoints[:,0]-alpha*orthog_dir[0], endpoints[:,1]-alpha*orthog_dir[1], 'g:')\n_ = plt.plot(endpoints[:,0], endpoints[:,1], 'g-')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-18-output-1.png){width=614 height=611}\n:::\n:::\n\n\nSo to do anomaly detection via the __subspace__ method, we set a threshold on the distance of each point from the subspace.\n\n## Anomaly Detection via the Low-Rank Approximation\n\nIn practice, this is a simple process.\n\nGiven a data matrix $A$:\n\n1.  Compute the Singular value decomposition of $A$, $$U\\Sigma V^T = A.$$\n2.  Compute a low-rank approximation to $A$, $$N = U'\\Sigma'(V')^T.$$\n3.  Compute the residuals not explained by $N$:  $$O = A-N.$$\n4.  Identify the rows of $O$ with largest $\\ell_2$ norm: these rows correspond to anomalies.\n\nIn this recipe, rows of $O$ are the difference vectors between each point and its projection in the subspace.\n\nSo the $\\ell_2$ norm gives us the distance of each point from the subspace.\n\nThere are two unspecified steps in the process:\n\n1. Selecting the columns of $U$ to be used in forming $N$\n2. Deciding how many of the largest rows of $O$ are anomalies.\n\nFor 1, the general idea is to choose a $k$ at the knee of the singular value plot.  \n\nFor 2, there are statistical methods that generally work reasonably well.  However, one can always just rank the points by their residual norm, which is what we'll do.\n\n### Example 1: Facebook Spatial Likes\n\nThis data consists of the number of 'Likes' during a six month period, for each of 9000 users across the 210 content categories that Facebook assigns to pages.\n\nRows are users, Columns are categories.  $A$ is $9000 \\times 210$.\n\n$$ \\mbox{users}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\mbox{FB categories}} =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\sigma_1\\vu_1&\\sigma_k\\vu_k\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\large k}\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\vv_1&\\dots&\\dots\\\\\\dots&\\dots&\\vv_k&\\dots&\\dots\\end{array}\\right]$$\n\n\n$$ A = U\\Sigma V^T$$\n\n::: {#73890794 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=18}\n``` {.python .cell-code}\ndata = np.loadtxt('data/social/spatial_data.txt')\ndata[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 2., 8.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0.]], shape=(10, 211))\n```\n:::\n:::\n\n\nFirst we'll look at the total number of likes for each user (the row sums).\n\n::: {#5cced19b .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=19}\n``` {.python .cell-code}\nFBSpatial = data[:,1:]\nFBSnorm = np.linalg.norm(FBSpatial,axis=1,ord=1)\nplt.plot(FBSnorm)\nplt.title('Number of Likes Per User')\n_ = plt.xlabel('Users')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-20-output-1.png){width=806 height=449}\n:::\n:::\n\n\nNow let's check whether the low rank approximation holds.\n\n::: {#10767bc2 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=20}\n``` {.python .cell-code}\nu,s,vt = np.linalg.svd(FBSpatial,full_matrices=False)\nplt.plot(s)\n_ = plt.title('Singular Values of Spatial Like Matrix')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-21-output-1.png){width=806 height=431}\n:::\n:::\n\n\nWe'll approximate this data as having effective rank 25.\n\nNow let's \n\n1. Separate the portion of the data lying in the normal space from the anomalous space,\n2. Identify the top 30 anomalous users (having the largest residual component), and\n3. Plot their total number of likes against the set of all users.\n\n::: {#988fbfcc .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=21}\n``` {.python .cell-code}\n# zero out all singular values other than the first 25\nscopy = s.copy()\nscopy[25:] = 0.\n\n# compute the low-rank approximation to the data\nN = u @ np.diag(scopy) @ vt\n\n# compute the residuals (unexplained variation)\nO = FBSpatial - N\n\n# compute the l2 norm of the residuals\nOnorm = np.linalg.norm(O,axis=1)\n\n# find the 30 largest residuals \nanomSet = np.argsort(Onorm)[-30:]\n\n# plot them\nplt.plot(Onorm)\nplt.plot(anomSet,Onorm[anomSet],'ro')\n_ = plt.title('Norm of Residual (rows of O)')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-22-output-1.png){width=789 height=431}\n:::\n:::\n\n\n::: {#0ed9342b .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=22}\n``` {.python .cell-code}\n# large = np.nonzero(Onorm>100))\n# get top 30 anomalies\nanomSet = np.argsort(Onorm)[-30:]\nplt.plot(FBSnorm)\nplt.plot(anomSet,FBSnorm[anomSet],'ro')\n_ = plt.title('Top 30 Anomalous Users - Total Number of Likes')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-23-output-1.png){width=806 height=431}\n:::\n:::\n\n\nNotice that the anomalous users are not necessarily those that have made the most `likes`.  \n\nNext we'll pick out nine anomalous users and look at their pattern of likes across the 210 categories.\n\n::: {#d0d09e04 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=23}\n``` {.python .cell-code}\nplt.figure(figsize=(9,6))\nfor i in range(1,10):\n    ax = plt.subplot(3,3,i)\n    plt.plot(FBSpatial[anomSet[i-1],:])\n    plt.xlabel('FB Content Categories')\nplt.subplots_adjust(wspace=0.25,hspace=0.45)\n_ = plt.suptitle('Nine Example Anomalous Users',size=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-24-output-1.png){width=724 height=561}\n:::\n:::\n\n\nAnd let's do the same for nine normal users.\n\n::: {#049100d9 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=24}\n``` {.python .cell-code}\n# choose non-anomalous users\nset = np.argsort(Onorm)[0:7000]\n# that have high overall volume\nmax = np.argsort(FBSnorm[set])[::-1]\nplt.figure(figsize=(9,6))\nfor i in range(1,10):\n    ax = plt.subplot(3,3,i)\n    plt.plot(FBSpatial[set[max[i-1]],:])\n    plt.xlabel('FB Content Categories')\nplt.subplots_adjust(wspace=0.25,hspace=0.45)\n_ = plt.suptitle('Nine Example Normal Users',size=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-25-output-1.png){width=715 height=561}\n:::\n:::\n\n\n### Example 2: Facebook Temporal LIkes\n\nThis data consists of the number of 'Likes' for each of 9000 users, over 6 months, on a daily basis\n\nRows are users, Columns are days.\n\n$$ \\mbox{users}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{cccc}\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_1}\\\\\\vdots\\\\\\vdots\\end{array}&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_2}\\\\\\vdots\\\\\\vdots\\end{array}&\\dots&\\begin{array}{c}\\vdots\\\\\\vdots\\\\{\\bf a_n}\\\\\\vdots\\\\\\vdots\\end{array}\\\\\\end{array}\\right]}^{\\mbox{days}} =\n\\overbrace{\\left[\\begin{array}{cc}\\vdots&\\vdots\\\\\\vdots&\\vdots\\\\\\sigma_1\\vu_1&\\sigma_k\\vu_k\\\\\\vdots&\\vdots\\\\\\vdots&\\vdots\\end{array}\\right]}^{\\large k}\n\\times\n\\left[\\begin{array}{ccccc}\\dots&\\dots&\\vv_1&\\dots&\\dots\\\\\\dots&\\dots&\\vv_k&\\dots&\\dots\\end{array}\\right]$$\n\n\n$$ A = U\\Sigma V^T$$\n\nFirst we'll look at the singular values.\n\n::: {#abed904d .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=25}\n``` {.python .cell-code}\ndata = np.loadtxt('data/social/temporal_data.txt')\nFBTemporal = data[:,1:]\nFBTnorm = np.linalg.norm(FBTemporal,axis=1,ord=1)\nu,s,vt = np.linalg.svd(FBTemporal,full_matrices=False)\n_ = plt.plot(s)\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-26-output-1.png){width=806 height=411}\n:::\n:::\n\n\nWe'll again assume an effective rank of 25.\n\nNext, plot the anomalous users as before.\n\n::: {#f3913640 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=26}\n``` {.python .cell-code}\n# choose the top 25 columns of U for the normal space\nunorm = u[:,0:24]\nP = unorm.dot(unorm.T)\nN = P.dot(FBTemporal)\nO = FBTemporal - N\nOnorm = np.linalg.norm(O,axis=1)\n# get top 30 anomalies\nanomSet = np.argsort(Onorm)[-30:]\nplt.plot(FBTnorm)\nplt.plot(anomSet,FBTnorm[anomSet],'ro')\n_ = plt.title('Top 30 Anomalous Users - Total Number of Likes')\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-27-output-1.png){width=806 height=431}\n:::\n:::\n\n\nNow let's look at sample anomalous and normal users.   \n\n::: {#2b4722dc .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"-\"}}' execution_count=27}\n``` {.python .cell-code}\nplt.figure(figsize=(9,6))\nfor i in range(1,10):\n    ax = plt.subplot(3,3,i)\n    plt.plot(FBTemporal[anomSet[i-1],:])\n    plt.xlabel('Days')\nplt.subplots_adjust(wspace=0.25,hspace=0.45)\n_ = plt.suptitle('Nine Example Anomalous Users',size=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-28-output-1.png){width=724 height=561}\n:::\n:::\n\n\n::: {#72de1a2d .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=28}\n``` {.python .cell-code}\n# choose non-anomalous users\nset = np.argsort(Onorm)[0:7000]\n# that have high overall volume\nmax = np.argsort(FBTnorm[set])[::-1]\nplt.figure(figsize=(9,6))\nfor i in range(1,10):\n    ax = plt.subplot(3,3,i)\n    plt.plot(FBTemporal[set[max[i-1]],:])\n    plt.xlabel('Days')\nplt.subplots_adjust(wspace=0.25,hspace=0.45)\n_ = plt.suptitle('Nine Example Normal Users',size=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](12-Anomaly-Detection-SVD-III_files/figure-revealjs/cell-29-output-1.png){width=707 height=561}\n:::\n:::\n\n\nInterestingly, what makes a user anomalous seems to have reversed from the case of the spatial data.\n\n",
    "supporting": [
      "12-Anomaly-Detection-SVD-III_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}