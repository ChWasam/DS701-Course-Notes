{
  "hash": "1be0c66046ab641fc82e0e1f2493a116",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Algebra Refresher\njupyter: python3\n---\n\n# Introduction\n\n## Introduction to linear algebra\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/04-Linear-Algebra-Refresher.ipynb)\n\nLinear algebra is the branch of mathematics involving vectors and matrices. In particular, how vectors are transformed.  Knowledge of linear algebra is essential in data science. \n\n## Linear algebra\n\nLinear algebra allows us to understand the operations and transformations used to manipulate and extract information from data.\n\n:::: {.frag}\nExamples\n\n:::: {.incremental}\n- Deep neural networks use matrix-vector and matrix-matrix multiplication.\n- Natural language processing use the dot product to determine word similarity.\n- Least squares uses matrix inverses and matrix factorizations to compute models for predicting continuous values.\n- PCA (dimensionality reduction) uses the matrix factorization called the Singular Value Decomposition (SVD).\n- Graphs are described by adjacency matrices. Eigenvectors and eigenvalues of this matrix provide information about the graph structure.\n - Recommendation systems represent the interactions between users and items as a large matrix.  Linear algebra is then used to compute similarities and predict missing ratings.\n::::\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Lecture overview\n:::\n\n:::: {.fragment}\nLinear algebra is used to implement data science algorithms efficiently and accurately.\n::::\n\n:::: {.fragment}\nYou will not have to program linear algebra algorithms. You will use appropriate Python packages.\n::::\n\n:::: {.fragment}\nThe goal of this lecture is to refresh the following topics:\n\n:::: {.incremental}\n- vectors,\n- matrices,\n- operations with vectors and matrices,\n- eigenvectors and eigenvalues,\n- linear systems and least squares,\n- matrix factorizations.\n::::\n::::\n\n::: {.content-hidden when-profile=\"web\"}\n## References\n:::\n\nBelow is a list of very useful resources for learning about linear algebra:\n\n- __Linear Algebra and Its Applications (6th edition)__, David C. Lay, Judi J. McDonald, and Steven R. Lay, Pearson, 2021,\n- __Introduction to Linear Algebra (6th edition)__, Gilbert Strang, Wellesley-Cambridge Press, 2023,\n    - Gilbert Strang's [lecture videos](https://youtube.com/playlist?list=PL221E2BBF13BECF6C&si=ImY77CfkyNVJvPtt)\n- __Linear Algebra and Learning from Data__, Gilbert Strang, Wellesley-Cambridge Press, 2019,\n- __Numerical Linear Algebra__, Lloyn N. Trefethen and David Bau, SIAM, 1997.\n\n\n# Vectors and Vector Operations\n\n## Vectors\n\nA vector of length $n$, $\\mathbf{x}\\in\\mathbb{R}^{n}$, is a 1-dimensional (1-D) array of real numbers\n\n$$\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n$$\n\nWhen discussing vectors we will only consider column vectors. A row vector can always be obtained from a column vector via transposition\n\n$$\n\\mathbf{x}^{T} = [x_1, x_2, \\ldots, x_n].\n$$\n\n## Examples of Vectors\n\nIn practice we use vectors to encode measurements or features.  \n\n* a housing dataset with two features: $\\mathbf{x}=[\\text{bedrooms},\\text{area}]^T$ where the first component counts bedrooms and the second measures square footage.  \n\n* an image patch can be represented by a long vector of pixel intensities. \n\n**Class activity (1–2 minutes):** With a partner, list three different kinds of data that you can naturally represent as vectors.  Share your examples with the class.\n\n:::: {.fragment}\nExamples: Geographic coordinates (Lat, Long, Elevation), RGB color intensities, term frequencies, sensor readings...\n::::\n\n## Geometric interpretation of vectors\n\n* Vectors in $\\mathbb{R}^{2}$ can be visualized as points in a 2-D plane (or arrows originating at the origin), \n* Vectors in $\\mathbb{R}^{3}$ can be visualized as points in a 3-D space (or arrows originating at the origin).\n* Although we can't visualize vectors in $\\mathbb{R}^{n}$ for $n>3$, we can still think of them as points in an $n$-dimensional space, although _the curse of dimensionality_ comes into play (more later)\n\nLet \n\n$$\n\\mathbf{x}=\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},~\n\\mathbf{y} = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix},~\n\\mathbf{z} = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}.\n$$\n\n::: {.content-hidden when-profile=\"slides\"}\nThese vectors are illustrated in @fig-vector-viz.\n:::\n\n::: {#cell-fig-vector-viz .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nx = np.array([2, 2])\ny = np.array([3, -1])\nz = np.array([-2, -1])\nV = np.array([x, y, z])\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['r', 'b', 'g'], \n           angles='xy', \n           scale_units='xy', \n           scale=1)\nax.set_xlim([-6, 6])\nax.set_ylim([-2, 4])\nax.text(3.3, -1.1, '$(3,-1)$', size=16)\nax.text(2.3, 1.9, '$(2,2)$', size=16)\nax.text(-3.7, -1.3, '$(-2,-1)$', size=16)\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Illustration of vectors](04-Linear-Algebra-Refresher_files/figure-revealjs/fig-vector-viz-output-1.png){#fig-vector-viz width=796 height=416}\n:::\n:::\n\n\n## Vector Operations\n\n**Scalar multiplication:** Let $c\\in\\mathbb{R}$, $\\mathbf{x}\\in\\mathbb{R}^{n}$, then\n$$\nc\\mathbf{x} = \\begin{bmatrix} cx_1 \\\\ cx_2 \\\\ \\vdots \\\\ cx_n \\end{bmatrix}.\n$$\n\n---\n\nMultiplication by a scalar $c\\in\\mathbb{R}$. \n\n* For $c>1$ the vector is _lengthened_. \n* For $0<c<1$ the vector _shrinks_. \n* If we negate $c$ the direction of the vector is _flipped 180 degrees_. \n\nFigure @fig-vector-scaling shows the vector $\\mathbf{x} = [2, 2]$ multiplied by the scalar value $c=2$.\n\n::: {#cell-fig-vector-scaling .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nx = np.array([2, 2])\ny = np.array([4, 4])\nV = np.array([x, y])\norigin = np.array([[0, 0], [0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['r', 'b'], \n           angles='xy', \n           scale_units='xy', \n           scale=1,\n           alpha= 0.5)\nax.set_xlim([-5, 5])\nax.set_ylim([-1, 5])\nax.text(2.3, 1.9, '$x$', size=16)\nax.text(4.3, 3.9, '$cx$', size=16)\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scalar multiplication of a vector](04-Linear-Algebra-Refresher_files/figure-revealjs/fig-vector-scaling-output-1.png){#fig-vector-scaling width=792 height=416}\n:::\n:::\n\n\n## Example: Image processing\n\n* in image processing each pixel's color is typically represented by a 3‑element RGB vector.  \n* Multiplying the RGB vector by a scalar less than one darkens the pixel, while\n* multiplying by a scalar greater than one brightens it.  \n\nE.g. scaling the color $(0.2,0.5,0.7)$ by $0.5$ yields $(0.1,0.25,0.35)$\n\nScaling by $2$ yields $(0.4,1.0,1.0)$ (clipping values to stay between $0$ and $1$).\n\n\n---\n\n## Vector addition\n\nLet $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ then\n\n$$\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}.\n$$\n\n\n---\n\nWe plot the sum of $\\mathbf{u} = [1, 2]$ and $\\mathbf{v} = [4, 1]$ in @fig-vector-addition. \nThe sum $\\mathbf{u} + \\mathbf{v} = [5, 3]$ is obtained by placing the tip of one vector to the tail of the other vector. \n\n::: {#cell-fig-vector-addition .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nu = np.array([1, 2])\nv = np.array([4, 1])\nw = np.array([5, 3])\nV = np.array([u, v, w])\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['b', 'b', 'r'], \n           angles='xy', \n           scale_units='xy', \n           scale=1)\nax.set_xlim([-1, 6])\nax.set_ylim([-1, 4])\nax.text(1.3, 1.9, '$u$', size=16)\nax.text(4.3, 1.2, '$v$', size=16)\nax.text(5.3, 2.9, '$u+v$', size=16)\nplt.plot([1, 5], [2, 3], 'g--')\nplt.plot([4, 5], [1, 3], 'g--')\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Vector addition](04-Linear-Algebra-Refresher_files/figure-revealjs/fig-vector-addition-output-1.png){#fig-vector-addition width=796 height=416}\n:::\n:::\n\n\n---\n\nVector addition occurs whenever you combine measurements componentwise.  \n\nFor example, \n\n* if $\\mathbf{u}=[100,80]$ and $\\mathbf{v}=[20,50]$ record the number of items sold by two shops on Monday and Tuesday, then \n* their sum $\\mathbf{u}+\\mathbf{v}=[120,130]$ gives the total items sold by both shops on those days.\n\n**Class activity (1–2 minutes):** Add the vectors $\\mathbf{u}=[-1,4]$ and $\\mathbf{v}=[3,-2]$ by hand and sketch the result.  Discuss how the parallelogram rule visualizes vector addition.\n\n---\n\n**Dot product:** Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ then the dot product is defined as\n$$\n\\mathbf{u}\\cdot\\mathbf{v} = \\sum_{i=0}^n u_i v_i.\n$$\n\n**Vector 2-norm:** The 2-norm of a vector $\\mathbf{v}\\in\\mathbb{R}^{n}$ is defined as\n$$\n\\Vert \\mathbf{v}\\Vert_2 = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n v_i^2}.\n$$\n\nThis norm is referred to as the $\\ell_2$ norm. In these notes, the notation $\\Vert \\mathbf{v} \\Vert$, indicates the 2-norm.\n\n---\n\nDot products and norms arise constantly in data science.\n\nLet's look at another numerical example.\n\nIf $\\mathbf{u}=[1,2,0]$ and $\\mathbf{v}=[3,-1,4]$, then\n$$\n\\mathbf{u}\\cdot\\mathbf{v} = 1\\cdot 3 + 2\\cdot(-1) + 0\\cdot 4 = 1.\n$$\n\n* norm of $\\mathbf{u}$ is $\\|\\mathbf{u}\\|_2 = \\sqrt{1^2+2^2+0^2} = \\sqrt{5}$ and the \n* norm of $\\mathbf{v}$ is $\\sqrt{3^2+(-1)^2+4^2}=\\sqrt{26}$.  \n\n\n## Definitions\n\n**Unit vector:** A unit vector $\\mathbf{v}$ is a vector such that $\\Vert \\mathbf{v} \\Vert_2 = 1$. \n    - All vectors of the form $\\frac{\\mathbf{v}}{\\Vert \\mathbf{v} \\Vert_2 }$ are unit vectors.\n\n**Distance:** Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$, the distance between $\\mathbf{u}$ and $\\mathbf{v}$ is\n$$\n\\Vert \\mathbf{u} - \\mathbf{v} \\Vert_2.\n$$\n\n**Orthogonality:** Two vectors $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ are orthogonal if and only if $\\mathbf{u}\\cdot\\mathbf{v}=0$. \n\n**Angle between vectors:**  Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$, the angle between these vectors is \n$$\n\\cos{\\theta} = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\Vert \\mathbf{u}\\Vert_2 \\Vert\\mathbf{v}\\Vert_2}.\n$$\n\n---\n\nThe dot product of two vectors $\\mathbf{u}, \\mathbf{v}$ can be used to project $\\mathbf{u}$ onto $\\mathbf{v}$\n\n$$\n\\mathrm{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\Vert \\mathbf{v} \\Vert^2}\\mathbf{v}.\n$$\n\nThis is illustrated in @fig-dot-product.\n\n::: {#cell-fig-dot-product .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\nax = plt.gca()\n\n# Define vectors u and v\nu = np.array([1, 2])\nv = np.array([4, 1])\n\n# Project u onto v\nprojection = np.dot(u, v) * v / np.dot(v, v)\n\nV = np.array([u, v, projection])\n\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], color=['b', 'b', 'r'], angles='xy', scale_units='xy', scale=1)\n\nax.set_xlim([-1, 6])\nax.set_ylim([-1, 4])\nax.text(1.3, 1.9, r'$\\mathbf{u}$', size=16)\nax.text(4.3, 1.2, r'$\\mathbf{v}$', size=16)\nax.text(0.4, -0.3, r'$\\mathrm{proj}_{\\mathbf{v}}\\mathbf{u}$', size=16)\n\nplt.plot([u[0], projection[0]], [u[1], projection[1]], 'g--')\n\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Dot product](04-Linear-Algebra-Refresher_files/figure-revealjs/fig-dot-product-output-1.png){#fig-dot-product width=796 height=416}\n:::\n:::\n\n\nObserve that a right angle forms between the vectors $\\mathbf{u}$ and $\\mathbf{v}$ when $\\mathbf{u}\\cdot \\mathbf{v} = 0$. \n\nAnd we can calculate the angle between $\\mathbf{u}$ and $\\mathbf{v}$.\n\n::: {#0ff4973c .cell execution_count=5}\n``` {.python .cell-code}\n# Define vectors u and v\nu = np.array([1, 2])\nv = np.array([4, 1])\n\ntheta = np.arccos(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v)))\nprint(f\"The angle between u and v is {theta} radians or {np.degrees(theta)} degrees.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe angle between u and v is 0.8621700546672264 radians or 49.398705354995535 degrees.\n```\n:::\n:::\n\n\n---\n\nSuch cosine similarities are used in natural language processing to quantify how similar two word‑embedding vectors are.\n\n**Class activity (1–2 minutes):** \n\nCompute the dot product of $\\mathbf{u}=[1,2,0]$ and $\\mathbf{v}=[3,-1,4]$.  \n\nAre the vectors orthogonal?  \n\nNext compute $\\|\\mathbf{u}\\|_2$, $\\|\\mathbf{v}\\|_2$ and the angle between them.  \n\n## More Definitions\n\n**Linear dependence:** A set of $n$ vectors $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{n}\\in\\mathbb{R}^n$ is linearly dependent if there exists scalars $a_1,\\ldots, a_n$ not all zero such that \n$$\n\\sum_{i=1}^{n} a_i \\mathbf{v}_i = 0.\n$$\n\n**Linear independence:** The vectors $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{n}$ are linearly independent if they are not linearly dependent, i.e., the equation\n$$\na_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n = 0,\n$$\n\nis only satisfied if $a_i=0$ for $i=1, \\ldots,n$.\n\n**Span:** Given a set of vectors $V = \\{\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{n}\\}$, where $\\mathbf{v}_i\\in\\mathbb{R}^n$, the span(V) is the set of all linear combinations of vectors in $V$.\n\n---\n\nLet's look at an example.\n\nFor example, the vectors $\\mathbf{v}_1=(1,1,0)$ and $\\mathbf{v}_2=(2,2,0)$ are linearly dependent because $\\mathbf{v}_2=2\\mathbf{v}_1$.  \n\nIn contrast, the standard basis vectors $e_1=(1,0,0)$, $e_2=(0,1,0)$ and $e_3=(0,0,1)$ in $\\mathbb{R}^3$ are linearly independent, and their span is the entire three‑dimensional space.  \n\nWhen working with data, linear independence tells us whether a set of features carries redundant information.\n\n:::: {.fragment}\n**Class activity (1–2 minutes):** Consider the set of vectors $\\{[1,0], [0,1], [1,1]\\}$ in $\\mathbb{R}^2$.  Work with a neighbor to decide whether these vectors are linearly independent.  What is their span?  Justify your answer.\n::::\n\n# Matrices\n\n## Matrices\n\nA matrix $A\\in\\mathbb{R}^{m\\times n}$ is a 2-D array of numbers\n\n$$\nA = \n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} &a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\n$$\n\nwith $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.\n\n> Note: By convention, matrix indexing is opposite of graphical vector indexing.\n\n---\n\nSimilar to vectors, we can multiply matrices by scalar values and add matrices of the same dimension, i.e.,\n\n**Scalar multiplication:** Let $c\\in\\mathbb{R}$ and $A\\in\\mathbb{R}^{m\\times n}$, then\n$$\ncA =\n\\begin{bmatrix}\nca_{11} & ca_{12} & \\cdots & ca_{1n} \\\\\nca_{21} & ca_{22} & \\cdots & ca_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nca_{m1} & ca_{m2} & \\cdots & ca_{mn} \\\\\n\\end{bmatrix}.\n$$\n\n---\n\n**Matrix addition:** Let $A, B\\in\\mathbb{R}^{m\\times n}$, then \n$$\nA + B =\n\\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn} \\\\\n\\end{bmatrix}\n$$\n\n---\n\n**Transpose:** The transpose $A^{T}$ is defined as\n\n$$\nA^{T} = \n\\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} &a_{2n} & \\cdots & a_{nm} \\\\\n\\end{bmatrix}.\n$$\n\nThe transpose turns columns of the matrix into rows (equivalently rows into columns). A square matrix is called symmetric if $A=A^{T}$.\n\n## Matrix multiplication\n\nWe discuss the following two important matrix multiplication operations\n\n- matrix-vector multiplication,\n- matrix-matrix multiplication.\n\n::: {.content-hidden when-profile=\"slides\"}\n### Matrix-vector multiplication\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Matrix-vector multiplication\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{R}^{n}$, then $A\\mathbf{x}\\in\\mathbb{R}^{m}$\ncan be defined _row-wise_ as \n\n$$\nA\\mathbf{x} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1a_{11} + x_2 a_{12} + \\cdots + x_na_{1n} \\\\\nx_1a_{21} + x_2 a_{22} + \\cdots + x_na_{2n} \\\\\n\\vdots \\\\\nx_1a_{m1} + x_2 a_{m2} + \\cdots + x_na_{mn} \\\\\n\\end{bmatrix}.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Matrix-vector multiplication cont.\n:::\n\nEquivalently, this means that $A\\mathbf{x}$ is a linear combination of the _columns_ of $A$, i.e.,\n\n$$\nA\\mathbf{x} = \nx_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1}  \\end{bmatrix} \n+ \nx_2  \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2}  \\end{bmatrix}\n+\n\\cdots\n+\nx_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn}  \\end{bmatrix}.\n$$\n\nObserve that the matrix $A$ is a linear transformation that maps vectors in $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n\n::: {.content-hidden when-profile=\"slides\"}\n### Matrix-matrix multiplication\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Matrix-matrix multiplication\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ and $B\\in\\mathbb{R}^{n\\times p}$, then the elements of $C=AB\\in\\mathbb{R}^{m\\times p}$ are\n\n$$\nc_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj},\n$$\n\nfor $i=1,\\ldots, m$ and $j=1, \\ldots, p$.\n\n\n$$\nAB = \n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{11} & b_{12} & \\cdots & b_{1p} \\\\\nb_{21} & b_{22} & \\cdots & b_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nc_{11} & c_{12} & \\cdots & c_{1p} \\\\\nc_{21} & c_{22} & \\cdots & c_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{m1} & c_{m2} & \\cdots & c_{mp} \\\\\n\\end{bmatrix}.\n$$\n\n# Vector spaces\n\n## Vector spaces\n\nAn essential concept in linear algebra is the notion of a **vector space**.\n\nA vector space is a set $V$ such that for any two vectors in the set, say $\\mathbf{u},\\mathbf{v}\\in V$, and any scalars $c$ and $d$, the linear combination $c\\mathbf{u} + d\\mathbf{v}$ is also in $V$.  \n\nIn addition, a vector space must satisfy the following properties\n\n:::: {.incremental}\n1. $\\mathbf{u} + (\\mathbf{v} + \\mathbf{w}) = (\\mathbf{u} + \\mathbf{v}) + \\mathbf{w}$ _(associativity)_.\n1. $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$ _(commutativity)_.\n1. There exists $\\mathbf{0}\\in V$ such that $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ for all $\\mathbf{v}\\in V$ _(additive identity element)_.\n1. For every $\\mathbf{v}\\in V$, there exists $-\\mathbf{v}\\in V$ such that $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$ _(additive inverse)_.\n1. $c(d\\mathbf{v}) = (cd)\\mathbf{v}$ _(associativity)_.\n1. $1\\mathbf{v} = \\mathbf{v}$ _(multiplicative identity element)_.\n1. $c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}$ _(distributivity)_.\n1. $(c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}$ _(distributivity)_.\n::::\n\n::: {.content-hidden when-profile=\"web\"}\n## Vector spaces continued\n:::\n\nSome examples of vector spaces are:\n\n:::: {.incremental}\n- The set of n-dimensional vectors with real numbers, i.e., $\\mathbb{R}^n$.\n- Given a matrix $A\\in\\mathbb{R}^{m\\times n}$, \n    - the _column space_ $col(A)$, which is the span of all columns in the matrix $A$ is a vector space. \n    - The similarly defined _row space_ is also a vector space.\n- Given a matrix $A\\in\\mathbb{R}^{n\\times n}$, the set of all solutions to the\n  equation $A\\mathbf{x} = \\mathbf{0}$ is a vector space. This space is called the\n  null space of matrix $A$.\n- The set of all $m\\times n$ matrices with real numbers is also a vector space.\n- The set of all polynomials of degree $n$ is a vector space.\n::::\n\n# Important matrices\n\n## Important matrices\n\nWe introduce notation for some commonly used and important matrices.\n\n::: {.content-visible when-profile=\"slides\"}\n## Identity matrix\n:::\n\nThe $n \\times n$ identity matrix is\n\n$$\n\\mathbf{I} = \n\\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1 \\\\\n\\end{bmatrix}.\n$$\n\nFor every $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$, then $\\mathbf{AI} = \\mathbf{IA}$. \n\nIn practice the identity matrix leaves any vector unchanged.  For example,\n$$\n\\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\begin{bmatrix}3\\\\ -2 \\end{bmatrix} = \\begin{bmatrix}3\\\\ -2 \\end{bmatrix}.\n$$\nBecause of this property we sometimes call $I$ the *do nothing* transform.\n\n::: {.content-visible when-profile=\"slides\"}\n## Matrix inverse\n:::\n\nThe inverse $A^{-1}\\in\\mathbb{R}^{n\\times n}$ is defined as the matrix for which \n$$AA^{-1} = A^{-1}A = I,$$ \n\nWhen $A^{-1}$ exists the matrix is said to be _invertible_. \n\nNote that $(AB)^{-1} = B^{-1}A^{-1}$ for invertible $B\\in\\mathbb{R}^{n\\times n}$.\n\n---\n\nFor example, the $2\\times 2$ matrix\n$$\nA = \\begin{bmatrix}1 & 2\\\\ 3 & 4\\end{bmatrix}\n$$\nhas inverse\n$$\nA^{-1} = \\begin{bmatrix}-2 & 1\\\\ 1.5 & -0.5\\end{bmatrix},\n$$\nand one can check that $AA^{-1} = I$ and $A^{-1}A = I$.\n\n<br><br>\n\n**Class activity (1–2 minutes):** Show that $AA^{-1} = I$ and $A^{-1}A = I$.\n\n::: {.content-visible when-profile=\"slides\"}\n## Diagonal matrices\n:::\nA diagonal matrix $D\\in\\mathbb{R}^{n\\times n}$ has entries $d_{ij}=0$ if $i\\neq j$, i.e.,\n\n$$\nD =\n\\begin{bmatrix}\nd_{11} & 0 & \\cdots & 0 \\\\\n0 & d_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{nn} \\\\\n\\end{bmatrix}.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Orthogonal matrices\n:::\nA square matrix $Q\\in\\mathbb{R}^{n}$ is orthogonal if \n\n$$QQ^{T}=Q^{T}Q=I.$$ \n\nIn particular, the inverse of an orthogonal matrix is it's transpose.\n\n> Note: Think about this in terms of matrix multiplication of rows and columns and\n> the dot product of orthogonal vectors.\n\n---\n\nA familiar example of an orthogonal matrix is the $2\\times 2$ rotation matrix\n$$\nR(\\theta) = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta\\\\ \\sin \\theta & \\cos \\theta\\end{bmatrix}.\n$$\nFor $\\theta=45^\\circ$ this becomes $\\big[\\tfrac{\\sqrt{2}}{2}, -\\tfrac{\\sqrt{2}}{2}; \\tfrac{\\sqrt{2}}{2}, \\tfrac{\\sqrt{2}}{2}\\big]$, and one can verify that $R(\\theta)R(\\theta)^T=I$.  \n\nOrthogonal matrices preserve lengths and angles, which is why they model rigid rotations and reflections.\n\n<br><br>\n\n:::: {.fragment}\n**Class activity (1–2 minutes):** Show that the $2\\times 2$ matrix $\\begin{bmatrix}0 & -1\\\\ 1 & 0\\end{bmatrix}$ and check that it is orthogonal.  What geometric transformation does it represent?\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Lower triangular matrices\n:::\nA lower triangular matrix $L\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries above the main diagonal are zero\n\n$$\nL =\n\\begin{bmatrix}\nl_{11} & 0 & \\cdots & 0 \\\\\nl_{12} & l_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{1n} & l_{n2} & \\cdots & l_{nn} \\\\\n\\end{bmatrix}.\n$$\n\n---\n\nSolving a linear system $L\\mathbf{x}=\\mathbf{b}$ with $L$ lower triangular can be done efficiently by forward substitution.  \n\nFor example, let\n$$\nL = \\begin{bmatrix}1 & 0\\\\ 2 & 3\\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix}1\\\\4\\end{bmatrix}.\n$$\nThen the system $L\\mathbf{x}=\\mathbf{b}$ yields $x_1=1$ from the first equation and $2x_1+3x_2=4$ from the second, giving $x_2=\\tfrac{2}{3}$.\n\n**Class activity (1–2 minutes):** Solve the lower triangular system $\\begin{bmatrix}1 & 0\\\\ -1 & 2\\end{bmatrix}\\mathbf{x} = \\begin{bmatrix}3\\\\1\\end{bmatrix}$ by forward substitution.\n\n::: {.content-visible when-profile=\"slides\"}\n## Upper triangular matrices\n:::\nAn upper triangular matrix $U\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries below the main diagonal are zero\n\n$$\nU = \n\\begin{bmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} & \\cdots & u_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & u_{nn} \\\\\n\\end{bmatrix}.\n$$\n\n---\n\nUpper triangular systems are solved by backward substitution.  For example, if\n$$\nU = \\begin{bmatrix}4 & 1\\\\ 0 & 5\\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix}9\\\\10\\end{bmatrix},\n$$\nthe equation $U\\mathbf{x}=\\mathbf{b}$ implies $5x_2=10$ so $x_2=2$, and then $4x_1 + x_2 =9$ gives $x_1 = \\tfrac{7}{4}$.\n\n**Class activity (1–2 minutes):** Solve the upper triangular system $\\begin{bmatrix}2 & 3\\\\ 0 & -1\\end{bmatrix}\\mathbf{x} = \\begin{bmatrix}7\\\\ -2\\end{bmatrix}$ by backward substitution.\n\n::: {.content-visible when-profile=\"web\"}\nThe inverse of a lower triangular matrix is itself a lower triangular matrix. This is also true for upper triangular matrices, i.e., the inverse is also upper triangular.\n:::\n\n## Eigenvalues and eigenvectors\n\nAn eigenvector of an $n\\times n$ matrix $\\mathbf{A}$ is a nonzero vector $\\mathbf{x}$ such that \n\n$$\nA\\mathbf{x} = \\lambda\\mathbf{x}\n$$ \n\nfor some scalar $\\lambda.$ \n\nThe scalar $\\lambda$ is called an eigenvalue.\n\nAn $n \\times n$ matrix has at most $n$ distinct eigenvectors and at most $n$ distinct eigenvalues.\n\n---\n\nFor example, consider the matrix\n$$\nA = \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}.\n$$\nIt has eigenvalues $\\lambda_1=3$ and $\\lambda_2=1$ with corresponding eigenvectors $\\mathbf{v}_1=[1,1]^T$ and $\\mathbf{v}_2=[1,-1]^T$.  \n\n\nEigenvectors reveal directions that the matrix stretches or compresses without changing direction.\n\n<br><br>\n\n**Class activity (1–2 minutes):** Verify that $A\\mathbf{v}_1 = 3\\mathbf{v}_1$ and $A\\mathbf{v}_2 = 1\\mathbf{v}_2$. \n\n::: {.content-visible when-profile=\"slides\"}\n# Matrix decompositions\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Matrix decompositions\n\nWe introduce here important matrix decompositions. These are useful in solving linear equations. Furthermore they play an important role in various data science applications.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### LU factorization\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## LU factorization\n:::\n\nAn LU decomposition of a square matrix $A\\in\\mathbb{R}^{n\\times n}$ is a factorization of $A$ into a product of matrices\n\n$$ \nA = LU,\n$$\n\nwhere $L$ is a lower triangular square matrix and $U$ is an upper triangular square matrix. For example, when $n=3$, we have\n\n$$ \n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0\\\\\nl_{31} & l_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33} \\\\\n\\end{bmatrix}\n$$\n\nIt simplifies the process of solving linear equations and is more numerically\nstable than computing the inverse of $A$.\n\n---\n\nThen instad of computing the inverse of $A$, we can solve the linear system $A\\mathbf{x}=\\mathbf{b}$ in two steps:\n\n1. **Forward substitution:** First, solve the equation $L\\mathbf{y} = \\mathbf{b}$ for $\\mathbf{y}$.\n2. **Backward substitution:** Then, solve $U\\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$.\n\nThis method is particularly useful because triangular matrices are much easier to\nwork with. It can make solving linear systems faster and more efficient, \nespecially for large matrices.\n\n\n---\n\nFor example, consider the matrix\n$$\nA = \\begin{bmatrix}4 & 3\\\\6 & 3\\end{bmatrix}.\n$$\nOne possible LU factorization is\n$$\nA = LU = \\begin{bmatrix}1 & 0\\\\ 1.5 & 1\\end{bmatrix}\\begin{bmatrix}4 & 3\\\\ 0 & -1.5\\end{bmatrix}.\n$$\nOnce we have $L$ and $U$ we can solve $A\\mathbf{x}=\\mathbf{b}$ by first solving $L\\mathbf{y}=\\mathbf{b}$ and then $U\\mathbf{x}=\\mathbf{y}$.\n\n\n::: {.content-visible when-profile=\"web\"}\n### QR decomposition\n\n\nA QR decomposition of a square matrix $A\\in\\mathbb{R}^{n\\times n}$ is a factorization of $A$ into a product of matrices\n\n$$\nA=QR,\n$$\nwhere $Q$ is an orthogonal square matrix and $R$ is an upper-triangular square matrix.\n\n\nQR factorization is useful in solving linear systems and performing least squares fitting.\n\n---\n\nThis factorization has a couple of important benefits:\n\n1. **Solving Linear Systems**: When you're working with a system of equations represented by $A\\mathbf{x} = \\mathbf{b}$, you can substitute the QR factorization into this equation:\n\n   $$\n   QR\\mathbf{x} = \\mathbf{b}\n   $$\n\n   Since $Q$ is orthogonal, you can multiply both sides by $Q^T$ (the transpose of $Q$) to simplify it:\n\n   $$\n   R\\mathbf{x} = Q^T\\mathbf{b}\n   $$\n\n   Now, you can solve this upper triangular system for $\\mathbf{x}$ using backward substitution, which is typically easier and more stable.\n\n2. **Least Squares Problems**: In many data science applications, you want to find the best fit line or hyperplane for your data. QR factorization is particularly useful here because it helps in minimizing the error when the system is overdetermined (more equations than unknowns). You can solve the least squares problem by leveraging the QR decomposition to find:\n\n   $$\n   \\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}\n   $$\n\nBy converting the problem into a triangular system, QR factorization often provides a more stable numerical solution than other methods, especially for poorly conditioned matrices.\n\n\n---\n\nAs a simple example, consider the $2\\times 2$ matrix\n$$\nA = \\begin{bmatrix}1 & 1\\\\1 & -1\\end{bmatrix}.\n$$\nUsing the _Gram–Schmidt process_ we can factor it as\n$$\nA = QR \\quad\\text{with}\\quad Q = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1\\\\1 & -1\\end{bmatrix},\\quad R = \\sqrt{2}\\begin{bmatrix}1 & 0\\\\ 0 & 1\\end{bmatrix}.\n$$\nHere $Q$ has orthonormal columns and $R$ is upper triangular.\n\n\n\n\n\n::: {.content-visible when-profile=\"web\"}\n### Eigendecomposition\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Eigendecomposition\n:::\n\nLet $A\\in\\mathbb{R}^{n\\times n}$ have $n$ linearly independent eigenvectors $\\mathbf{x}_i$ for $i=1,\\ldots, n$, then $A$ can be factorized as\n\n$$\nA = X\\Lambda X^{-1},\n$$\n\nwhere the columns of matrix $X$ are the eigenvectors of $A$, and \n\n$$\n\\Lambda =\n\\begin{bmatrix}\n\\lambda_{1} & 0 & \\cdots & 0 \\\\\n0 & \\lambda_{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_{n}  \\\\\n\\end{bmatrix},\n$$\n\nis a diagonal matrix of the eigenvalues. \n\nIn this case, the matrix $A$ is said to be diagonalizable.\n\n<br>\nInstead of calulating $A\\mathbf{x}$ directly, we can map it by first\ntransforming it using $X^{-1}$ and then stretching it by $\\Lambda$ and finally\ntransforming it using $X$.\n\n$$\nA \\mathbf{x} = X \\Lambda X^{-1} \\mathbf{x}\n$$\n\nWe'll use eigendecomposition in Principal Component Analysis (PCA) to reduce dimensionality in datasets while preserving as much variance as possible.\n\nA special case occurs when $A$ is symmetric. Recall that a matrix is symmetric when $A = A^T.$\n\nIn this case, it can be shown that the eigenvectors of $A$ are all mutually orthogonal. Consequently, $X^{-1} = X^{T}$ and we can decompose $A$ as:\n\n$$A = XDX^T.$$\n\nThis is known as the spectral theorem and this decomposition of $A$ is its spectral decomposition. The eigenvalues of a matrix are also called its spectrum.\n\nFor symmetric matrices the eigenvectors can be chosen to be orthonormal, so $X^{-1} = X^T$.  In the earlier example $A=\\begin{bmatrix}2&1\\\\1&2\\end{bmatrix}$, which is symmetric, its eigendecomposition can be written in orthonormal form as $A = X \\Lambda X^T$ with\n$$\nX = \\tfrac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1\\\\ 1 & -1\\end{bmatrix},\\quad \\Lambda = \\mathrm{diag}(3,1).\n$$\nBecause the columns of $X$ are orthonormal, the factorization $A = X\\Lambda X^T$ is often easier to use numerically and conceptually.\n\n\n::: {.content-visible when-profile=\"web\"}\n### Singular value decomposition\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Singular value decomposition\n:::\n\nFor the previous few examples, we required $\\mathbf{A}$ to be square. Now\nlet $A\\in\\mathbb{R}^{m\\times n}$ with $m>n$, then $A$ admits a decomposition\n\n$$\nA = U\\Sigma V^{T}.\n$$\nThe matrices $U\\in\\mathbb{R}^{m\\times m}$ and $V\\in\\mathbb{R}^{n\\times n}$ are orthogonal. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors.\n\nThe matrix $\\Sigma\\in\\mathbb{R}^{m\\times n}$ is a diagonal matrix of the form\n\n$$\n\\Sigma = \n\\begin{bmatrix}\n\\sigma_{11} & 0 & \\cdots & 0 \\\\\n0 & \\sigma_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_{mn} \\\\\n\\end{bmatrix}.\n$$\n\nThe values $\\sigma_{ij}$ are the singular values of the matrix $A$. \n\nAmazingly, it can be proven that every matrix $A\\in\\mathbb{R}^{m\\times n}$ has a singular value decomposition.\n\n---\n\nFor example, let\n$$\nA = \\begin{bmatrix}3 & 1\\\\ 1 & 3\\end{bmatrix}.\n$$\nThis matrix has singular value decomposition $A = U\\Sigma V^T$ where the singular values are $\\sigma_1=4$ and $\\sigma_2=2$.  The first singular vector (in $U$) points along the direction $[1,1]^T$ and corresponds to the dominant variation in the matrix.  In data analysis we often keep only a few largest singular values to approximate a matrix with lower rank—a technique used in latent semantic analysis and image compression.\n\n**Class activity (1–2 minutes):** Use NumPy (or by hand if you prefer) to compute the singular values of the matrix $\\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}$.  Discuss how the singular values relate to how the matrix scales different directions.\n\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## See notes for more examples\n:::\n\n# Linear Systems and Least Squares\n\n## Linear systems of equations\n\nA system of $m$ linear equations in $n$ unknowns can be written as\n\n$$\n\\begin{align*}\na_{11} x_{1} + a_{12} x_{2} + \\cdots + a_{1n} x_{n} &= b_1, \\\\\na_{21} x_{1} + a_{22} x_{2} + \\cdots + a_{2n} x_{n} &= b_2, \\\\\n\\vdots  \\qquad \\qquad \\quad \\\\\na_{m1} x_{1} + a_{m2} x_{2} + \\cdots + a_{mn} x_{n} &= b_m.\\\\\n\\end{align*}\n$$\n\nObserve that this is simply the matrix vector equation\n\n$$\nA\\mathbf{x}=\\mathbf{b}.\n$$\n\n$$\nA\\mathbf{x} = \n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{bmatrix}.\n$$\n\n<!-- merged continuation of linear systems section; removed redundant heading -->\n\n---\n\nA linear system of equations may have \n\n- no solution, \n- a unique solution, \n- or infinitely many solutions \n\ndepending on the relationship between the rows and columns of $A$.  \n\nWhether $m<n$ or $m>n$ does not by itself determine the number of solutions\n\nthe key is the _rank_ of $A$.\n\n---\n\nFor example, the system\n$$\n2x + y = 5,\\\\\nx - y = 1\n$$\nhas a unique solution $(x,y)=(2,1)$.  \n\nIn contrast, the system\n$$\n2x + y = 5,\\\\\n4x + 2y = 10\n$$\nhas infinitely many solutions because the second equation is just twice the first.  \n\nFinally the system\n$$\nx + y = 2,\\\\\nx + y = 3\n$$\nhas no solution because the two lines are parallel and never intersect.\n\n---\n\nWhen the coefficient matrix $A$ is invertible (square and full rank), there is always a unique solution given by $\\mathbf{x}=A^{-1}\\mathbf{b}$.  \n\nWhen $A$ is rank deficient or when there are more equations than unknowns, a solution may not exist or there may be infinitely many solutions.\n\nRank deficient means that the matrix has less than $n$ linearly independent columns.\n\n::: {.content-visible when-profile=\"web\"}\n## Linear systems of equations continued\n\nWe can use matrix factorizations to help us solve a linear system of equation. We demonstrate how to do this with the LU decomposition. Observe that \n$$A\\mathbf{x} = LU\\mathbf{x} = \\mathbf{b}.$$ \n\nThen \n\n$$\n\\mathbf{x} = U^{-1}L^{-1}\\mathbf{b}.\n$$ \n\nThe process of inverting $L$ and $U$ is called backward and forward substitution.\n\n:::\n\n## Least squares\n\nIn data science it is often the case that we have to solve the linear system \n\n$$\nA \\mathbf{x} = \\mathbf{b}.\n$$ \n\nThis problem may have no solution -- perhaps due to noise or measurement error.\n\nIn such a case, we look for a vector $\\mathbf{x}$ such that $A\\mathbf{x}$ is a good approximation to $\\mathbf{b}.$\n\nThe quality of the approximation can be measured using the distance from $A\\mathbf{x}$ to $\\mathbf{b},$ i.e.,\n\n$$\n\\Vert A\\mathbf{x} - \\mathbf{b}\\Vert_2.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Least squares continued\n:::\nThe general least-squares problem is given $A\\in\\mathbb{R}^{m\\times n}$ and and $\\mathbf{b}\\in\\mathbb{R}^{m}$, find a vector $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{n}$ such that $\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert_2$ is minimized, i.e. \n\n$$\n\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x} \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.\n$$\n\nThis emphasizes the fact that the least squares problem is a minimization problem.\n\n::: {.content-visible when-profile=\"web\"}\nMinimizations problems are an example of a broad class of problems called _optimization_ problems. In optimization problems we attempt to find an optimal solution that minimizes (or maximizes) a set particular set of equations (and possibly constraints). \n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Least squares continued\n:::\nWe can connect the above minimization of the distance between vectors to the minimization of the sum of squared errors. Let $\\mathbf{y} = A\\mathbf{x}$ and observe that\n\n$$\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert_2^2 = \\Vert \\mathbf{y}-\\mathbf{b}\\Vert_2^2 =  \\sum_i (y_i-b_i)^2.$$\n\n::: {.content-visible when-profile=\"web\"}\nThe above expression is the sum of squared errors. In statistics, the $y_i$ are the estimated values and the $b_i$ are the measured values. This is the most common measure of error used in statistics and is a key principle. \n:::\n\nMinimizing the length of $A\\mathbf{x} - \\mathbf{b}$ is equivalent to minimizing the sum of the squared errors. \n\n::: {.content-visible when-profile=\"slides\"}\n## Least squares continued\n:::\nWe can find $\\hat{\\mathbf{x}}$ using either \n\n:::: {.incremental}\n* geometric arguments based on projections of the vector $\\mathbf{b}$,\n* by calculus (taking the derivative of the right-hand-side expression above and setting it equal to zero).\n::::\n\n:::: {.fragment}\nEither way, we obtain the result that $\\hat{\\mathbf{x}}$ is the solution of:\n    \n$$A^TA\\mathbf{x} = A^T\\mathbf{b}.$$\n\nThis system of equations is called the normal equations.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Least squares continued\n:::\nWe can prove that these equations always have at least one solution. \n\nWhen $A^TA$ is invertible, the system is said to be overdetermined. This means that there is a unique solution\n\n$$\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T\\mathbf{b}.$$\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Least squares beware\n:::\nBe aware that computing the solution using $(A^TA)^{-1}A^T$ can be numerically unstable. A more stable method is to use the QR decomposition of $A$, i.e., $\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}$. \n\nThe NumPy function `np.linalg.lstsq()` solves the least squares problem in a stable way. \n\nConsider the following example where the solution to the least squares problem is `x=np.array([1, 1])`.\n\n::: {#73c8c208 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nA = np.array([[-1.42382504, -1.4238264 ],\n              [ 1.26372846,  1.26372911], \n              [-0.87066174, -0.87066138]])\nb = A @ np.array([1, 1])\n\n# Pseudoinverse\nx1 = np.linalg.inv(A.T @ A) @ A.T @ b\nprint(x1)\n# QR\n[Q, R] = np.linalg.qr(A)\nx2 = np.linalg.solve(R, Q.T @ b)\nprint(x2)\n# np.linalg.lstsq\nx3, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\nprint(x3) \nprint(np.linalg.norm(x1-np.array([1, 1])))\nprint(np.linalg.norm(x2-np.array([1, 1])))\nprint(np.linalg.norm(x3-np.array([1, 1])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1.00010437 0.99918931]\n[1. 1.]\n[1. 1.]\n0.0008173802348693703\n1.3212084675495204e-10\n4.1498941065303375e-10\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n\n# Recap\n\n## Recap\n\nWe have introduced the following concepts:\n\n- vectors,\n- matrices,\n- matrix multiplication,\n- matrix decompositions,\n- linear systems of equations,\n- least squares,\n- eigenvalues and eigenvectors.\n\n> See notes for python implementations of linear algebra operations.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n\n# Appendix: Python Implementations of Linear Algebra Operations\n\n## 1. Vector Operations\n\n### Vector Creation and Basic Properties\n\n::: {#5940d0c5 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import qr, lu, svd, eig, solve_triangular\nfrom scipy.sparse.linalg import eigs\n\n# Create vectors\nx = np.array([2, 2])\ny = np.array([3, -1])\nz = np.array([-2, -1])\n\nprint(f\"Vector x: {x}\")\nprint(f\"Vector y: {y}\")\nprint(f\"Vector z: {z}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVector x: [2 2]\nVector y: [ 3 -1]\nVector z: [-2 -1]\n```\n:::\n:::\n\n\n### Scalar Multiplication\n\n::: {#9bc2c826 .cell execution_count=8}\n``` {.python .cell-code}\n# Scalar multiplication\nc = 2\nscaled_x = c * x\nprint(f\"Scalar multiplication: {c} * {x} = {scaled_x}\")\n\n# Example: Image processing (RGB scaling)\nrgb_color = np.array([0.2, 0.5, 0.7])\ndarkened = 0.5 * rgb_color\nbrightened = np.clip(2 * rgb_color, 0, 1)  # Clip to stay in [0,1]\nprint(f\"Original RGB: {rgb_color}\")\nprint(f\"Darkened: {darkened}\")\nprint(f\"Brightened: {brightened}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScalar multiplication: 2 * [2 2] = [4 4]\nOriginal RGB: [0.2 0.5 0.7]\nDarkened: [0.1  0.25 0.35]\nBrightened: [0.4 1.  1. ]\n```\n:::\n:::\n\n\n### Vector Addition\n\n::: {#07c20769 .cell execution_count=9}\n``` {.python .cell-code}\n# Vector addition\nu = np.array([1, 2])\nv = np.array([4, 1])\nw = u + v\nprint(f\"Vector addition: {u} + {v} = {w}\")\n\n# Example: Sales data\nshop1_sales = np.array([100, 80])  # Monday, Tuesday\nshop2_sales = np.array([20, 50])\ntotal_sales = shop1_sales + shop2_sales\nprint(f\"Total sales: {total_sales}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVector addition: [1 2] + [4 1] = [5 3]\nTotal sales: [120 130]\n```\n:::\n:::\n\n\n### Dot Product\n\n::: {#aee4dcd3 .cell execution_count=10}\n``` {.python .cell-code}\n# Dot product\nu = np.array([1, 2, 0])\nv = np.array([3, -1, 4])\ndot_product = np.dot(u, v)\nprint(f\"Dot product: {u} · {v} = {dot_product}\")\n\n# Alternative syntax\ndot_product_alt = u @ v  # Matrix multiplication operator\nprint(f\"Dot product (alternative): {dot_product_alt}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDot product: [1 2 0] · [ 3 -1  4] = 1\nDot product (alternative): 1\n```\n:::\n:::\n\n\n### Vector Norms\n\n::: {#71567eb0 .cell execution_count=11}\n``` {.python .cell-code}\n# Vector 2-norm (Euclidean norm)\nnorm_u = np.linalg.norm(u)\nnorm_v = np.linalg.norm(v)\nprint(f\"||u||_2 = {norm_u}\")\nprint(f\"||v||_2 = {norm_v}\")\n\n# Manual calculation\nnorm_u_manual = np.sqrt(np.sum(u**2))\nprint(f\"||u||_2 (manual): {norm_u_manual}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n||u||_2 = 2.23606797749979\n||v||_2 = 5.0990195135927845\n||u||_2 (manual): 2.23606797749979\n```\n:::\n:::\n\n\n### Unit Vectors\n\n::: {#61d3803b .cell execution_count=12}\n``` {.python .cell-code}\n# Create unit vector\nunit_u = u / np.linalg.norm(u)\nprint(f\"u: {u}\")\nprint(f\"Unit vector from u: {unit_u}\")\nprint(f\"Norm of unit vector: {np.linalg.norm(unit_u)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nu: [1 2 0]\nUnit vector from u: [0.4472136  0.89442719 0.        ]\nNorm of unit vector: 0.9999999999999999\n```\n:::\n:::\n\n\n### Distance Between Vectors\n\n::: {#5446d6e4 .cell execution_count=13}\n``` {.python .cell-code}\n# Distance between vectors\ndistance = np.linalg.norm(u - v)\nprint(f\"Distance between u and v: {distance}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistance between u and v: 5.385164807134504\n```\n:::\n:::\n\n\n### Orthogonality Check\n\n::: {#839c66f8 .cell execution_count=14}\n``` {.python .cell-code}\n# Check orthogonality\ndef are_orthogonal(vec1, vec2, tolerance=1e-10):\n    return abs(np.dot(vec1, vec2)) < tolerance\n\nprint(f\"Are u and v orthogonal? {are_orthogonal(u, v)}\")\n\n# Example of orthogonal vectors\north1 = np.array([1, 0])\north2 = np.array([0, 1])\nprint(f\"Are orth1 and orth2 orthogonal? {are_orthogonal(orth1, orth2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAre u and v orthogonal? False\nAre orth1 and orth2 orthogonal? True\n```\n:::\n:::\n\n\n### Angle Between Vectors\n\n::: {#5cdac99a .cell execution_count=15}\n``` {.python .cell-code}\n# Angle between vectors\ndef angle_between_vectors(vec1, vec2):\n    cos_theta = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n    # Clamp to avoid numerical errors\n    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n    return np.arccos(cos_theta)\n\nu_2d = np.array([1, 2])\nv_2d = np.array([4, 1])\ntheta = angle_between_vectors(u_2d, v_2d)\nprint(f\"Angle between u and v: {theta} radians = {np.degrees(theta)} degrees\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAngle between u and v: 0.8621700546672264 radians = 49.398705354995535 degrees\n```\n:::\n:::\n\n\n### Vector Projection\n\n::: {#195a261d .cell execution_count=16}\n``` {.python .cell-code}\n# Project u onto v\ndef project_vector(u, v):\n    return (np.dot(u, v) / np.dot(v, v)) * v\n\nprojection = project_vector(u_2d, v_2d)\nprint(f\"Projection of u onto v: {projection}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProjection of u onto v: [1.41176471 0.35294118]\n```\n:::\n:::\n\n\n### Linear Independence Check\n\n::: {#ca273641 .cell execution_count=17}\n``` {.python .cell-code}\n# Check linear independence\ndef check_linear_independence(vectors):\n    \"\"\"Check if a set of vectors is linearly independent\"\"\"\n    matrix = np.column_stack(vectors)\n    rank = np.linalg.matrix_rank(matrix)\n    return rank == len(vectors)\n\n# Example vectors\nv1 = np.array([1, 0])\nv2 = np.array([0, 1])\nv3 = np.array([1, 1])\n\nvectors = [v1, v2, v3]\nis_independent = check_linear_independence(vectors)\nprint(f\"Are vectors {v1}, {v2}, {v3} linearly independent? {is_independent}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAre vectors [1 0], [0 1], [1 1] linearly independent? False\n```\n:::\n:::\n\n\n## 2. Matrix Operations\n\n### Matrix Creation and Basic Properties\n\n::: {#83c184ea .cell execution_count=18}\n``` {.python .cell-code}\n# Create matrices\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nB = np.array([[7, 8],\n              [9, 10],\n              [11, 12]])\n\nprint(f\"Matrix A (2x3):\\n{A}\")\nprint(f\"Matrix B (3x2):\\n{B}\")\nprint(f\"A shape: {A.shape}\")\nprint(f\"B shape: {B.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A (2x3):\n[[1 2 3]\n [4 5 6]]\nMatrix B (3x2):\n[[ 7  8]\n [ 9 10]\n [11 12]]\nA shape: (2, 3)\nB shape: (3, 2)\n```\n:::\n:::\n\n\n### Scalar Multiplication of Matrices\n\n::: {#dd8463ec .cell execution_count=19}\n``` {.python .cell-code}\n# Scalar multiplication\nc = 2\nscaled_A = c * A\nprint(f\"Scalar multiplication:\\n{scaled_A}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScalar multiplication:\n[[ 2  4  6]\n [ 8 10 12]]\n```\n:::\n:::\n\n\n### Matrix Addition\n\n::: {#8880018b .cell execution_count=20}\n``` {.python .cell-code}\n# Matrix addition (matrices must have same dimensions)\nC = np.array([[1, 1, 1],\n              [2, 2, 2]])\nmatrix_sum = A + C\nprint(f\"Matrix addition A + C:\\n{matrix_sum}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix addition A + C:\n[[2 3 4]\n [6 7 8]]\n```\n:::\n:::\n\n\n### Matrix Transpose\n\n::: {#e973f26a .cell execution_count=21}\n``` {.python .cell-code}\n# Matrix transpose\nA_T = A.T\nprint(f\"A transpose:\\n{A_T}\")\n\n# Check if matrix is symmetric\nsymmetric_matrix = np.array([[1, 2, 3],\n                           [2, 4, 5],\n                           [3, 5, 6]])\nis_symmetric = np.allclose(symmetric_matrix, symmetric_matrix.T)\nprint(f\"Is matrix symmetric? {is_symmetric}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA transpose:\n[[1 4]\n [2 5]\n [3 6]]\nIs matrix symmetric? True\n```\n:::\n:::\n\n\n### Matrix-Vector Multiplication\n\n::: {#6c22e95e .cell execution_count=22}\n``` {.python .cell-code}\n# Matrix-vector multiplication\nx = np.array([1, 2, 3])\nresult = A @ x  # or np.dot(A, x)\nprint(f\"Matrix-vector multiplication A @ x:\\n{result}\")\n\n# Interpretation as linear combination of columns\ncol1, col2, col3 = A[:, 0], A[:, 1], A[:, 2]\nlinear_combination = x[0]*col1 + x[1]*col2 + x[2]*col3\nprint(f\"As linear combination of columns:\\n{linear_combination}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix-vector multiplication A @ x:\n[14 32]\nAs linear combination of columns:\n[14 32]\n```\n:::\n:::\n\n\n### Matrix-Matrix Multiplication\n\n::: {#1f4d3d45 .cell execution_count=23}\n``` {.python .cell-code}\n# Matrix-matrix multiplication\nproduct = A @ B\nprint(f\"Matrix multiplication A @ B:\\n{product}\")\n\n# Element-wise calculation (for demonstration)\ndef matrix_multiply_manual(A, B):\n    m, n = A.shape\n    n_b, p = B.shape\n    assert n == n_b, \"Incompatible dimensions\"\n    \n    C = np.zeros((m, p))\n    for i in range(m):\n        for j in range(p):\n            C[i, j] = np.sum(A[i, :] * B[:, j])\n    return C\n\nmanual_product = matrix_multiply_manual(A, B)\nprint(f\"Manual matrix multiplication:\\n{manual_product}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix multiplication A @ B:\n[[ 58  64]\n [139 154]]\nManual matrix multiplication:\n[[ 58.  64.]\n [139. 154.]]\n```\n:::\n:::\n\n\n## 3. Special Matrices\n\n### Identity Matrix\n\n::: {#2544cda6 .cell execution_count=24}\n``` {.python .cell-code}\n# Identity matrix\nI_3 = np.eye(3)\nprint(f\"3x3 Identity matrix:\\n{I_3}\")\n\n# Verify identity property\nsquare_matrix = np.array([[1, 2], [3, 4]])\nI_2 = np.eye(2)\nprint(f\"A @ I = I @ A:\")\nprint(f\"A @ I:\\n{square_matrix @ I_2}\")\nprint(f\"I @ A:\\n{I_2 @ square_matrix}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3x3 Identity matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nA @ I = I @ A:\nA @ I:\n[[1. 2.]\n [3. 4.]]\nI @ A:\n[[1. 2.]\n [3. 4.]]\n```\n:::\n:::\n\n\n### Matrix Inverse\n\n::: {#ca207653 .cell execution_count=25}\n``` {.python .cell-code}\n# Matrix inverse\nA_square = np.array([[1, 2], [3, 4]])\nA_inv = np.linalg.inv(A_square)\nprint(f\"Matrix A:\\n{A_square}\")\nprint(f\"Matrix A inverse:\\n{A_inv}\")\n\n# Verify inverse property\nprint(f\"A @ A_inv:\\n{A_square @ A_inv}\")\nprint(f\"A_inv @ A:\\n{A_inv @ A_square}\")\n\n# Check if matrix is invertible\ndef is_invertible(matrix):\n    return np.linalg.det(matrix) != 0\n\nprint(f\"Is A invertible? {is_invertible(A_square)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[1 2]\n [3 4]]\nMatrix A inverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\nA @ A_inv:\n[[1.0000000e+00 0.0000000e+00]\n [8.8817842e-16 1.0000000e+00]]\nA_inv @ A:\n[[1.00000000e+00 0.00000000e+00]\n [1.11022302e-16 1.00000000e+00]]\nIs A invertible? True\n```\n:::\n:::\n\n\n### Diagonal Matrices\n\n::: {#0564f0c7 .cell execution_count=26}\n``` {.python .cell-code}\n# Diagonal matrix\ndiagonal_values = [1, 2, 3]\nD = np.diag(diagonal_values)\nprint(f\"Diagonal matrix:\\n{D}\")\n\n# Extract diagonal elements\ndiag_elements = np.diag(D)\nprint(f\"Diagonal elements: {diag_elements}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDiagonal matrix:\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nDiagonal elements: [1 2 3]\n```\n:::\n:::\n\n\n### Orthogonal Matrices\n\n::: {#1ff323c4 .cell execution_count=27}\n``` {.python .cell-code}\n# Rotation matrix (orthogonal)\ntheta = np.pi/4  # 45 degrees\nrotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                           [np.sin(theta), np.cos(theta)]])\nprint(f\"Rotation matrix (45°):\\n{rotation_matrix}\")\n\n# Verify orthogonality\nis_orthogonal = np.allclose(rotation_matrix @ rotation_matrix.T, np.eye(2))\nprint(f\"Is rotation matrix orthogonal? {is_orthogonal}\")\n\n# 90-degree rotation matrix\nrotation_90 = np.array([[0, -1], [1, 0]])\nprint(f\"90° rotation matrix:\\n{rotation_90}\")\nprint(f\"Is 90° rotation orthogonal? {np.allclose(rotation_90 @ rotation_90.T, np.eye(2))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRotation matrix (45°):\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nIs rotation matrix orthogonal? True\n90° rotation matrix:\n[[ 0 -1]\n [ 1  0]]\nIs 90° rotation orthogonal? True\n```\n:::\n:::\n\n\n### Triangular Matrices\n\n::: {#6d8c302f .cell execution_count=28}\n``` {.python .cell-code}\n# Lower triangular matrix\nL = np.array([[1, 0, 0],\n              [2, 3, 0],\n              [4, 5, 6]])\nprint(f\"Lower triangular matrix:\\n{L}\")\n\n# Upper triangular matrix\nU = np.array([[1, 2, 3],\n              [0, 4, 5],\n              [0, 0, 6]])\nprint(f\"Upper triangular matrix:\\n{U}\")\n\n# Extract lower/upper triangular parts\nmatrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nlower_part = np.tril(matrix)\nupper_part = np.triu(matrix)\nprint(f\"Lower triangular part:\\n{lower_part}\")\nprint(f\"Upper triangular part:\\n{upper_part}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLower triangular matrix:\n[[1 0 0]\n [2 3 0]\n [4 5 6]]\nUpper triangular matrix:\n[[1 2 3]\n [0 4 5]\n [0 0 6]]\nLower triangular part:\n[[1 0 0]\n [4 5 0]\n [7 8 9]]\nUpper triangular part:\n[[1 2 3]\n [0 5 6]\n [0 0 9]]\n```\n:::\n:::\n\n\n## 4. Matrix Decompositions\n\n### LU Decomposition\n\n::: {#b104718e .cell execution_count=29}\n``` {.python .cell-code}\n# LU decomposition\nA_square = np.array([[4, 3], [6, 3]], dtype=float)\nP, L, U = lu(A_square)\nprint(f\"Original matrix A:\\n{A_square}\")\nprint(f\"P (permutation):\\n{P}\")\nprint(f\"L (lower triangular):\\n{L}\")\nprint(f\"U (upper triangular):\\n{U}\")\nprint(f\"Verification P@L@U:\\n{P @ L @ U}\")\n\n# Solve system using LU decomposition\ndef solve_with_lu(A, b):\n    P, L, U = lu(A)\n    # Solve Ly = Pb for y\n    Pb = P @ b\n    y = solve_triangular(L, Pb, lower=True)\n    # Solve Ux = y for x\n    x = solve_triangular(U, y, lower=False)\n    return x\n\nb = np.array([7, 9])\nx_lu = solve_with_lu(A_square, b)\nprint(f\"Solution using LU: {x_lu}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal matrix A:\n[[4. 3.]\n [6. 3.]]\nP (permutation):\n[[0. 1.]\n [1. 0.]]\nL (lower triangular):\n[[1.         0.        ]\n [0.66666667 1.        ]]\nU (upper triangular):\n[[6. 3.]\n [0. 1.]]\nVerification P@L@U:\n[[4. 3.]\n [6. 3.]]\nSolution using LU: [1. 1.]\n```\n:::\n:::\n\n\n### QR Decomposition\n\n> FIXME: This is not working.\n\n```python\n# QR decomposition\nA_rect = np.array([[1, 1], [1, -1]], dtype=float)\nQ, R = qr(A_rect)\nprint(f\"Original matrix A:\\n{A_rect}\")\nprint(f\"Q (orthogonal):\\n{Q}\")\nprint(f\"R (upper triangular):\\n{R}\")\nprint(f\"Verification Q@R:\\n{Q @ R}\")\n\n# Verify Q is orthogonal\nprint(f\"Q@Q.T (should be identity):\\n{Q @ Q.T}\")\n\n# Solve least squares using QR\ndef solve_least_squares_qr(A, b):\n    Q, R = qr(A)\n    return solve_triangular(R, Q.T @ b, lower=False)\n\n# Example overdetermined system\nA_over = np.array([[1, 1], [1, 2], [1, 3]], dtype=float)\nb_over = np.array([2, 3, 4])\nx_qr = solve_least_squares_qr(A_over, b_over)\nprint(f\"Least squares solution using QR: {x_qr}\")\n```\n\n### Eigendecomposition\n\n::: {#53241f1f .cell execution_count=30}\n``` {.python .cell-code}\n# Eigenvalues and eigenvectors\nsymmetric_A = np.array([[2, 1], [1, 2]], dtype=float)\neigenvalues, eigenvectors = eig(symmetric_A)\nprint(f\"Matrix A:\\n{symmetric_A}\")\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\\n{eigenvectors}\")\n\n# Verify eigenvalue equation: Av = λv\nfor i, (lam, v) in enumerate(zip(eigenvalues, eigenvectors.T)):\n    Av = symmetric_A @ v\n    lambda_v = lam * v\n    print(f\"Eigenvalue {i+1}: λ={lam:.3f}\")\n    print(f\"Av = {Av}\")\n    print(f\"λv = {lambda_v}\")\n    print(f\"Close? {np.allclose(Av, lambda_v)}\\n\")\n\n# Eigendecomposition: A = XΛX^(-1)\nX = eigenvectors\nLambda = np.diag(eigenvalues)\nA_reconstructed = X @ Lambda @ np.linalg.inv(X)\nprint(f\"Reconstructed A:\\n{A_reconstructed}\")\nprint(f\"Original A:\\n{symmetric_A}\")\nprint(f\"Reconstruction accurate? {np.allclose(A_reconstructed, symmetric_A)}\")\n\n# For symmetric matrices: A = XΛX^T\nif np.allclose(symmetric_A, symmetric_A.T):\n    A_symmetric_recon = X @ Lambda @ X.T\n    print(f\"Symmetric reconstruction:\\n{A_symmetric_recon}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix A:\n[[2. 1.]\n [1. 2.]]\nEigenvalues: [3.+0.j 1.+0.j]\nEigenvectors:\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nEigenvalue 1: λ=3.000+0.000j\nAv = [2.12132034 2.12132034]\nλv = [2.12132034+0.j 2.12132034+0.j]\nClose? True\n\nEigenvalue 2: λ=1.000+0.000j\nAv = [-0.70710678  0.70710678]\nλv = [-0.70710678+0.j  0.70710678+0.j]\nClose? True\n\nReconstructed A:\n[[2.+0.j 1.+0.j]\n [1.+0.j 2.+0.j]]\nOriginal A:\n[[2. 1.]\n [1. 2.]]\nReconstruction accurate? True\nSymmetric reconstruction:\n[[2.+0.j 1.+0.j]\n [1.+0.j 2.+0.j]]\n```\n:::\n:::\n\n\n### Singular Value Decomposition (SVD)\n\n::: {#cf41abc3 .cell execution_count=31}\n``` {.python .cell-code}\n# SVD decomposition\nA_rect = np.array([[3, 1], [1, 3]], dtype=float)\nU, sigma, Vt = svd(A_rect)\nprint(f\"Original matrix A:\\n{A_rect}\")\nprint(f\"U (left singular vectors):\\n{U}\")\nprint(f\"Singular values: {sigma}\")\nprint(f\"V^T (right singular vectors transposed):\\n{Vt}\")\n\n# Reconstruct matrix\nSigma = np.diag(sigma)\nA_reconstructed = U @ Sigma @ Vt\nprint(f\"Reconstructed A:\\n{A_reconstructed}\")\nprint(f\"Reconstruction accurate? {np.allclose(A_reconstructed, A_rect)}\")\n\n# Low-rank approximation\ndef low_rank_approximation(A, k):\n    U, sigma, Vt = svd(A)\n    return U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n\n# Example: approximate with rank 1\nA_rank1 = low_rank_approximation(A_rect, 1)\nprint(f\"Rank-1 approximation:\\n{A_rank1}\")\n\n# SVD for rectangular matrix\nA_tall = np.array([[1, 2], [3, 4], [5, 6]], dtype=float)\nU_tall, sigma_tall, Vt_tall = svd(A_tall)\nprint(f\"Tall matrix A:\\n{A_tall}\")\nprint(f\"U shape: {U_tall.shape}\")\nprint(f\"Sigma shape: {sigma_tall.shape}\")\nprint(f\"Vt shape: {Vt_tall.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal matrix A:\n[[3. 1.]\n [1. 3.]]\nU (left singular vectors):\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nSingular values: [4. 2.]\nV^T (right singular vectors transposed):\n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nReconstructed A:\n[[3. 1.]\n [1. 3.]]\nReconstruction accurate? True\nRank-1 approximation:\n[[2. 2.]\n [2. 2.]]\nTall matrix A:\n[[1. 2.]\n [3. 4.]\n [5. 6.]]\nU shape: (3, 3)\nSigma shape: (2,)\nVt shape: (2, 2)\n```\n:::\n:::\n\n\n## 5. Linear Systems and Least Squares\n\n### Solving Linear Systems\n\n::: {#4da12a70 .cell execution_count=32}\n``` {.python .cell-code}\n# Solve Ax = b for square, invertible A\nA_system = np.array([[2, 1], [1, -1]], dtype=float)\nb_system = np.array([5, 1])\n\n# Method 1: Direct inverse (not recommended for large systems)\nx_inverse = np.linalg.inv(A_system) @ b_system\nprint(f\"Solution using inverse: {x_inverse}\")\n\n# Method 2: Using numpy.linalg.solve (recommended)\nx_solve = np.linalg.solve(A_system, b_system)\nprint(f\"Solution using solve: {x_solve}\")\n\n# Verify solution\nprint(f\"Verification A@x = b: {A_system @ x_solve}\")\nprint(f\"Original b: {b_system}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSolution using inverse: [2. 1.]\nSolution using solve: [2. 1.]\nVerification A@x = b: [5. 1.]\nOriginal b: [5 1]\n```\n:::\n:::\n\n\n### Forward and Backward Substitution\n\n::: {#fc062aaa .cell execution_count=33}\n``` {.python .cell-code}\n# Forward substitution for lower triangular system Ly = b\ndef forward_substitution(L, b):\n    n = len(b)\n    y = np.zeros(n)\n    for i in range(n):\n        y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]\n    return y\n\n# Backward substitution for upper triangular system Ux = y\ndef backward_substitution(U, y):\n    n = len(y)\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n    return x\n\n# Example\nL_example = np.array([[1, 0], [2, 3]], dtype=float)\nb_example = np.array([1, 4])\ny_forward = forward_substitution(L_example, b_example)\nprint(f\"Forward substitution result: {y_forward}\")\n\nU_example = np.array([[4, 1], [0, 5]], dtype=float)\ny_example = np.array([9, 10])\nx_backward = backward_substitution(U_example, y_example)\nprint(f\"Backward substitution result: {x_backward}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nForward substitution result: [1.         0.66666667]\nBackward substitution result: [1.75 2.  ]\n```\n:::\n:::\n\n\n### Least Squares Problems\n\n> FIXME: This is not working.\n\n```python\n# Overdetermined system (more equations than unknowns)\nA_over = np.array([[-1.42382504, -1.4238264],\n                   [1.26372846, 1.26372911],\n                   [-0.87066174, -0.87066138]])\nb_over = A_over @ np.array([1, 1])  # Known solution\n\n# Method 1: Normal equations (can be numerically unstable)\nx_normal = np.linalg.inv(A_over.T @ A_over) @ A_over.T @ b_over\nprint(f\"Normal equations solution: {x_normal}\")\n\n# Method 2: QR decomposition (more stable)\nQ, R = qr(A_over)\nx_qr = solve_triangular(R, Q.T @ b_over, lower=False)\nprint(f\"QR solution: {x_qr}\")\n\n# Method 3: NumPy's least squares (most robust)\nx_lstsq, residuals, rank, s = np.linalg.lstsq(A_over, b_over, rcond=None)\nprint(f\"lstsq solution: {x_lstsq}\")\n\n# Compare errors\ntrue_solution = np.array([1, 1])\nprint(f\"Normal equations error: {np.linalg.norm(x_normal - true_solution)}\")\nprint(f\"QR error: {np.linalg.norm(x_qr - true_solution)}\")\nprint(f\"lstsq error: {np.linalg.norm(x_lstsq - true_solution)}\")\n\n# Pseudoinverse approach\nx_pinv = np.linalg.pinv(A_over) @ b_over\nprint(f\"Pseudoinverse solution: {x_pinv}\")\n```\n\n### Computing Residuals and Fit Quality\n\n> FIXME: This is not working.\n\n```python\n# Compute residuals and R-squared\ndef compute_fit_statistics(A, x, b):\n    y_pred = A @ x\n    residuals = b - y_pred\n    ss_res = np.sum(residuals**2)\n    ss_tot = np.sum((b - np.mean(b))**2)\n    r_squared = 1 - (ss_res / ss_tot)\n    \n    return {\n        'residuals': residuals,\n        'ss_res': ss_res,\n        'r_squared': r_squared,\n        'rmse': np.sqrt(ss_res / len(b))\n    }\n\nstats = compute_fit_statistics(A_over, x_lstsq, b_over)\nprint(f\"Fit statistics:\")\nprint(f\"R-squared: {stats['r_squared']:.6f}\")\nprint(f\"RMSE: {stats['rmse']:.6f}\")\nprint(f\"Sum of squared residuals: {stats['ss_res']:.6f}\")\n```\n\n## 6. Advanced Operations and Applications\n\n### Matrix Rank and Condition Number\n\n::: {#1b1c2e3f .cell execution_count=34}\n``` {.python .cell-code}\n# Matrix rank\nA_rank = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nrank = np.linalg.matrix_rank(A_rank)\nprint(f\"Matrix rank: {rank}\")\n\n# Condition number\nA_cond = np.array([[1, 2], [2, 4.0001]])  # Nearly singular\ncond_num = np.linalg.cond(A_cond)\nprint(f\"Condition number: {cond_num}\")\nprint(f\"Matrix is {'well-conditioned' if cond_num < 1e12 else 'ill-conditioned'}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix rank: 2\nCondition number: 250008.00009100154\nMatrix is well-conditioned\n```\n:::\n:::\n\n\n### Matrix Powers and Matrix Functions\n\n::: {#3ca82f63 .cell execution_count=35}\n``` {.python .cell-code}\n# Matrix power\nA_power = np.array([[2, 1], [1, 2]])\nA_squared = np.linalg.matrix_power(A_power, 2)\nprint(f\"A^2:\\n{A_squared}\")\n\n# Matrix exponential\nfrom scipy.linalg import expm, logm, sqrtm\n\nA_exp = expm(A_power)\nprint(f\"Matrix exponential e^A:\\n{A_exp}\")\n\n# Matrix square root\nA_sqrt = sqrtm(A_power)\nprint(f\"Matrix square root:\\n{A_sqrt}\")\nprint(f\"Verification (sqrt(A))^2:\\n{A_sqrt @ A_sqrt}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA^2:\n[[5 4]\n [4 5]]\nMatrix exponential e^A:\n[[11.40190938  8.68362755]\n [ 8.68362755 11.40190938]]\nMatrix square root:\n[[1.3660254 0.3660254]\n [0.3660254 1.3660254]]\nVerification (sqrt(A))^2:\n[[2. 1.]\n [1. 2.]]\n```\n:::\n:::\n\n\n### Kronecker Product\n\n::: {#18b4369b .cell execution_count=36}\n``` {.python .cell-code}\n# Kronecker product\nA_kron = np.array([[1, 2], [3, 4]])\nB_kron = np.array([[5, 6], [7, 8]])\nkron_product = np.kron(A_kron, B_kron)\nprint(f\"Kronecker product A ⊗ B:\\n{kron_product}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKronecker product A ⊗ B:\n[[ 5  6 10 12]\n [ 7  8 14 16]\n [15 18 20 24]\n [21 24 28 32]]\n```\n:::\n:::\n\n\n### Vector and Matrix Norms\n\n::: {#3cdb14c3 .cell execution_count=37}\n``` {.python .cell-code}\n# Different vector norms\nv_norm = np.array([1, -2, 3])\nl1_norm = np.linalg.norm(v_norm, ord=1)  # L1 norm\nl2_norm = np.linalg.norm(v_norm, ord=2)  # L2 norm (default)\nlinf_norm = np.linalg.norm(v_norm, ord=np.inf)  # L∞ norm\n\nprint(f\"Vector: {v_norm}\")\nprint(f\"L1 norm: {l1_norm}\")\nprint(f\"L2 norm: {l2_norm}\")\nprint(f\"L∞ norm: {linf_norm}\")\n\n# Matrix norms\nA_norm = np.array([[1, 2], [3, 4]])\nfrobenius_norm = np.linalg.norm(A_norm, ord='fro')\nspectral_norm = np.linalg.norm(A_norm, ord=2)  # Largest singular value\n\nprint(f\"Matrix Frobenius norm: {frobenius_norm}\")\nprint(f\"Matrix spectral norm: {spectral_norm}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVector: [ 1 -2  3]\nL1 norm: 6.0\nL2 norm: 3.7416573867739413\nL∞ norm: 3.0\nMatrix Frobenius norm: 5.477225575051661\nMatrix spectral norm: 5.464985704219043\n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "04-Linear-Algebra-Refresher_files"
    ],
    "filters": [],
    "includes": {}
  }
}