{
  "hash": "8a767c393481bda124a12cf9565fad88",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Algebra Refresher\njupyter: python3\n---\n\n\n\n\nLinear alegbra is the branch of mathematics involving vectors and matrices. In particular, how vectors are transformed.  Knowledge of linear algebra is essential in data science. This is because linear algebra underpins the representation of data and the algorithms that transform this data.\n\nThis lecture is a review of some aspects of linear algebra that are important for data science. Given the prerequisites for this course, I assume that you previously learned this material. \n\nThe goal of this lecture is to refresh the following topics:\n\n- vectors,\n- matrices,\n- operations with vectors and matrices,\n- eigenvectors and eigenvalues,\n- linear systems and least squares,\n- matrix factorizations.\n\nBelow is a list of very useful resrouces for learning about linear algebra:\n\n- __Linear Algebra and Its Applications (6th edition)__, David C. Lay, Judi J. McDonald, and Steven R. Lay, Pearson, 2021,\n- __Introduction to Linear Algebra (6th edition)__, Gilbert Strang, Wellesley-Cambridge Press, 2023,\n- __Linear Algebra and Learning from Data__, Gilbert Strang, Wellesley-Cambridge Press, 2019,\n- __Numerical Linear Algebra__, Lloyn N. Trefethen and David Bau, SIAM, 1997.\n\n\n## Vectors\n\nA vector of length $n$, $\\mathbf{x}\\in\\mathbb{R}^{n}$, is a 1-dimensional (1-D) array of real numbers\n\n$$\n\\mathbf{x} =\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n$$\n\n\nWhen discussing vectors we will only consider column vectors. A row vector can always be obtained from a column vector via transposition\n\n$$\n\\mathbf{x}^{T} = [x_1, x_2, \\ldots, x_n].\n$$\n\n\nVectors in $\\mathbb{R}^{2}$ can be visualized as points in a 2-D plane, whereas vectors in $\\mathbb{R}^{3}$ can be visualized as points in a 3-D space.\n\nLet \n\n$$\\mathbf{x}=\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},~\n\\mathbf{y} = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix},~\n\\mathbf{z} = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}.\n$$\n\nThese vectors are illustrated in @fig-vector-viz.\n\n::: {#cell-fig-vector-viz .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nx = np.array([2, 2])\ny = np.array([3, -1])\nz = np.array([-2, -1])\nV = np.array([x, y, z])\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['r', 'b', 'g'], \n           angles='xy', \n           scale_units='xy', \n           scale=1)\nax.set_xlim([-6, 6])\nax.set_ylim([-2, 4])\nax.text(3.3, -1.1, '$(3,-1)$', size=16)\nax.text(2.3, 1.9, '$(2,2)$', size=16)\nax.text(-3.7, -1.3, '$(-2,-1)$', size=16);\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Illustration of vectors](04-Linear-Algebra-Refresher_files/figure-html/fig-vector-viz-output-1.png){#fig-vector-viz width=573 height=416}\n:::\n:::\n\n\nRecall the following definitions.\n\n 1. Scalar multiplication: Let $c\\in\\mathbb{R}$, $\\mathbf{x}\\in\\mathbb{R}^{n}$, then $$c\\mathbf{x} = \\begin{bmatrix} cx_1 \\\\ cx_2 \\\\ \\vdots \\\\ cx_n \\end{bmatrix}.$$\n 1. Vector addition: Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ then\n$$ \\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}.$$\n1. Dot product: Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ then the dot product is defined as\n$$ \\mathbf{u}\\cdot\\mathbf{v} = \\sum_{i=0}^n u_i v_i.$$\n1. Vector 2-norm: The 2-norm of a vector $\\mathbf{v}\\in\\mathbb{R}^{n}$ is defined as\n    $$\\Vert \\mathbf{v}\\Vert_2 = \\sqrt{\\mathbf{v}\\cdot\\mathbf{v}} = \\sqrt{\\sum_{i=1}^n v_i^2}.$$ \n    This norm is referred to as the $\\ell_2$ norm. In these notes, the notation $\\Vert \\mathbf{v} \\Vert$, indicates the 2-norm.\n1. A unit vector $\\mathbf{v}$ is a vector such that $\\Vert \\mathbf{v} \\Vert_2 = 1$. All vectors of the form $\\frac{\\mathbf{v}}{\\Vert \\mathbf{v} \\Vert_2 }$ are unit vectors.\n1. Distance: Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$, the distance between $\\mathbf{u}$ and $\\mathbf{v}$ is\n$$ \\Vert \\mathbf{u} - \\mathbf{v} \\Vert_2.$$\n1. Orthogonality: Two vectors $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$ are orthogonal if and only if $\\mathbf{u}\\cdot\\mathbf{v}=0$. \n1. Angle between vectors:  Let $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{n}$, the angle between these vectors is $$\\cos{\\theta} = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\Vert \\mathbf{u}\\Vert_2 \\Vert\\mathbf{v}\\Vert_2}.$$\n1. A set of $n$ vectors $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{n}\\in\\mathbf{R}^n$ is linearly dependent if there exists scalars $c_1,\\ldots, c_n$ not all zero such that \n    $$\n    \\sum_{i=1}^{n} a_i \\mathbf{v}_i = 0.\n    $$\n\n    The vectors $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{n}$ are linearly independent if they are not linearly independent, i.e., the equation\n    $$\n    a_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n = 0,\n    $$\n    is only satisfied if $a_i=0$ for $i=1, \\ldots,n$.\n\n### Visualizing vector operations\n\nFor helpful visualizations of the above defintions, we'll restrict ourselves to $\\mathbb{R}^{2}.$\n\n#### Scalar multiplication\n\nMultiplication by a scalar $c\\in\\mathbb{R}$. For $c>1$ the vector is lengthened. For $0<c<1$ the vector shrinks. If we negate $c$ the direction of the vector is flipped 180 degrees. This is shown in @fig-vector-scaling. In this figure we show the vector $\\mathbf{x} = [2, 2]$ multiplied by the scalar value $c=2$.\n\n::: {#cell-fig-vector-scaling .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nx = np.array([2, 2])\ny = np.array([4, 4])\nV = np.array([x, y])\norigin = np.array([[0, 0], [0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['r', 'b'], \n           angles='xy', \n           scale_units='xy', \n           scale=1,\n           alpha= 0.5)\nax.set_xlim([-5, 5])\nax.set_ylim([-1, 5])\nax.text(2.3, 1.9, '$x$', size=16)\nax.text(4.3, 3.9, '$cx$', size=16)\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scalar multiplication of a vector](04-Linear-Algebra-Refresher_files/figure-html/fig-vector-scaling-output-1.png){#fig-vector-scaling width=569 height=416}\n:::\n:::\n\n\n#### Vector addition\n\nWe plot the sum of $\\mathbf{u} = [1, 2]$ and $\\mathbf{v} = [4, 1]$ in @fig-vector-addition. The sum $\\mathbf{u} + \\mathbf{v} = [5, 3]$ is obtained by placing the tip of one vector to the tail of the other vector. The sum is the vector that connects the unconnected tip to the unconnected tail. \n\n::: {#cell-fig-vector-addition .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nu = np.array([1, 2])\nv = np.array([4, 1])\nw = np.array([5, 3])\nV = np.array([u, v, w])\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['b', 'b', 'r'], \n           angles='xy', \n           scale_units='xy', \n           scale=1)\nax.set_xlim([-1, 6])\nax.set_ylim([-1, 4])\nax.text(1.3, 1.9, '$u$', size=16)\nax.text(4.3, 1.2, '$v$', size=16)\nax.text(5.3, 2.9, '$u+v$', size=16)\nplt.plot([1, 5], [2, 3], 'g--')\nplt.plot([4, 5], [1, 3], 'g--')\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Vector addition](04-Linear-Algebra-Refresher_files/figure-html/fig-vector-addition-output-1.png){#fig-vector-addition width=573 height=416}\n:::\n:::\n\n\n#### Dot prodcut\n\nThe dot product of two vectors $\\mathbf{u}, \\mathbf{v}$ can be used to project $\\mathbf{u}$ onto $\\mathbf{v}$. This is illustrated in Figure @fig-dot-product.\n\n::: {#cell-fig-dot-product .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure()\nax = plt.gca()\nu = np.array([1, 2])\nv = np.array([4, 1])\nw = np.dot(u, v) * v / np.dot(v, v)\nV = np.array([u, v, w])\norigin = np.array([[0, 0, 0], [0, 0, 0]])\nplt.quiver(*origin, V[:, 0], V[:, 1], \n           color=['b', 'b', 'r'], \n           angles='xy', \n           scale_units='xy', \n           scale=1)\nax.set_xlim([-1, 6])\nax.set_ylim([-1, 4])\nax.text(1.3, 1.9, '$u$', size=16)\nax.text(4.3, 1.2, '$v$', size=16)\nax.text(0.4, -0.3, r'$\\frac{u\\cdot v}{\\Vert v \\Vert}$', size=16)\nplt.plot([u[0], w[0]], [u[1], w[1]], 'g--')\n# plt.plot([4, 5], [1, 3], 'g--')\nax.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Dot product](04-Linear-Algebra-Refresher_files/figure-html/fig-dot-product-output-1.png){#fig-dot-product width=573 height=416}\n:::\n:::\n\n\nObserve that a right angle forms between the vectors $\\mathbf{u}$ and $\\mathbf{v}$ when $\\mathbf{u}\\cdot \\mathbf{v} = 0$. \n\n## Matrices\n\nA matrix $A\\in\\mathbb{R}^{m\\times n}$ is a 2-D array of numbers\n\n$$\nA = \n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} &a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix},\n$$\n\nwith $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.\n\nSimilar to vectors, we can multiply matrices by scalar values and add matrices of the same dimension, i.e.,\n\n1. Let $c\\in\\mathbb{R}$ and $A\\in\\mathbb{R}^{m\\times n}$, then\n$$\ncA =\n\\begin{bmatrix}\nca_{11} & ca_{12} & \\cdots & ca_{1n} \\\\\nca_{21} & ca_{22} & \\cdots & ca_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nca_{m1} & ca_{m2} & \\cdots & ca_{mn} \\\\\n\\end{bmatrix}.\n$$\n1. Let $A, B\\in\\mathbb{R}^{m\\times n}$, then \n$$\nA + B =\n\\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn} \\\\\n\\end{bmatrix}\n$$\n\nThe transpose $A^{T}$ is defined as\n\n$$\nA^{T} = \n\\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} &a_{2n} & \\cdots & a_{nm} \\\\\n\\end{bmatrix}.\n$$\n\nThe transpose turns columns of the matrix into rows (equivalently rows into columns). A square matrix is called symmetric if $A=A^{T}$.\n\n## Matrix multiplication\n\nWe discuss the following two important matrix multiplication operations\n\n- matrix-vector multiplication,\n- matrix-matrix multiplication.\n\n### Matrix-vector multiplication\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{R}^{n}$, then $A\\mathbf{x}\\in\\mathbb{R}^{m}$ is defined as \n\n$$\nA\\mathbf{x} = \n\\begin{bmatrix}\nx_1a_{11} + x_2 a_{12} + \\cdots + x_na_{1n} \\\\\nx_1a_{21} + x_2 a_{22} + \\cdots + x_na_{2n} \\\\\n\\vdots \\\\\nx_1a_{m1} + x_2 a_{m2} + \\cdots + x_na_{mn} \\\\\n\\end{bmatrix}.\n$$\n\nEquivalently, this means that $A\\mathbf{x}$ is a linear combination of the columns of $A$, i.e.,\n\n$$\nA\\mathbf{x} = \nx_1 \\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1}  \\end{bmatrix} \n+ \nx_2  \\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2}  \\end{bmatrix}\n+\n\\cdots\n+\nx_n \\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn}  \\end{bmatrix}.\n$$\n\nObserve that the matrix $A$ is a linear transformation that maps vectors in $R^{n}$ to $\\mathbb{R}^{m}$.\n\n### Matrix-matrix multiplication\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ and $B\\in\\mathbb{R}^{n\\times p}$, then the elements of $C=AB\\in\\mathbb{R}^{m\\times p}$ are\n\n$$\nc_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj},\n$$\n\nfor $i=1,\\ldots, m$ and $j=1, \\ldots, p$.\n\n## Important matrices\n\nWe introduce notation for some commonly used and important matrices.\n\nThe $n \\times n$ identity matrix is\n\n$$\nI = \n\\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1 \\\\\n\\end{bmatrix}.\n$$.\n\nFor every $A\\in\\mathbb{R}^{n\\times n}$, then $AI = IA$. \n\nThe inverse $A^{-1}\\in\\mathbb{R}^{n\\times n}$ is defined as the matrix for which $AA^{-1} = A^{-1}A = I$. When $A^{-1}$ exists the matrix is said to be invertible. Note that $(AB)^{-1} = B^{-1}A^{-1}$ for invertible $B\\in\\mathbb{R}^{n\\times n}$.\n\nA diagonal matrix $D\\in\\mathbb{R}^{n\\times n}$ has entries $d_{ij}=0$ if $i\\neq j$, i.e.,\n\n$$\nD =\n\\begin{bmatrix}\nd_{11} & 0 & \\cdots & 0 \\\\\n0 & d_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{nn} \\\\\n\\end{bmatrix}.\n$$\n\nA square matrix $Q\\in\\mathbb{R}^{n}$ is orthogonal if $QQ^{T}=Q^{T}Q=I$. In particular, the inverse of an orthogonal matrix is it's transpose.\n\nA lower triangular matrix $L\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries above the main diagonal are zero\n\n$$\nL =\n\\begin{bmatrix}\nl_{11} & 0 & \\cdots & 0 \\\\\nl_{12} & l_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{1n} & l_{n2} & \\cdots & l_{nn} \\\\\n\\end{bmatrix}.\n$$\n\nAn upper triangular matrix $U\\in\\mathbb{R}^{n\\times n}$ is a matrix where all the entries below the main diagonal are zero\n\n$$\nU = \n\\begin{bmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n} \\\\\n0 & u_{22} & \\cdots & u_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & u_{nn} \\\\\n\\end{bmatrix}.\n$$\n\nThe inverse of a lower triangular matrix is itself a lower triangular matrix. This is also true for upper triangular matrices, i.e., the inverse is also upper triangular.\n\n## Eigenvalues and eigenvectors\n\nAn eigenvector of an $n\\times n$ matrix $A$ is a nonzero vector $\\mathbf{x}$ such that $A\\mathbf{x} = \\lambda\\mathbf{x}$ for some scalar $\\lambda.$  The scalar $\\lambda$ is called an eigenvalue.\n\nAn $n \\times n$ matrix has at most $n$ distinct eigenvectors and at most $n$ distinct eigenvalues.\n\n## Matrix decompositions\n\nWe introduce here important matrix decompositions. These are useful in solving linear equations. Furthermore they play an important role in various data science applications.\n\n### LU factorization\n\nAn LU decomposition of a square matrix $A\\in\\mathbb{R}^{n\\times n}$ is a factorization of $A$ into a product of matrices\n\n$$ \nA = LU,\n$$\n\nwhere $L$ is a lower triangular square matrix and $U$ is an triangular square matrix. For example, when $n=3$, we have\n\n$$ \n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nl_{11} & 0 & 0 \\\\\nl_{21} & l_{22} & 0\\\\\nl_{31} & l_{32} & a_{33} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nu_{11} & u_{12} & u_{13} \\\\\n0 & u_{22} & u_{23} \\\\\n0 & 0 & u_{33} \\\\\n\\end{bmatrix}\n$$\n\n\n### QR decomposition\n\nA QR decomposition of a square matrix $A\\in\\mathbb{R}^{n\\times n}$ is a factorization of $A$ into a product of matrices\n\n$$\nA=QR,\n$$\nwhere $Q$ is an orthogonal square matrix and $R$ is an upper-triangular square matrix.\n\n### Eigendecomposition\n\nLet $A\\in\\mathbb{R}^{n}$ have n linearly independent eigenvectors $\\mathbf{x}_i$ for $i=1,\\ldots, n$, then $A$ can be factorized as\n\n$$\nA = X\\Lambda X^{-1},\n$$\nwhere $X$ is a matrix of the eigenvectors, and \n\n$$\n\\Lambda =\n\\begin{bmatrix}\n\\lambda_{1} & 0 & \\cdots & 0 \\\\\n0 & \\lambda_{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_{n}  \\\\\n\\end{bmatrix},\n$$\nis a diagonal matrix of the eigenvalues. \n\nIn this case, the matrix $A$ is said to be diagonalizable.\n\nA special case occurs when $A$ is symmetric. Recall that a matrix is symmetric when $A = A^T.$\n\nIn this case, it can be shown that the eigenvectors of $A$ are all mutually orthogonal. Consequently, $X^{-1} = X^{T}$ and we can decompose $A$ as:\n\n$$A = XDX^T.$$\n\nThis amazing fact is known as the spectral theorem and this decomposition of $A$ is its spectral decomposition. The eigenvalues of a matrix are also called its spectrum.\n\n### Singular value decomposition\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ with $m>n$, then $A$ admits a decomposition\n\n$$\nA = U\\Sigma V^{T}.\n$$\nThe matrices $U\\in\\mathbb{R}^{m\\times m}$ and $V\\in\\mathbb{R}^{n\\times n}$ are orthogonal. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors.\n\nThe matrix $\\Sigma\\in\\mathbb{R}^{m\\times n}$ is a diagonal matrix of the form\n\n$$\n\\Sigma = \n\\begin{bmatrix}\n\\sigma_{11} & 0 & \\cdots & 0 \\\\\n0 & \\sigma_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_{mn} \\\\\n0 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 0 \\\\\n\\end{bmatrix}.\n$$\nThe values $\\sigma_{ij}$ are the singular values of the matrix $A$. Amazingly, it can be proven that every matrix $A\\in\\mathbb{R}^{m\\times n}$ has a singular value decomposition.\n\n## Linear systems of equations\n\nA system of $m$ linear equations in $n$ unknowns can be written as\n\n$$\n\\begin{align*}\na_{11} x_{1} + a_{12} x_{2} + \\cdots + a_{1n} x_{n} &= b_1 \\\\\na_{21} x_{1} + a_{22} x_{2} + \\cdots + a_{2n} x_{n} &= b_2 \\\\\n\\vdots  \\qquad \\qquad \\quad \\\\\na_{m1} x_{1} + a_{m2} x_{2} + \\cdots + a_{mn} x_{n} &= b_m,\\\\\n\\end{align*}\n$$\nis simply the matrix vector equation\n\n$$\nA\\mathbf{x}=\\mathbf{b}.\n$$\n\nA linear system of equations may have:\n\n- infinitely many solutions,\n- a unique solution,\n- no solutions.\n\nWhen $m > n$, the system is said to be overdetermined and in general has infinitely many solutions. When $m<n$ the system is underdetermined and in general has no solution. For the case when $m=n$ and the matrix has $n$ linearly dependent columns, the solution is always unique.\n\nFor an invertible square matrix $A\\mathbf{x}=\\mathbf{b}$, the solution is always $\\mathbf{x}=A^{-1}\\mathbf{b}$. \n\nMatrix factorizations to help us solve linear systems of equations \nFor square matrices, we can use \n\nWe can use matrix factorizations to help us solve a linear system of equation. We demonstrate how to do this with the LU decomposition. Observe that $A\\mathbf{x} = LU\\mathbf{x} = \\mathbf{b}$. Then \n\n$$\n\\mathbf{x} = U^{-1}L^{-1}\\mathbf{b}.\n$$ \n\nThe process of inverting $L$ and $U$ is called backward and forward substitution.\n\n\n## Least squares\n\nIn data science it is often the case that we have to solve the linear system \n\n$$ A \\mathbf{x} = \\mathbf{b},$$ \n\nThis problem may have no solution -- perhaps due to noise or measurement error.\n\nIn such a case, we look for a vector $\\mathbf{x}$ such that $A\\mathbf{x}$ is a good approximation to $\\mathbf{b}.$\n\nThe quality of the approximation can be measured using the distance from $A\\mathbf{x}$ to $\\mathbf{b},$ i.e.,\n\n$$\\Vert A\\mathbf{x} - \\mathbf{b}\\Vert_2.$$\n\nThe general least-squares problem is given $A\\in\\mathbb{R}^{m\\times n}$ and and $\\mathbf{b}\\in\\mathbb{R}^{m}$, find a vector $\\hat{\\mathbf{x}}\\in\\mathbb{R}^{n}$ such that $\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert_2$ is minimized, i.e. \n\n$$\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x} \\Vert A\\mathbf{x} - \\mathbf{b}\\Vert.$$\n\nThis emphasizes the fact that the least squares problem is a minimization problem. Minimizations problems are an example of a broad class of problems called _optimization_ problems. In optimization problems we attempt to find an optimal solution that minimizes (or maximizes) a set particular set of equations (and possibly constraints). \n\nWe can connect the above minimization of the distance between vectors to the minimization of the sum of squared errors. Let $\\mathbf{y} = A\\mathbf{x}$ and observe that\n\n$$\\Vert A\\mathbf{x}-\\mathbf{b}\\Vert_2^2 = \\Vert \\mathbf{y}-\\mathbf{b}\\Vert_2^2 =  \\sum_i (y_i-b_i)^2.$$\n\nThe above expression is the sum of squared errors. In statistics, the $y_i$ are the estimated values and the $b_i$ are the measured values. This is the most common measure of error used in statistics and is a key principle. \n\nMinimizing the length of $A\\mathbf{x} - \\mathbf{b}$ is equivalent to minimizing the sum of the squared errors. \n\nWe can find $\\hat{\\mathbf{x}}$ using either \n\n* geometric arguments based on projections of the vector $\\mathbf{b}$,\n* by calculus (taking the derivative of the right-hand-side expression above and setting it equal to zero).\n\nEither way, we obtain the result that $\\hat{\\mathbf{x}}$ is the solution of:\n    \n$$A^TA\\mathbf{x} = A^T\\mathbf{b}.$$\n\nThis system of equations is called the normal equations.\n\nWe can prove that these equations always have at least one solution. \n\nWhen $A^TA$ is invertible, the system is said to be overdetermined.  This means that there is a unique solution\n\n$$\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T\\mathbf{b}.$$\n\nBe aware that computing the solution using $(A^TA)^{-1}A^T$ can be numerically unstable. A more stable method is to use the QR decomposition of $A$, i.e., $\\hat{\\mathbf{x}} = R^{-1}Q^T\\mathbf{b}$.\n\n",
    "supporting": [
      "04-Linear-Algebra-Refresher_files"
    ],
    "filters": [],
    "includes": {}
  }
}