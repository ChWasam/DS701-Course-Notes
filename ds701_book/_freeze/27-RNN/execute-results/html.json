{
  "hash": "f92363c852007144b0587889ede7e75e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Recurrent Neural Networks'\njupyter: python3\nbibliography: references.bib\n---\n\n\n\n\n# RNN Theory\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/27-RNN.ipynb)\n\nWe introduce recurrent neural networks (RNNs) which is a neural network architecture used for machine learning on sequential data.\n\n## What are RNNs?\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- A type of artificial neural network designed for processing sequences of data.\n- Unlike traditional neural networks, RNNs have connections that form directed cycles, allowing information to persist.\n:::\n::: {.column width=\"50%\"}\n![](drawio/RNN.png){.lightbox}\n:::\n::::\n\nThe above figure shows an RNN architecture. The block $A$ can be viewed as one stage of the RNN. $A$ accepts as input $x_t$ and outputs a value $\\hat{y}_t$. The loop with the hidden state $h_{t-1}$ illustrates how information passes from one step of the network to the next.\n\n## Why Use RNNs?\n\n- **Sequential Data**: Ideal for tasks where data points are dependent on previous ones.\n- **Memory**: Capable of retaining information from previous inputs, making them powerful for tasks involving context. This is achieved from hidden states and feedback loops.\n\n## RNN Applications\n\n- **Natural Language Processing (NLP)**: Language translation, sentiment analysis, and text generation.\n- **Speech Recognition**: Converting spoken language into text.\n- **Time Series Prediction**: Forecasting stock prices, weather, and other temporal data.\n- **Music Generation**: Creating a sequence of musical notes.\n\n## Outline\n\n1. Basic RNN Architecture\n1. LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units)\n1. Examples\n\n## RNN Architecture\n\n![](drawio/RNN-Full.png){.lightbox}\n\n## Forward Propagation\n\nThe forward pass in the RNN architecture is given by the following operations\n\n1. $h_t = g_h(W_{hh} h_{t-1} + W_{hx} x_t + b_h)$\n1. $\\hat{y}_t = g_y(W_{yh} h_t + b_y)$\n\nThe vector $x_t$ is the $t$-th element of the input sequence. The vector $h_t$ is the hidden state at the $t$-th point of the sequence. The dimension of $h_t$ is a hyperparameter of the RNN. \n\nThe vector $\\hat{y}_t$ is the $t$-th output of the sequence. The functions $g_h$ and $g_y$ are nonlinear activation functions. \n\nThe weight matrices $W_{hh}$, $W_{hx}$, $W_{yh}$, and biases $b_h$, $b_y$ are trainable parameters **that are reused in each time step**. Note that we must define the vector $h_0$. A common choice is $h_0 = 0$.\n\n## RNN Cell \n\n![](drawio/LSTM-internal.png){.lightbox fig-align=\"center\"}\n\n## Model Types\n\nDepending on the application there many be varying number of outputs $\\hat{y}_t$. The figure below illustrates how the architecture can change.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n![](figs/RNNConfigurations.png){.lightbox}\n:::\n::: {.column width=\"40%\"}\n1. One-to-one, is a regular feed forward neural network.\n1. One-to-many, e.g., image captioning (1 image and output a sequence of words).\n1. Many-to-one, e.g., sentiment analysis from a sequence of words or stock price prediction.\n1. Many-to-many, e.g., machine translation or video frame-by-frame action classification.\n:::\n::::\n\nOur subsequent illustrations of RNNs all display many-to-many architectures.\n\n## Stacked RNNs\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](drawio/StackedRNN.png){.lightbox}\n\n:::\n::: {.column width=\"50%\"}\n\nIt is also possible to stack multiple RNNs on top of each other. This is illustrated in the figure below.\n\nEach layer has its own set of weights and biases.\n:::\n::::\n\n## Loss Calculation for Sequential Data\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](drawio/RNN-Loss2.png){.lightbox}\n \n:::\n::: {.column width=\"50%\"}\n\nFor each prediction in the sequence $\\hat{y}_t$ and corresponding true value $y_t$\nwe calculate the loss $\\mathcal{L}_t(y_t, \\hat{y}_t)$. \n\nThe total loss is the sum of all the individual losses, \n$\\mathcal{L} = \\sum_t \\mathcal{L}_t$. This is illustrated in the figure above.\n\nFor categorical data we use cross-entropy loss \n$\\mathcal{L}_t(y_t, \\hat{y}_t) = -y_t\\log(\\hat{y}_t)$. \n\nFor continuous data we would use a mean-square error loss.\n\n:::\n::::\n\n## Vanishing Gradients\n\nThe key idea of RNNs is that earlier items in the sequence  influence the more recent outputs. This is highlighted in blue in the figure below.\n\nHowever, for longer sequences the influence can be significantly reduced. This is illustrated in red in the figure below.\n\nDue to the potential for long sequences of data, during training you are likely to encounter small gradients. As a result, RNNs are prone to vanishing gradients.\n\n![](drawio/VanishingGradient.png){.lightbox}\n\n## RNN Limitations\n\n- **Vanishing Gradients**:\n  - During training, gradients can become very small (vanish), making it difficult for the network to learn long-term dependencies.\n\n- **Long-Term Dependencies**:\n  - RNNs struggle to capture dependencies that are far apart in the sequence, leading to poor performance on tasks requiring long-term memory.\n\n- **Difficulty in Parallelization**:\n  - The sequential processing of data in RNNs makes it challenging to parallelize training, leading to slower training times.\n\n- **Limited Context**:\n  - Standard RNNs have a limited ability to remember context over long sequences, which can affect their performance on complex tasks.\n\n- **Bottleneck Problem**:\n  - The hidden state is all that carries forward history and it can be a bottleneck in how expressive it can be.    \n\n## Addressing RNN Limitations\n\nSome proposed solutions for mitigating the previous issues are\n\n- **LSTM (Long Short-Term Memory)**\n- **GRU (Gated Recurrent Units)** \n\nThese are more advanced variants of RNNs designed to address some of these limitations by improving the ability to capture long-term dependencies and addressing gradient issues.\n\n## LSTM\n\nBelow is an illustration of the LSTM architecture.\n\n![](drawio/LSTM.png){.lightbox}\n\n---\n\nThe new components are:\n\n- The input cell state $c_{t-1}$ and output cell state $c_{t}$.\n- Yellow blocks consisting of neural network layers with sigmoid $\\sigma$ or tanh activation functions.\n- Red circles indicating point-wise operations.\n\n\n## Cell State\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nThe cell state $c_{t-1}$ is the input to the LSTM block.\n\nThis value then moves through the LSTM block.\n\nIt is modified by either a multiplication or addition interaction.\n\nAfter these operations, the modified cell state is $c_{t}$ is sent to the next LSTM block. In addition, $c_t$ is added to the hidden state after tanh activation.\n\n:::\n::: {.column width=\"50%\"}\n![](drawio/CellState.png){.lightbox}\n:::\n::::\n\n## Forget Gate Layer\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nThe forget layer computes a value \n\n$$\nf_t = \\sigma(W_f[h_{t-1}, x_t] + b_f),\n$$\n\nwhere $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function.\n\nThe output $f_t$ is a vector of numbers between 0 and 1. These values multiply the corresponding vector values in $c_{t-1}$.\n\nA value of 0 says throw that component of $c_{t-1}$ away. A value of 1 says keep that component of $c_{t-1}$.\n\nThis operation tells us which *old* information in the cell state we should keep or remove.\n\n:::\n::: {.column width=\"50%\"}\n![](drawio/ForgetLayer.png){.lightbox}\n:::\n::::\n\n## Input Gate Layer\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nThe input gate layer computes values\n\n$$\n\\begin{align}\ni_t &= \\sigma(W_i[h_{t-1}, x_t] + b_i), \\\\\n\\tilde{c}_{t} &= \\operatorname{tanh}(W_c[h_t-1, x_t] + b_c).\n\\end{align}\n$$\n\nThe value that is added to the cell state $c_{t-1}$ is $i_t \\cdot \\tilde{c}_t$.\n\nThis value tells us what *new* information to add to the cell state.\n\nAt this stage of the process, the cell state now has the formula\n\n$$\nc_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t.\n$$\n\n:::\n::: {.column width=\"50%\"}\n![](drawio/InputGateLayer.png){.lightbox}\n:::\n::::\n\n## Output Layer\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nThe output gate layer computes values\n\n$$\n\\begin{align}\no_t &= \\sigma(W_o[h_{t-1}, x_t] + b_o), \\\\\nh_{t} &=o_t * \\operatorname{tanh}(c_t).\n\\end{align}\n$$\n\nThe vector $o_t$ from the sigmoid layer tells us what parts of the cell state we use for output. The output is a filtered version of the hyperbolic tangent of cell state $c_t$. \n\nThe output of the block is $\\hat{y}_t$. It is the same as the hidden state $h_t$ that is sent to the $t+1$ LSTM block.\n\n:::\n::: {.column width=\"50%\"}\n![](drawio/OutputLayer.png){.lightbox}\n:::\n::::\n\n## Weight Summary\n\nFor the LSTM architecture, we have the following sets of weights\n\n- $W_f$ and $b_f$ (forget layer),\n- $W_i, W_c$ and $b_i, b_c$ (input gate layer),\n- $W_o$ and $b_0$ (output layer).\n\n\n## Advantages of LSTMs\n\n- **Long-term Dependencies**: LSTMs can capture long-term dependencies in sequential data, making them effective for tasks like language modeling and time series prediction.\n- **Avoiding Vanishing Gradient**: The architecture of LSTMs helps mitigate the vanishing gradient problem, which is common in traditional RNNs.\n- **Flexibility**: LSTMs can handle variable-length sequences and are versatile for different types of sequential data.\n- **Memory**: They have a memory cell that can maintain information over long periods, which is crucial for tasks requiring context retention.\n\n## Disadvantages of LSTMs\n\n- **Complexity**: LSTMs are more complex than simpler models like traditional RNNs, leading to longer training times and higher computational costs.\n- **Overfitting**: Due to their complexity, LSTMs are prone to overfitting, especially with small datasets.\n- **Resource Intensive**: They require more computational resources and memory, which can be a limitation for large-scale applications.\n- **Hyperparameter Tuning**: LSTMs have many hyperparameters that need careful tuning, which can be time-consuming and challenging.\n- **Bottleneck Problem**: The hidden and cell states are all that carries forward history and they can be a bottleneck in how expressive it can be.    \n\n\n## GRU\n\nA variant of the LSTM architecture is the Gated Recurrence Unit.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nIt combines the forget and input gates into a single update gate. It also combines the cell-state and hidden state. The operations that are now performed are given below:\n\n$$\n\\begin{align}\nz_t &= \\sigma(W_z[h_{t-1}, x_t] + b_z),\\\\\nr_t &= \\sigma(W_r[h_{t-1}, x_t] + b_r), \\\\\n\\tilde{h}_t &= \\sigma(W_{\\tilde{h}}[h_{t-1}, x_t] + b_{\\tilde{h}}), \\\\\nh_t &= (1- z_t)\\cdot h_{t-1} + z_t\\cdot \\tilde{h}_t.\n\\end{align}\n$$\n:::\n::: {.column width=\"50%\"}\n![](drawio/GRU.png){.lightbox}\n:::\n::::\n\n## Advantages of GRUs\n\n- **Simpler Architecture**: GRUs have a simpler architecture compared to LSTMs, with fewer gates, making them easier to implement and train.\n- **Faster Training**: Due to their simpler structure, GRUs often train faster and require less computational power than LSTMs.\n- **Effective Performance**: GRUs can perform comparably to LSTMs on many tasks, especially when dealing with shorter sequences.\n- **Less Prone to Overfitting**: With fewer parameters, GRUs are generally less prone to overfitting compared to LSTMs.\n\n## Disadvantages of GRUs\n\n- **Less Expressive Power**: The simpler architecture of GRUs might not capture complex patterns in data as effectively as LSTMs.\n- **Limited Long-term Dependencies**: GRUs might struggle with very long-term dependencies in sequential data compared to LSTMs.\n- **Less Flexibility**: GRUs offer less flexibility in terms of controlling the flow of information through the network.\n- **Bottleneck Problem**: Like vanilaa RNNs, the hidden state is all that carries forward history and can be a bottleneck in how expressive it can be. \n\n# RNN Examples\n\n## Appliance Energy Prediction\n\nIn this example we will use a [dataset](https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction) containing the energy usage in Watt hours (Wh) of appliances in a low energy building.\n\nAnother example of Time Series Analysis.\n\nIn addition to the appliance energy information, the dataset includes the house temperature and humidity conditions. We will build an LSTM model that predicts the energy usage of the appliances.\n\n## Load the Data\n\nThe code cells below load the dataset.\n\n::: {#149ff959 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport os\n\nfile_path = 'energydata_complete.csv'\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n\nif os.path.exists(file_path):\n    data = pd.read_csv(file_path)\nelse:\n    data = pd.read_csv(url)\n    data.to_csv(file_path, index=False)\n\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>Appliances</th>\n      <th>lights</th>\n      <th>T1</th>\n      <th>RH_1</th>\n      <th>T2</th>\n      <th>RH_2</th>\n      <th>T3</th>\n      <th>RH_3</th>\n      <th>T4</th>\n      <th>...</th>\n      <th>T9</th>\n      <th>RH_9</th>\n      <th>T_out</th>\n      <th>Press_mm_hg</th>\n      <th>RH_out</th>\n      <th>Windspeed</th>\n      <th>Visibility</th>\n      <th>Tdewpoint</th>\n      <th>rv1</th>\n      <th>rv2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01-11 17:00:00</td>\n      <td>60</td>\n      <td>30</td>\n      <td>19.89</td>\n      <td>47.596667</td>\n      <td>19.2</td>\n      <td>44.790000</td>\n      <td>19.79</td>\n      <td>44.730000</td>\n      <td>19.000000</td>\n      <td>...</td>\n      <td>17.033333</td>\n      <td>45.53</td>\n      <td>6.600000</td>\n      <td>733.5</td>\n      <td>92.0</td>\n      <td>7.000000</td>\n      <td>63.000000</td>\n      <td>5.3</td>\n      <td>13.275433</td>\n      <td>13.275433</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-01-11 17:10:00</td>\n      <td>60</td>\n      <td>30</td>\n      <td>19.89</td>\n      <td>46.693333</td>\n      <td>19.2</td>\n      <td>44.722500</td>\n      <td>19.79</td>\n      <td>44.790000</td>\n      <td>19.000000</td>\n      <td>...</td>\n      <td>17.066667</td>\n      <td>45.56</td>\n      <td>6.483333</td>\n      <td>733.6</td>\n      <td>92.0</td>\n      <td>6.666667</td>\n      <td>59.166667</td>\n      <td>5.2</td>\n      <td>18.606195</td>\n      <td>18.606195</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-11 17:20:00</td>\n      <td>50</td>\n      <td>30</td>\n      <td>19.89</td>\n      <td>46.300000</td>\n      <td>19.2</td>\n      <td>44.626667</td>\n      <td>19.79</td>\n      <td>44.933333</td>\n      <td>18.926667</td>\n      <td>...</td>\n      <td>17.000000</td>\n      <td>45.50</td>\n      <td>6.366667</td>\n      <td>733.7</td>\n      <td>92.0</td>\n      <td>6.333333</td>\n      <td>55.333333</td>\n      <td>5.1</td>\n      <td>28.642668</td>\n      <td>28.642668</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-01-11 17:30:00</td>\n      <td>50</td>\n      <td>40</td>\n      <td>19.89</td>\n      <td>46.066667</td>\n      <td>19.2</td>\n      <td>44.590000</td>\n      <td>19.79</td>\n      <td>45.000000</td>\n      <td>18.890000</td>\n      <td>...</td>\n      <td>17.000000</td>\n      <td>45.40</td>\n      <td>6.250000</td>\n      <td>733.8</td>\n      <td>92.0</td>\n      <td>6.000000</td>\n      <td>51.500000</td>\n      <td>5.0</td>\n      <td>45.410389</td>\n      <td>45.410389</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-01-11 17:40:00</td>\n      <td>60</td>\n      <td>40</td>\n      <td>19.89</td>\n      <td>46.333333</td>\n      <td>19.2</td>\n      <td>44.530000</td>\n      <td>19.79</td>\n      <td>45.000000</td>\n      <td>18.890000</td>\n      <td>...</td>\n      <td>17.000000</td>\n      <td>45.40</td>\n      <td>6.133333</td>\n      <td>733.9</td>\n      <td>92.0</td>\n      <td>5.666667</td>\n      <td>47.666667</td>\n      <td>4.9</td>\n      <td>10.084097</td>\n      <td>10.084097</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Resample Data\n\nWe're interested in the `Appliances` column, which is the energy use of the appliances in Wh. \n\nFirst, we'll resample the data to hourly resolution and fill missing values using the forward fill method.\n\n::: {#800fdc09 .cell execution_count=3}\n``` {.python .cell-code}\ndata['date'] = pd.to_datetime(data['date'])\ndata.set_index('date', inplace=True)\n\ndata = data['Appliances'].resample('h').mean().ffill() # Resample and fill missing\n\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\ndate\n2016-01-11 17:00:00     55.000000\n2016-01-11 18:00:00    176.666667\n2016-01-11 19:00:00    173.333333\n2016-01-11 20:00:00    125.000000\n2016-01-11 21:00:00    103.333333\nFreq: h, Name: Appliances, dtype: float64\n```\n:::\n:::\n\n\n## Preparing the Data\n\nWe create train-test splits and scale the data accordingly. \n\nIn addition, we create our own dataset class. In this class we create lagged sequences of the energy usage data. The amount of lag, or look-back sequence length, is set to 24.\n\n::: {#ab13f172 .cell execution_count=4}\n``` {.python .cell-code}\ntrain_size = int(len(data) * 0.8)\ntest_size = len(data) - train_size\n\ntrain_data = data[:train_size]\ntest_data = data[train_size:]\n\n# Normalize data\nscaler = MinMaxScaler()\ntrain_data_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\ntest_data_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n\n# Prepare data for LSTM\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, seq_length):\n        self.data = data\n        self.seq_length = seq_length\n\n    def __len__(self):\n        return len(self.data) - self.seq_length\n\n    def __getitem__(self, index):\n        X = self.data[index:index + self.seq_length]\n        y = self.data[index + self.seq_length]\n        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n\nseq_length = 24\ntrain_dataset = TimeSeriesDataset(train_data_scaled, seq_length)\ntest_dataset = TimeSeriesDataset(test_data_scaled, seq_length)\n\n# Create DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n```\n:::\n\n\n## LSTM Architecture\n\nWe define our LSTM architecture in the code cell below.\n\n::: {#966726c0 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\n# Define the LSTM model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=16, output_size=1):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        x = self.fc(x[:, -1, :])  # Use the output of the last time step\n        return x\n\nmodel = LSTMModel()\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLSTMModel(\n  (lstm): LSTM(1, 16, batch_first=True)\n  (fc): Linear(in_features=16, out_features=1, bias=True)\n)\n```\n:::\n:::\n\n\n<!-- \n## LSTM Architecture Visualized\n\n```{.python}\nfrom torchview import draw_graph\n# Create a dummy input tensor with the shape (batch_size, sequence_length, input_size)\ndummy_input = torch.randn(1, 10, 1)\n\n# Perform a forward pass to create the computation graph\noutput = model(dummy_input)\n\n# Generate and save the visualization of the model\n# Generate the visualization of the model\nmodel_graph = draw_graph(model, input_size=(1, 10, 1), depth=2, hide_inner_tensors=True)\n\n# Display the graph\nmodel_graph.visual_graph\n\nImage(filename='LSTMModel.png')\n``` \n-->\n\n## Loss and Training\n\nWe use a mean-square error loss function and the Adams optimizer. We train the model for 20 epochs. We display the decrease in the loss during training. Based on the plot, did the optimizer converge?\n\n::: {#9d79a97f .cell execution_count=6}\n``` {.python .cell-code}\n# Set criterion and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 20\ntrain_losses = []\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    for X, y in train_loader:\n\n      optimizer.zero_grad()\n      outputs = model(X)\n      loss = criterion(outputs, y)\n      loss.backward()\n      optimizer.step()\n\n      train_loss += loss.item()\n    train_losses.append(train_loss)\n\n# Plot the training losses over epochs\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, epochs + 1), train_losses, marker='o', linestyle='-', color='b')\nplt.title('Training Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(range(1, epochs + 1))  # Set x-tick marks to be integers\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](27-RNN_files/figure-html/cell-7-output-1.png){width=821 height=449}\n:::\n:::\n\n\n## Evaluation\n\nWe'll evaluate the model's performance by plotting the predicted values from the test set on top of the actual test set values. \n\nHow did the model perform? Are there any obvious problems?\n\n::: {#39969a67 .cell execution_count=7}\n``` {.python .cell-code}\n# Evaluate the model\nmodel.eval()\npredictions = []\ntrues = []\nwith torch.no_grad():\n    for X, y in test_loader:\n        preds = model(X)\n        predictions.extend(preds.numpy())\n        trues.extend(y.numpy())\n\n# Rescale predictions and true to original scale\npredictions_rescaled = scaler.inverse_transform(predictions)\ntrue_rescaled = scaler.inverse_transform(trues)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(true_rescaled, label='True Values')\nplt.plot(predictions_rescaled, label='Predicted Values', alpha=0.7)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](27-RNN_files/figure-html/cell-8-output-1.png){width=798 height=485}\n:::\n:::\n\n\n<!-- \n## Music Generation\n\nThe goal is to create an LSTM architecture to create a piece of music. We will train the LSTM on some example snippets of music. The trained model will then be able to generate a new piece of music.\n\nWe will first need to write some helper functions to get this to work.\n\n## MIDI Data\n\nWe will use the package [pretty_midi](https://github.com/craffel/pretty-midi). This package will allow us to convert MIDI (Musical Instrument Digital Interface) files into NumPy arrays. MIDI files are a common format for music data. They contain information about the notes, timing, and instruments used in a piece of music. We can then use the NumPy arrays to train our LSTM.\n\nBelow is the code that will convert our MIDI file to a NumPy array. The output is an array of the start time of the note, the end time of the note, the note pitch, and the note velocity.\n\n```{.python}\n#| code-fold: false\nimport pretty_midi\nimport numpy as np\n\ndef midi_to_notes(midi_file):\n    midi_data = pretty_midi.PrettyMIDI(midi_file)\n    notes = []\n    for instrument in midi_data.instruments:\n        if not instrument.is_drum:\n            for note in instrument.notes:\n                notes.append([note.start, note.end, note.pitch, note.velocity])\n    return np.array(notes)\n```\n\n## Maestro dataset\n\nThe [Maestro](https://magenta.tensorflow.org/datasets/maestro#v300) dataset is a dataset composed of about 200 hours of virtuosic piano performances. I took a few small samples from some of these files to train our LSTM and generate some new music.\n\n```{.python}\n#| code-fold: false\nmidi_files = [\n    os.path.join('music', 'music_sample_1.mid'),\n    os.path.join('music', 'music_sample_2.mid'),\n    os.path.join('music', 'music_sample_3.mid')]\nall_notes = []\nfor midi_file in midi_files:\n  notes = midi_to_notes(midi_file)\n  all_notes.append(notes)\nall_notes = np.concatenate(all_notes)\n```\n\n```{.python}\nimport subprocess\n\ndef midi_to_mp3(midi_file, mp3_file):\n    # Use the 'timidity' tool to convert MIDI to WAV\n    wav_file = midi_file.replace('.mid', '.wav')\n    subprocess.run(['timidity', midi_file, '-Ow', '-o', wav_file])\n    \n    # Use the 'ffmpeg' tool to convert WAV to MP3\n    subprocess.run(['ffmpeg', '-i', wav_file, mp3_file])\n\ndef midi_to_wav(midi_file):\n    # Use the 'timidity' tool to convert MIDI to WAV\n    wav_file = midi_file.replace('.mid', '.wav')\n    print(wav_file)\n    print(f\"Converting {midi_file} to {wav_file}\")\n    subprocess.run(['timidity', midi_file, '-Ow', '-o', wav_file])\n    \n\n# Convert the generated MIDI file to MP3\n#midi_to_mp3('music/music_sample_1.mid', 'music/music_sample_1.mp3')\n#midi_to_mp3('music/music_sample_2.mid', 'music/music_sample_2.mp3')\n#midi_to_mp3('music/music_sample_3.mid', 'music/music_sample_3.mp3')\nprint(os.path.exists(os.path.join('music', 'music_sample_1.mid')))\nif not os.path.exists(os.path.join('music', 'music_sample_1.wav')):\n    midi_to_wav(os.path.join('music', 'music_sample_1.mid'))\nif not os.path.exists(os.path.join('music', 'music_sample_2.wav')):\n    midi_to_wav(os.path.join('music', 'music_sample_2.mid'))\nif not os.path.exists(os.path.join('music', 'music_sample_3.wav')):\n    midi_to_wav(os.path.join('music', 'music_sample_3.mid'))\n```\n\n<audio src=\"music/music_sample_1.wav\" controls></audio>\n<audio src=\"music/music_sample_2.wav\" controls></audio>\n<audio src=\"music/music_sample_3.wav\" controls></audio>\n\n\n## Sequences of Notes\n\nWe will convert the note sequences into a format suitable for training the LSTM model. An example of the split is shown below.\n\n![](drawio/TrainTestPrep.png)\n\n\nThis means we create sequences of notes and corresponding targets.\n\n\n```{.python}\n#| code-fold: false\nsequence_length = 48  # Length of each input sequence\nX = []\ny = []\n\nfor i in range(len(all_notes) - sequence_length):\n    X.append(all_notes[i:i+sequence_length])\n    y.append(all_notes[i+sequence_length])\n\nX = np.array(X)\ny = np.array(y)\nprint(\"Shape of X\", X.shape)\nprint(\"Shape of y\", y.shape)\n```\n\n## LSTM Architecture\n\nWe now create the LSTM architecture.\n\n\n```{.python}\n#| code-fold: false\nimport torch\nimport torch.nn as nn\n\nclass MusicLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(MusicLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n        \n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Example usage\ninput_dim = X.shape[2]  # Number of features (e.g., pitch, velocity)\nhidden_dim = 20\nnum_layers = 2\noutput_dim = X.shape[2]\nmodel = MusicLSTM(input_dim, hidden_dim, num_layers, output_dim)\n```\n\n\n## Training\n\nWe will train our model for 100 epochs.\n\n```{.python}\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 100\nbatch_size = 8\ntrain_losses = []\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for i in range(0, len(X), batch_size):\n        inputs = torch.tensor(X[i:i+batch_size], dtype=torch.float32)\n        targets = torch.tensor(y[i:i+batch_size], dtype=torch.float32)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_losses.append(train_loss)\n# Plot the training losses over epochs\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', color='b')\nplt.title('Training Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n```\n\n## Generate\n\nWe next write a function that will help us generate a new piece of music.\n\n```{.python}\n#| code-fold: false\ndef generate_music(model, start_sequence, length):\n    model.eval()\n    generated = start_sequence\n    for _ in range(length):\n        input_seq = torch.tensor(generated[-sequence_length:], dtype=torch.float32).unsqueeze(0)\n        next_note = model(input_seq).detach().numpy()\n        generated = np.vstack((generated, next_note))\n    return generated\n\n# Example usage\nstart_sequence = X[10]  # Starting sequence for generation\ngenerated_music = generate_music(model, start_sequence, 100)  # Generate 100 notes\nprint(generated_music.shape)\n```\n\n## Download the Song\n\nWe can generate an output midi file to play. The generated song is not very good. We could probably create a better song through an improved model, more training, and a larger training set.\n\n```{.python}\ndef notes_to_midi(notes, output_file):\n    midi = pretty_midi.PrettyMIDI()\n    instrument = pretty_midi.Instrument(program=0)\n    for note in notes:\n        start, end, pitch, velocity = note\n        midi_note = pretty_midi.Note(\n            velocity=int(velocity),\n            pitch=int(pitch),\n            start=float(start),\n            end=float(end)\n        )\n        instrument.notes.append(midi_note)\n    midi.instruments.append(instrument)\n    midi.write(output_file)\n\n# Uncomment this line to write the following generated music to a MIDI file\nnotes_to_midi(generated_music, os.path.join('music', 'generated_music.midi'))\n```\n\n## Play the Generated Song\n\nNow let's play the generated song.\n\n```{.python}\nimport subprocess\n\ndef midi_to_wav(midi_file):\n    # Use the 'timidity' tool to convert MIDI to WAV\n    wav_file = midi_file.replace('.midi', '.wav')\n    print(f\"Converting {midi_file} to {wav_file}\")\n    subprocess.run(['timidity', midi_file, '-Ow', '-o', wav_file])\n \nif not os.path.exists(os.path.join('music', 'generated_music.wav')):\n    midi_to_wav(os.path.join('music', 'generated_music.midi'))\n```\n\n<audio src=\"music/generated_music.wav\" controls></audio> \n-->\n\n## Other Applications\n\nAndrej Karpathy has an excellent blog post titled [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). We will describe some of the applications below.\n\n## Shakespeare\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nUsing the a 3-layer RNN with 512 hidden nodes on each layer, Karpathy trained a model on the complete works of Shakespeare. With this model he was able to generate texts similar to what is seen on the right.\n\nObserve that there are some typos and the syntax is not perfect, but overall the style appears Shakespearean.\n\n:::\n::: {.column width=\"5-%\"}\n![](figs/ShakespeareText.png){.lightbox}\n:::\n::::\n\n## Wikipedia\n\n:::: {.columns}\n::: {.column width=\"30%\"}\nUsing a 100MB [dataset](http://prize.hutter1.net) of raw Wikipedia data, Karpathy trained RNNs which generated Wikipedia content. An example of the generated markdown is shown to the right.\n\nThe text is nonsense, but is structured like a Wikipedia post.\nInterestingly, the model *hallucinated* and fabricated a url that does not exist.\n:::\n::: {.column width=\"70%\"}\n![](figs/WikipediaContent.png){.lightbox}\n:::\n::::\n\n\n## Baby Names\n\n:::: {.columns}\n::: {.column width=\"30%\"}\nUsing a list of 8000 baby names from this [link](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/), Karpathy trained an RNN to predict baby names.\n\nTo the right is a list of generated names not on the lists. More of the names can be found [here](https://cs.stanford.edu/people/karpathy/namesGenUnique.txt).\n\nSome interesting additional names the model generated were\n`Baby, Char, Mars, Hi, and With`.\n:::\n::: {.column width=\"70%\"}\n![](figs/BabyNames.png){.lightbox}\n:::\n::::\n\n\n## Recap\n\nWe discussed the basic RNN architecture. We then discussed the LSTM and GRU modifications. These modifications allowed the RNNs to handle long-term dependencies.\n\nWe then considered an example application of energy consumption prediction.\n\nWe also discussed the unreasonable effectiveness of RNNs.\n\n## What's Next?\n\nAs we saw, the RNN architecture has evolved to include LSTMs and GRUs. However, the RNN architecture also evolved to add attention. This was introduced by @bahdanau2014neural. \n\nAttention in language models is a mechanism that allows the model to focus on relevant parts of the input sequence by assigning different weights to different words, enabling it to capture long-range dependencies and context more effectively.\n\nIn addition, recall that RNNs are only able to process data sequentially. For large scale natural language processing applications, this is a major computational bottleneck. This motivated the development of more advanced neural network architectures that could process sequential data in parallel and utilize attention. \n\nThe transformer architecture was introduced by @vaswani2017attention and combines both of these desirable features.\n\nTransformers form the basis for modern large language models (LLMs) and we will discuss them in our NLP lectures.\n\n## References\n\n1. Understanding LSTMs, Colah’s blog, 2015, [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n1. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Draft of January 5, 2024. – Chapter 9, RNNs and LSTMs, [https://web.stanford.edu/~jurafsky/slpdraft/9.pdf](https://web.stanford.edu/~jurafsky/slpdraft/9.pdf) \n1. The Unreasonable Effectiveness of LSTMs, Andrej Karpathy, 2015, [https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n\n",
    "supporting": [
      "27-RNN_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}