{
  "hash": "b525c9b9add845933818619082197433",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Probability and Statistics Refresher\njupyter: python3\n---\n\n\n\n::: {.content-visible when-profile=\"web\"}\nToday we'll review the essentials of probability and statistics that we'll need for this course.\n:::\n\n## Motivation\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/03-Probability-and-Statistics-Refresher.ipynb)\n\nWhy do we need knowledge of probability and statistics as a data scientist?\n\n:::: {.incremental}\n1. Data Analysis: They help in understanding and interpreting data. Statistical methods allow you to summarize data (mean and variance) and identify patterns.\n1. Model Building: Many machine learning algorithms are based on statistical principles. For example, linear regression and logistic regression rely on probability and statistics.\n1. Uncertainty Quantification: Probability helps in quantifying uncertainty. This is crucial in making predictions and decisions based on data, as it allows you to estimate the likelihood of different outcomes.\n1. Hypothesis Testing: Statistics provide tools for testing hypotheses and validating models. This is essential for determining whether the patterns observed in data are significant or just due to random chance.\n::::\n\n:::: {.fragment}\nCombined with linear algebra, probability and statistics provide the theoretical foundation and practical tools needed to extract meaningful insights from data and make data-driven decisions. \n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Lecture Overview\nWe will introduce the following concepts\n\n:::: {.incremental}\n- Frequentist and Bayesian views on probability,\n- Definitions and rules of probability,\n  - Events and sample spaces, Probability measures, Conditional probability, Bayes' theorem\n- Random variables and distributions \n  - probability mass function, probability density function, cumulative distribution functions\n- Mean, variance, and covariance,\n- Multivariate random variables,\n- Important distributions.\n::::\n:::\n\n## Probability\n\nWe all have a general sense of what is meant by probability. Probability is the study of randomness. It is the branch of mathematis that analyzes the chances (likelihood) of random events.\n\nThere are two different ways in which probability is interpreted. These views are\n\n:::: {.incremental}\n- frequentist, and\n- Bayesian.\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### The Frequentist View\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Frequentist View\n:::\n\nThe frequentist view of probability requires the definition of several concepts.\n\n- _Experiment_: a situation in which the outcomes occur randomly.\n  \n  __Example__: Driving to work, a commuter passes through a sequence of three intersections with traffic lights. At each light she either stops $s$ or continues $c$.\n\n- _Sample space_: the set of all possible outcomes of the experiment.\n\n  __Example__: $\\{ccc, ccs, csc, css, scc, ssc, scs, sss\\},$ where $csc$, for example, denotes the outcome that the commuter continues through the first light, stops at the second light, and continues through the third light.\n\n- _Event_: a subset of the sample space.\n\n  __Example__: continuing through the first light (i.e., $\\{ccc, ccs, csc, css\\}$). \n\n::: {.content-visible when-profile=\"slides\"}\n## The Frequentist View\n:::\n\nThe frequentist view is summarized by this quote from the first pages of Y. A. Rozanov. _Probability Theory: A Concise Course._ 1969.\n\n>Suppose an experiment under consideration can be repeated any number of times, so that, in principle at least, we can produce a whole series of “independent trials under identical conditions” in each of which, depending on chance, a particular event $A$ of interest either occurs or does not occur.\n> \n>Let $n$ be the total number of experiments in the whole series of trials, and let $n(A)$ be the number of experiement in which $A$ occurs. Then the ratio $n(A)/n$ is called the relative frequency of the event $A.$ \n>\n> It turns out that the relative frequencies observed in different series of trials are virtually the same for large $n,$ clustering about some constant $P(A),$ which is called the **probability of the event $A.$**\n\n::: {.content-visible when-profile=\"web\"}\nThis is called the **frequentist** intepretation of probability.\n\nThe key idea in the above definition is to be able to:\n\n> produce a whole series of “independent trials under identical conditions” \n\nWhich, when you think about it, is really a rather tricky notion.\n\nNevertheless, the frequentist view of probability is quite useful and we will often use it in this course.\n\nYou can think of it as treating each event as a sort of idealized coin-flip.\n\nIn other words, when we use the frequentist view, we will generally be thinking of a somewhat abstract situation where we __assume__ that \"independent trials under identical conditions\" is a good description of the situation.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Bayesian View\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Bayesian View\n:::\n\nTo understand the Bayesian view of probability, consider the following situations.\n\n:::: {.fragment}\nOn a visit to the doctor, we may ask, \"What is the probability that I have disease X?\"\n::::\n\n:::: {.fragment}\nOr, before digging a well, we may ask, \"What is the probability that I will strike water?\"\n::::\n\n:::: {.fragment}\nThese questions are __totally incompatible__ with the notion of \"independent trials under identical conditions\"!\n::::\n\n:::: {.fragment}\nEither I do, or do not, have disease X. Either I will, or will not, strike water.\n::::\n\n:::: {.fragment}\nWhat we are really asking is\n\n:::: {.incremental}\n- \"How certain should I be that I have disease X?\" \n- \"How certain should I be that I will strike water?\"\n::::\n::::\n\n:::: {.fragment}\nIn this setting, we are using probability to encode \"degree of belief\" or a \"state of knowledge.\"\n::::\n\n::: {.content-visible when-profile=\"web\"}\nThis is called the __Bayesian__ interpretation of probability.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Frequentist vs Bayes\n:::\n\nSomewhat amazingly, it turns out that whichever way we think of probability (frequentist or Bayesian),\n\n... the __rules__ that we use for computing probabilities are __exactly the same.__\n\nThis is a very deep and surprising thing.\n\nIn other words, it's often really a \"state of knowledge\" that we are really talking about when we use probability models in this course.\n\n> A thing appears random only through the incompleteness of our knowledge. \n>\nSpinoza, _Ethics,_ Part 1\n\nIn other words, we use probability as an abstraction that __hides details__ we don't want to deal with.  \n\nSo it's important to recognize that both frequentist and Bayesian views are __valid__ and __useful__ views of probability.\n\n::: {.content-visible when-profile=\"slides\"}\n## Frequentist vs Bayes\n:::\n\n![](figs/Bayes-Frequentism.png){fig-align=\"center\" width=\"30%\"} \n\n::: {.content-visible when-profile=\"web\"}\n>Any simple idea is approximate; as an illustration, consider an object ... what is an object? Philosophers are always \n>saying, “Well, just take a chair for example.” The moment they say that, you know that they do not know what they are \n>talking about any more. What is a chair? ... every object is a mixture of a lot of things, so we can deal with it \n> only as a series of approximations and idealizations.\n\n>The trick is the idealizations.\n\nRichard Feynman, _The Feynman Lectures on Physics, 12-2_\n\nHere is an illustration of this principle applied to probability:\n\n>In a serious work ... an expression such as “this phenomenon is due to chance” constitutes simply, \n>an elliptic form of speech. ... It really means “everything occurs as if this phenomenon were due to chance,” \n>or, to be more precise: “To describe, or interpret or formalize this phenomenon, \n>only probabilistic models have so far given good results.”\n\nGeorges Matheron, _Estimating and Choosing: An Essay on Probability in Practice_\n:::\n\nSo, now, let's talk about rules for computing probabilities.\n\n::: {.content-visible when-profile=\"web\"}\n### Sample Space and Events\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Sample Space and Events\n:::\n\n:::: {.fragment}\n__Definition.__ The sample space $\\Omega$ is the set of all possible outcomes of an experiment.\n::::\n\n:::: {.fragment}\n__Definition.__ An event $E\\subset\\Omega$ is an outcome, or a set of outcomes of an experiment.\n::::\n\n:::: {.fragment}\nExamples:\n\n:::: {.incremental}\n- Rolling a dice: $\\Omega = \\{1, 2, 3, 4, 5, 6 \\}$, $E = \\text{Rolling a 2}.$ \n- Flipping a coin 2 times: $\\Omega = \\{ (H, H), (H, T), (T, H), (T, T)\\}$, $E=\\text{Rolling a heads and a tail}$.\n- Distance a car travels before breaking down: $\\Omega = \\mathbb{R}_{+}$, $E=\\text{\nTravels greater than 100 miles}.$\n- Location of a dart thrown at a target. What is $\\Omega$ in this case? What is an event $E$?\n::::\n::::\n\n:::: {.fragment}\nThe sample space $\\Omega$ may be continuous or discrete and bounded or unbounded.\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Probability and Conditioning\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Probability Rules\n:::\n\n__Definition.__  Consider a sample space $\\Omega$. A _probability measure_ on $\\Omega$ is a function $P(\\cdot)$ defined on all the subsets of $\\Omega$ (the _events_) such that:\n  \n:::: {.incremental}\n1. $P(\\Omega) = 1$\n2. For any event $A \\subset \\Omega$, $P(A) \\geq 0.$\n3. For any events $A, B \\subset \\Omega$ where $A \\cap B = \\emptyset$, $P(A \\cup B) = P(A) + P(B)$.\n::::\n\n::: {.content-visible when-profile=\"web\"}\nOften we want to ask how a probability measure changes if we restrict the sample space to be some subset of $\\Omega$.  \n\nThis is called __conditioning.__\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Conditional probability\n:::\n\n__Definition.__ The _conditional probability_ of an event $A$ given that\nevent $B$ (having positive probability) is known to occur, is  \n\n$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)}~\\text{where}~P(B) > 0 $$\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThe function $P(\\cdot|B)$ is a probability measure over the sample space\n$B$.  \n\nNote that in the expression $P(A|B)$, $A$ is random but $B$ is fixed. \n\nNow if $B$ is a proper subset of $\\Omega,$ then $P(B) < 1$.   So $P(\\cdot|B)$ is a rescaling of the quantity $P(A\\cap B)$ so that $P(B|B) = 1.$ \n:::\n::: {.column width=\"40%\"}\n\n![](drawio/ConditionalProbability.png){fig-align=\"center\" width=80%}\n:::\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Law of total probability\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Law of total probability\n:::\n\nAn important tool for computing probabilities is provided by the law of total probabilities.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nLet $B_1$ and $B_2$ form the complete sample space $\\Omega$ and be disjoint $\\left(B_1 \\cap B_2 = \\emptyset\\right)$. Then for any event $A$,\n\n\\begin{align*}\nP(A) &= P(A\\cap B_1) + P(A \\cap B_2) \\\\\n&= P(A|B_1)P(B_1)+P(A|B_2)P(B_2)\n\\end{align*}\n:::\n::: {.column width=\"40%\"}\n![](drawio/LawOfTotalProbability.png){fig-align=\"center\" width=80%}\n:::\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Bayes' Theorem\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Bayes' Theorem\n:::\n\nBayes' Theorem is a simple way of manipulating conditional probabilities in a way that can be very useful.\n\nLet $A,B$ be events, then\n\n$$\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n$$\n\n:::: {.fragment}\n\nThe terms in the above equation are often given the following names:\n\n:::: {.incremental}\n- $P(A)$ is the prior probability.\n- $P(A|B)$ is the posterior probability.\n- $P(B|A)$ is the likelihood.\n- $P(B)$ is the marginal probability.\n::::\n::::\n\n\n:::: {.fragment}\nWe can extend this theorem to more than two events.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Bayes' Theorem continued\n:::\n\nStart with a situation in which we are interested in two events, $A_1$ and $A_2$. Let $B$ be any event in the samples space $\\Omega$.\n\nThese are exhaustive, meaning that in any experiment either $A_1$ or $A_2$ must occur. This means they form a partition of $\\Omega$, i.e., $\\Omega = A_1 \\cup A_2$ and $A_1 \\cap A_2 = \\emptyset$.\n\nIn this situation, Baye's theorem is:\n\n\\begin{align*}\nP(A_1|B) &= \\frac{P(A_1 \\cap B)}{P(B)} \\\\\n& = \\frac{P(A_1 \\cap B)}{P(B|A_1) + P(B|A_2)} \\\\\n&= \\frac{P(B|A_1)P(A_1)}{P(B|A_1) + P(B|A_2)}.\n\\end{align*}\n\n:::: {.fragment}\nThis is can be extended to the case where $A_1, A_2, ..., A_n$ form a partition of $\\Omega$.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Importance of Bayes' Theorem \n:::\n\nThis formula is useful because often the probabilities $P(B|A_i)$ can be estimated, while the probabilties $P(A_i|B)$ may be hard to estimate.\n\nOr perhaps $P(B)$ itself (which is really the denominator) is easy to estimate.\n\nWe interpret this transformation as updating our estimate of the probability of each $A_i$ based on new information, namely, that $B$ is true.   \n\nThis update transforms the __prior__ probabilities $P(A_i)$ into the __posterior__ probabilities $P(A_i|B)$.\n\n::: {.content-visible when-profile=\"web\"}\n### Bayes' Theorem Example\n\nEmpirical evidence suggests that amongs sets of twins, about 1/3 are identical. \n\nAssume therefore that probability of a pair of twins being identical to be 1/3.  \n\nNow, consider how a couple might update this probability after they get an ultrasound that shows that the twins are of the same gender.   \n\nWhat is their new estimate of the probability that their twins are identical?\n\nLet $I$ be the event that the twins are identical. Let $G$ be the event that gender is the same via ultrasound. \n\nThe prior probability here is $P(I)$.\n\nWhat we want to calculate is the posterior probability $P(I|G)$.\n\nFirst, we note:\n\n$$P(G|I) = 1.$$\n\nSurprisingly, people are sometimes confused about that fact!\n\nWe also assume that if the twins are not identical, they are like any two siblings. This means they have an equal probability of being the same gender, i.e., \n\n$$P(G|\\bar{I}) = 1/2.$$\n\nWe know from observing the population at large that among all sets of twins, about 1/3 are identical\n\n$$ P(I) = 1/3.$$\n\nThis statistic is easy obtained from data.\n\nThen:\n\n\\begin{align*}\nP(I|G) &= \\frac{P(G|I)P(I)}{P(G|I)P(I) + P(G|\\bar{I})P(\\bar{I})} \\\\ \n&= \\frac{1 \\cdot 1/3}{(1 \\cdot 1/3) + (1/2 \\cdot 2/3)} \\\\\n&= \\frac{1}{2}\n\\end{align*}\n\nSo we have updated our estimate of the twins being identical from 1/3\n(prior probability) to 1/2 (posterior probability).\n\nWe did this in a way that used quantities that were relatively easy to obtain or measure.\n\n:::\n::: {.content-visible when-profile=\"slides\"}\n## Independent Events\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### Independent Events\n:::\n\n__Definition.__ Two events $A$ and $B$ are __independent__ if $P(A\\cap B) = P(A) \\cdot P(B).$\n\nThis is exactly the same as saying that $P(A|B) = P(A).$  \n\nSo we can see that the intuitive notion of independence is that \"if one event occurs, that does not change the probability of the other event.\"\n\n## Random Variables\n\nWhen an experiment is performed, we might be interested in the actual outcome. However, more frequently we are interested in some function of the outcome. This leads us to the notion of a __random variable__.\n\n__Definition.__ A random variable $X$ is a function $X:\\Omega\\rightarrow \\mathbb{R}$.\n\nWe generally use capital letters to denote random variables and lowercase to denote non-random quantities.\n\n:::: {.fragment}\nWe distinguish between discrete and continuous random variables.  \n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Discrete random variables\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Discrete random variables\n:::\n\nA _discrete random variable_ is a random variable that can take on\nonly a finite or at most a countably infinite number of values.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- The number of points showing after a roll of a die. \n- The number of heads after flipping a coin twice.\n::::\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Continous random variables\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Continuous random variables\n:::\n\nA _continuous random variable_ is a random variable that can take on any value from a given range. That is, a continuous random variable has an uncountable set of possible outcomes.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- The lifetime of a light bulb.\n- The distance a car travels before breaking down.\n::::\n::::\n\n## Distributions\n\nTo collect information about what values of random variable are more probable than others, we introduce the concept of distributions.\n\n:::: {.fragment}\nWe have two options to consider:\n\n:::: {.incremental}\n- discrete distributions using the _probability mass function (PMF)_, and\n- continuous distributions using the _probability density function (PDF)_.\n::::\n::::\n\n:::: {.fragment}\nWe will also introduce the cumulative distribution functions corresponding to both PMFs and PDFS.\n::::\n\n## Discrete Distributions\n\n::: {.content-visible when-profile=\"web\"}\n### Probability Mass Function\n:::\n\n__Definition.__ For a discrete random variable $X$, we define the _probability mass function (PMF)_ $p(a)$ of $X$ by\n\n$$p(a) = P(X=a).$$\n\nThe PMF $p(a)$ is positive for at most a countable number of values of $a$. That is, if $X$ must assume one of the values $x_1, x_2,...,$ then\n\n\\begin{align*}\n&p(x_i)  \\geq 0 \\:  \\text{ for } i=1,2,... \\\\\n&p(x)  = 0 \\: \\text{ for all other values of } x.\n\\end{align*}\n\nSince $X$ must take on one of the values $x_i$, we have\n\n$$\\sum_{i=1}^{\\infty} p(x_i) = 1.$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Discrete Distribution Example\n:::\n\n__Example.__  Consider the roll of a single die.  The random variable here is the number of points showing. What is the PDF of this discrete random variable?\n\nWe assign equal probabilities to each outcome $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$:\n\n$$p(x_i) = \\frac{1}{6}.$$\n\n::: {#7714c9a3 .cell execution_count=3}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 3))\nx = np.arange(1, 7)\nplt.plot(x, 6*[1/6.], 'bo', ms=8)\nplt.vlines(x, 0, 1/6., colors='b', lw=5, alpha=0.5)\nplt.xlim([0.5, 6.5])\nplt.ylim([0, 1.1])\nplt.xlabel(r'x (Number of points showing)', size=14)\nplt.ylabel(r'$P(X = x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-3-output-1.png){width=446 height=287 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n### Discrete Cumulative Distribution Function\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Discrete Cumulative Distribution Function\n:::\n\n__Definition.__ The _cumulative distribution function (CDF)_ $F$ can be expressed in terms of $p(a)$ by\n\n$$F(a) = P(X \\leq a) = \\sum_{x\\leq a} p(x).$$\n\nIf $X$ is a discrete random variable whose possible values are $x_1 , x_2 , x_3 ,...$, where $x_1 < x_2 < x_3 <...$, then the distribution function $F$ of $X$ is a step function. That is, the value of $F$ is constant in the intervals $[x_{i−1},x_i)$ and then takes a step (or jump) of size $p(x_i)$ at $x_i$. \n\n\n::: {.content-visible when-profile=\"slides\"}\n## Discrete CDF example\n:::\n\n__Example.__  Let us return to the roll of a single die. The corresponding CDF is shown below.\n\n::: {#562f14f8 .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure()\nfor i in range(7):\n    plt.plot([i, i+1-.08], [i/6, i/6],'-b')\nfor i in range(1, 7):\n    plt.plot(i, i/6, 'ob')\n    plt.plot(i, (i-1)/6, 'ob', fillstyle = 'none')\nplt.xlim([0, 7])\nplt.ylim([-0.05, 1.1])\nplt.title('Cumulative Distribution Function (CDF)')\nplt.xlabel(r'$x$ (Number of points showing)', size=14)\nplt.ylabel(r'$P(X\\leq x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-4-output-1.png){width=599 height=457}\n:::\n:::\n\n\n## Continous Distributions\n\n::: {.content-visible when-profile=\"web\"}\n### Probability Density Function\n:::\n\nContinuous random variables are described by their _probability density functions (PDFs)_. If $f$ is a PDF, then it has the following properties:\n\n:::: {.incremental}\n- $f(x) \\geq 0,$\n- $\\int_{- \\infty}^{\\infty} f(x) dx = 1.$\n::::\n\n:::: {.fragment}\nThe probability density function plays a central role in probability theory, because all probability statements about a continuous random variable can be answered in terms of its PDF. \n\nFor instance, for a continuous variable $X$ where $P(X\\leq a) = \\int_{-\\infty}^a f(x) dx$, we obtain\n\n$$P(a \\leq X \\leq b) = \\int_{-\\infty}^b f(x) dx - \\int_{-\\infty}^a f(x) dx = \\int_a^b f(x) dx.$$\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Continuous Cumulative Distribution Function\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Continuous Cumulative Distribution Function\n:::\n\n__Definition.__ The _cumulative distribution function (CDF)_ $F$ can be expressed as\n\n$$F(a) = P(X\\leq a) = \\int_{-\\infty}^a f(x) dx.$$\n\n:::: {.fragment}\nThe relationship between the continuous CDF and PDF is\n\n$$f(x) = \\frac{d F(x)}{dx}.$$\n::::\n\n::: {.content-visible when-profile=\"web\"}\nThe above formula tells us that the PDF is the derivative of the CDF.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Continuous CDF example\n:::\n\nHere is an example of a continuous CDF of some random variable.\n\n::: {#3ad5f366 .cell execution_count=5}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure(figsize=(5, 3))\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.cdf(x),'b-', lw=5, alpha=0.6)\nplt.title('Cumulative Distribution Function (CDF)')\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$P(X\\leq x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-5-output-1.png){width=446 height=307 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n## Characterizing Random Variables\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Expected value\n:::\n\n__Definition.__ The _expected value_ $E[X]$ of a random variable $X$ is the probability-weighted sum or integral of all possible values of the R.V.  \n\nFor a discrete random variable, this is:\n\n$$E[X] \\equiv \\sum_{x} x \\cdot P(X=x).$$\n\nFor a continuous random variable with pdf $p()$\n\n$$E[X] \\equiv \\int_{-\\infty}^{+\\infty} x p(x) dx.$$\n\n::: {.content-visible when-profile=\"web\"}\nThe expected value is also called the average or the mean, although we\nprefer to reserve those terms for empirical statistics (actual measurements, not idealizations like these formulas).\n\nThe expected value is in some sense the \"center of mass\" of the random\nvariable. It is often denoted $\\mu$.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Expected value continued\n:::\n\nThe expected value is usually a quite useful characterization of the random variable.\n\nHowever, be careful: in certain situations, it may not be very informative, or important.  \n\nIn some cases a random variable may not ever take on the expected value as a possible value. \n\n::: {.content-visible when-profile=\"web\"}\nAs an example, consider the single die, whose expected value is 3.5\n:::\n\nIn other cases the notion of expected value isn't useful. Consider a person with their head in an oven and their feet in a freezer who claims \"on average I feel fine.\" \n\nImportantly, the mean may not be very informative when observations are highly variable. \n\n::: {.content-visible when-profile=\"slides\"}\n## Variance\n:::\n\nThe variability of random quantities is crucially important in order to characterize them.\n\nFor this we use __variance,__ the mean squared difference of the random variable from its expected value.\n\n__Definition.__ The variance of a random variable $X$ is\n\n$$\\text{Var}(X) \\equiv E[(X - E[X])^2].$$\n\nFor example, given a discrete R.V. with $E[X] = \\mu$ this would be:\n\n$$\\text{Var} (X) = \\sum_{x} (x-\\mu)^2 P(X=x).$$\n\nWe use the symbol $\\sigma^2$ to denote variance.\n\nThe units of variance are the square of the units of the mean. \n\nSo to compare variance and mean in the same units, we take the square root of the variance.\n\nThis is called the __standard deviation__ and is denoted $\\sigma$.\n\n::: {.content-visible when-profile=\"slides\"}\n## Stocks as random variables\n:::\n\nNext, let's recall the case of the Tesla and NVidia returns from the last lecture:\n\n::: {#ddbfdbab .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nimport yfinance as yf\n\nstocks = ['TSLA', 'NVDA']\ndf = pd.DataFrame()\nfor s in stocks:\n    df[s] = pd.DataFrame(yf.download(s, start='2023-01-01', end='2023-12-31', progress = False))['Close']\n\nrets = df.pct_change(30)\nrets[['TSLA', 'NVDA']].plot(lw=2)\nplt.legend(loc='best')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-6-output-1.png){width=592 height=412 fig-align='center'}\n:::\n:::\n\n\nTreating these two time series as random variables, we are interested in how they vary __together__.\n\n::: {.content-visible when-profile=\"slides\"}\n## Covariance\n:::\n\nThis is captured by the concept of __covariance.__\n\n__Definition.__ For two random variables $X$ and $Y$, their _covariance_ is defined as:\n\n$$\\text{Cov}(X,Y) = E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]$$\n\nIf covariance is positive, this tells us that $X$ and $Y$ tend to both be  above their means together and both below their means together.\n\nWe will often denote $\\text{Cov}(X,Y)$ as $\\sigma_{XY}$.\n\n::: {.content-visible when-profile=\"\"}\n## Correlation\n:::\nIf we are interested in asking \"how similar\" are two random variables, we want to normalize covariance by the amount of variance shown by the random variables.\n\nThe tool for this purpose is __correlation__, i.e., normalized covariance:\n\n$$\\rho(X,Y) = \\frac{E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]}{\\sigma_X \\sigma_Y}.$$\n\n$\\rho(X, Y)$ takes on values between -1 and 1.   \n\nIf $\\rho(X, Y) = 0$ then $X$ and $Y$ are __uncorrelated__. Note that this is not the same thing as being independent!\n\n$\\rho$ is sometimes called \"Pearson $r$\" after Karl Pearson who popularized it.\n\n::: {.content-visible when-profile=\"slides\"}\n## Stock covariance\n:::\n\nLet's look at our example again:\n\n::: {#6bb0841b .cell execution_count=7}\n``` {.python .cell-code}\nrets = df.pct_change(30)\nrets[['TSLA', 'NVDA']].plot(lw=2)\nplt.legend(loc='best')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-7-output-1.png){width=592 height=412}\n:::\n:::\n\n\n::: {#46b7a9c4 .cell execution_count=8}\n``` {.python .cell-code}\ndf.cov()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>NVDA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>TSLA</th>\n      <td>1757.018115</td>\n      <td>388.467438</td>\n    </tr>\n    <tr>\n      <th>NVDA</th>\n      <td>388.467438</td>\n      <td>115.701324</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn the case of Tesla ($X$) and Nvidia ($Y$) above, we find that\n\n$$\\text{Cov}(X,Y) \\approx 388.$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Stock correlation\n:::\n\nHow similar are these random variables? Let's compute $\\rho(X,Y)$.\n\n::: {#5f07257f .cell execution_count=9}\n``` {.python .cell-code}\ndf.corr()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>NVDA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>TSLA</th>\n      <td>1.000000</td>\n      <td>0.861583</td>\n    </tr>\n    <tr>\n      <th>NVDA</th>\n      <td>0.861583</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe observe that \n\n$$\\rho(X,Y) \\approx 0.86.$$\n\nThere appears to be some similarity between the two stocks.\n\n## Multivariate Random Variables\n\nA __multivariate random variable__ is a vector of random variables.\n\nWe often simply say a \"random vector\".\n\nThat is, \n\n$$ \\mathbf{X} = \\left[\\begin{array}{c}X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n\\end{array}\\right] $$\n\nThe expected value of a random vector is obtained by taking the expected value of each component random variable:\n    \n$$ E[\\mathbf{X}] = \\mathbf{\\mu_X} = \\left[\\begin{array}{c}E[X_1] \\\\ E[X_2] \\\\ \\vdots \\\\ E[X_n]\\end{array}\\right] $$\n\nTo properly characterize the variability of a random vector, we need to specify all the covariances of pairs of components.\n\nWe organize these values into a __covariance matrix:__\n\n$$ \\text{Cov}[\\mathbf{X}] = \n\\begin{bmatrix}\n\\text{Var}[X_1] & \\text{Cov}[X_1, X_2] & \\dots & \\text{Cov}[X_1, X_n] \\\\\n\\text{Cov}[X_2, X_1] & \\text{Var}[X_2] & \\dots & \\text{Cov}[X_2, X_n]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\text{Cov}[X_n,X_1] & \\text{Cov}[X_n, X_2] & \\dots & \\text{Var}[X_n]\\\\\n\\end{bmatrix} $$\n\nA couple things to note about a covariance matrix.\n\n1. The covariance matrix is symmetric (because $\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)$)\n2. The covariance matrix is __positive semidefinite__, which means $\\mathbf{z}^T \\text{Cov}[X] \\mathbf{z}\\geq 0$ for all vectors $\\mathbf{z}$.\n\n::: {.content-visible when-profile=\"web\"}\n### Random Variables as Vectors\n\n\nWhen working with data, we will often treat multiple observations of some feature as samples of a random variable.\n\nWe will also typically organize the observations into a vector.\n\nRecall our stock data:\n\n::: {#e9c0abfa .cell execution_count=10}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>NVDA</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-01-03</th>\n      <td>108.099998</td>\n      <td>14.315</td>\n    </tr>\n    <tr>\n      <th>2023-01-04</th>\n      <td>113.639999</td>\n      <td>14.749</td>\n    </tr>\n    <tr>\n      <th>2023-01-05</th>\n      <td>110.339996</td>\n      <td>14.265</td>\n    </tr>\n    <tr>\n      <th>2023-01-06</th>\n      <td>113.059998</td>\n      <td>14.859</td>\n    </tr>\n    <tr>\n      <th>2023-01-09</th>\n      <td>119.769997</td>\n      <td>15.628</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEach column can be treated as a vector.\n\nSo:\n\n* Let's say that our data frame `df` is represented as a matrix $D$, \n* and that `df['TSLA']` are observations of some random variable $X$, \n* and `df['NVDA']` are observations of some random variable $Y$. \n\nLet $D$ have $m$ rows (i.e., $m$ observations).\n\nNow, let us subtract from each column its mean, to form a new matrix $\\tilde{D}$.\n\nIn the new matrix $\\tilde{D}$, every column has zero mean.\n\nThen notice the following: the Covariance matrix of $(X, Y)$ is simply $\\frac{1}{m}\\tilde{D}^T\\tilde{D}$.\n\nTo see this compute\n\n\\begin{align*}\n\\text{Cov}(X,Y) &= E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right] \\\\ \n&= \\frac{1}{m} \\sum_i (\\tilde{D}_{i1} \\cdot \\tilde{D}_{i2})\\\\\n&= \\frac{1}{m}\\;\\tilde{d}_1^T\\tilde{d}_2,\n\\end{align*}\n\nwhere $\\tilde{d}_1$ and $\\tilde{d}_2$ are the columns of $\\tilde{D}$.\n\nThis shows that __covariance is actually an inner product__ between normalized observation vectors.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Low and High Variability\n\nHistorically, most sources of random variation that have concerned\nstatisticians are instances of low variability.  \n\nThe original roots of probability in the study of games of chance, and later in the study of biology and medicine, have mainly studied objects with low variability.\n\nNote that by \"low variability\" I don't mean that such variability is unimportant.\n\nSome examples of random variation in this category are: \n\n* the heights of adult humans\n* the number of trees per unit area in a mature forest\n* the sum of 10 rolls of a die\n* the time between emission of subatomic particles from a radioactive material.\n\nIn each of these cases, there are a range of values that are \"typical,\" and there is a clear threshold above what is typical, that essentially never occurs.\n\nOn the other hand, there are some situations in which variability is\nquite different.  \n\nIn these cases, there is no real \"typical\" range of\nvalues, and arbitrarily large values can occur with non-negligible\nfrequency.   \n\nSome examples in this category are\n\n* the distribution of wealth among individuals in society\n* the sizes of human settlements\n* the areas burnt in forest fires\n* the runs of gains and losses in various financial markets over time\n* and the number of collaborators a scholar has over their lifetime.   \n\n### Example\n\n> The banking system (betting against rare events) just lost [more than] 1\n> Trillion dollars (so far) on a single error, more than was ever earned\n> in the history of banking.\n\nNassim Nicholas Taleb, September 2008 \n\n![](figs/derivatives-portfolio-variation.png){fig-align=\"center\"}\n\nAn example of a run of observations showing high variability.   This figure shows the daily variations in a derivatives portfolio over the timeframe 1988-2008. About 99% of the variation over the 20 years occurs in a single day (the day the European Monetary System collapsed).\n:::\n\n## Important Distributions\n\n::: {.content-visible when-profile=\"web\"}\nNow we will review certain distributions that come up over and over again in typical situations.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\nWe will review the following important distributions\n\n:::: {.incremental}\n- Bernoulli distribution (discrete)\n- Poisson distribution (discrete)\n- Gaussian distribution (continous)\n::::\n\n:::: {.fragment}\nSee the course notes for other important distributions.\n::::\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Bernoulli Distribution\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Bernoulli Distribution\n:::\n\nAn experiment of a particularly simple type is one in which there are only two possible outcomes, such as \n\n:::: {.incremental}\n- head or tail,\n- success or failure,\n- defective or nondefective component,\n- patient recovers or does not recover.\n::::\n\n:::: {.fragment}\nEach distribution has one or more _parameters_.  Parameters are settings that control the distribution. A Bernoulli distribution has one parameter, $p$, which is the probability that the random variable is equal to 1. \n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Bernoulli Distribution\n:::\n__Definition.__ It is said that a random variable $X$ has a _Bernoulli distribution_ with parameter $p$ $(0\\leq p \\leq 1)$ if $X$ can take only the values 0 and 1 and the corresponding probabilities are\n\n$$P(X=1) = p \\: \\text{ and } \\: P(X=0) = 1-p.$$\n\n:::: {.fragment}\nNote that there is a particularly concise way of writing the above definition:\n\n$$ p(x) = P(X=x) = p^x (1-p)^{(1-x)} \\: \\text{ for } \\: x = 0 \\text{ and } x=1.$$\n\nThe mean of a $X$ is $p$ and the variance of $X$ is $p(1-p)$.\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### The Binomial Distribution\n\nThe binomial distribution considers precisely $N$ Bernoulli trials. Each trial has the probability of a success equal to $p$. $N$ and $p$ are the parameters of the binomial distribution. \n\nThe binomial distribution answers the question \"What is the probability there will be $k$ successes in $N$ trials?\"\n\n__Definition.__ If $X$ represents the number of successes that occur in $N$ trials, then $X$ is said to have a _binomial distribution_ with parameters $N$ and $p$ $(0\\leq p \\leq 1)$. The PMF of a binomial random variable is given by\n\n$$p(k) = P(X=k) = \\binom{N}{k}\\; p^k\\; (1-p)^{N-k} \\: \\text{ for } k=0,1,...,N.$$\n\nThe validity of the above PMF can be verified as follows. First we notice, that, by the assumed independence of trials, for any __given__ sequence of $k$ successes and $N-k$ failures, the probability is $p^k \\;(1-p)^{N-k}$. Then there are $\\binom{N}{k}$ different sequences of the $N$ outcomes leading to $k$ successes and $N-k$ failures.\n\nThe mean of the Binomial distribution is $pN$, and its variance is $p(1-p)N$.\n\nThe PMF of the Binomial distribution with $N = 10$ and $p=0.3$ is shown below.\n\n::: {#4429603b .cell execution_count=11}\n``` {.python .cell-code}\nfrom scipy.stats import binom\np = 0.3\nx = np.arange(binom.ppf(0.01, 10, p), binom.ppf(0.9995, 10, p))\nplt.ylim([0, 0.4])\nplt.xlim([-0.5, max(x)+0.5])\nplt.plot(x, binom.pmf(x, 10, p), 'bo', ms=8, label = 'binom pmf')\nplt.vlines(x, 0, binom.pmf(x, 10, p), colors='b', lw = 5, alpha=0.6)\nplt.title(f'Binomial PDF, $p$ = {p}, $N$ = 10', size=14)\nplt.xlabel(r'$k$', size=14)\nplt.ylabel(r'$P(X = k)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-11-output-1.png){width=603 height=458 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Geometric Distribution\n\nThe geometric distribution concerns Bernoulli trials as well. It has only one parameter $p$, the probability of success.\n\nThe geometric distribution answers the question: \"What is the probability it takes $k$ trials to obtain the first success?\"\n\n__Definition.__ It is said that a random variable $X$ has a _geometric distribution_ with parameter $p$ $(0\\leq p \\leq 1)$ if $X$ has a discrete distribution with \n\n$$ p(k) = P(X = k) = p(1-p)^{k-1} \\qquad \\text{for} \\: k \\geq 1.$$ \n\nThe mean of the geometric distribution is equal to $\\frac{1}{p}$ and its variance is $\\frac{1-p}{p^2}$. \n\nAn example of the geometric PMF is given below.\n\n::: {#da220063 .cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.stats import geom\np = 0.3\nx = np.arange(geom.ppf(0.01, p), geom.ppf(0.995, p))\nplt.ylim([0, 0.4])\nplt.xlim([0.5, max(x)])\nplt.plot(x, geom.pmf(x, p), 'bo', ms=8, label = 'geom pmf')\nplt.vlines(x, 0, geom.pmf(x, p), colors='b', lw = 5, alpha = 0.6)\nplt.title(f'Geometric PDF, $p$ = {p}', size=14)\nplt.xlabel(r'$k$', size=14)\nplt.ylabel(r'$P(X = k)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-12-output-1.png){width=611 height=458 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Poisson Distribution\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Poisson Distribution\n:::\n\nAlthough the Bernoulli trials underlie all of the previous distributions, they do not form the basis for the Poisson distribution. To introduce the Poisson distribution we will look at some examples of random variables that generally obey the Poisson probability law are as follows:\n\n1. The number of misprints on a page (or a group of pages) of a book;\n1. The number of people in a community who survive to age 100;\n1. The number of wrong telephone numbers that are dialed in a day;\n1. The number of customers entering a post office on a given day;\n1. The number of $\\alpha$-particles discharged in a fixed period of time from some radioactive material.\n\nIn the above examples the events appear to happen at a certain rate, but completely at random (i.e., without a certain structure). \n\n::: {.content-visible when-profile=\"slides\"}\n## The Poisson Distribution continued\n:::\n\nA Poisson distribution answers the question: \"How many successes occur in a fixed amount of time?\"\n\n__Definition.__ A random variable $X$ that takes on one of the values $0, 1, 2,...$ is said to have a _Poisson distribution_ with parameter $\\lambda$ if, for some $\\lambda > 0$,\n\n$$p(k) = P(X=k) = \\lambda^k \\frac{e^{- \\lambda}}{k!} \\: \\text{ for } k = 0, 1, 2,...$$\n\n::: {.content-visible when-profile=\"web\"}\nThis $p(k)$ defines a PMF, because\n\n$$\\sum_{k=0}^{\\infty} p(k) = \\sum_{k=0}^{\\infty} \\lambda^k \\frac{e^{- \\lambda}}{k!} = e^{- \\lambda} \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{- \\lambda} e^{\\lambda} = 1.$$\n\nHere, we used the fact that the exponential function $e^{\\lambda}$, can be expressed as series, $\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}$ for every real number $\\lambda$.\n:::\n\nBoth the mean and the variance of a Poisson distribution with parameter $\\lambda$ are equal to $\\lambda.$\n\n::: {.content-visible when-profile=\"slides\"}\n## Poisson Distribution Example\n:::\nThe PMF of the Poisson distribution with parameter $\\lambda = 0.3$  is shown below.\n\n::: {#93f0f357 .cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats import poisson\nmu = 3\nx = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.9995, mu))\nplt.xlim([-0.5, max(x)+0.5])\nplt.ylim(ymax = 0.4)\nplt.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\nplt.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.6)\nplt.title(f'Poisson PDF.  $\\lambda$ = {mu/10}', size=14)\nplt.xlabel(r'k', size=14)\nplt.ylabel(r'P(X = k)', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:8: SyntaxWarning: invalid escape sequence '\\l'\n<>:8: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/hy/hnd1x3g539dfkglbs96k3myh00d1lj/T/ipykernel_29865/1509079810.py:8: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f'Poisson PDF.  $\\lambda$ = {mu/10}', size=14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-13-output-2.png){width=603 height=457 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\nThe Poisson random variable has a tremendous range of applications in diverse areas. The reason for that is the fact that a Poisson distribution with mean $Np$ may be used as an approximation for a binomial distribution with parameters $N$ and $p$ when $N$ is large and $p$ is small enough so that $Np$ is of moderate size. \n\n\nThe Poisson distribution has an interesting role in our perception of randomness (which you can read more about [here](http://www.empiricalzeal.com/2012/12/21/what-does-randomness-look-like/#more-2450)).    \n\nThe classic example comes from history.  From the above site:\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Poisson Distribution and Horse Kicks\n:::\n\n>In 1898 Ladislaus Bortkiewicz, a Russian statistician of Polish descent, was trying to understand why, in some years, \n>an unusually large number of soldiers in the Prussian army were dying due to horse-kicks. In a single army corp, \n>there were sometimes 4 such deaths in a single year. Was this just coincidence?\n\nTo assess whether horse-kicks were random (not following any pattern) Bortkiewicz simply compared the number per year to what would be predicted by the Poisson distribution.\n\n::: {.content-visible when-profile=\"slides\"}\n## Poisson Distribution and Horse Kicks continued\n:::\n\n::: {#1580c0b4 .cell execution_count=14}\n``` {.python .cell-code}\n# note that this data is available in 'data/HorseKicks.txt'\nhorse_kicks = pd.DataFrame(\ndata = np.array([\n[0, 108.67, 109],\n[1, 66.29, 65],\n[2, 20.22, 22],\n[3, 4.11, 3],\n[4, 0.63, 1],\n[5, 0.08, 0],\n[6, 0.01, 0]]),\ncolumns = [\"Number of Deaths Per Year\",\"Predicted Instances (Poisson)\",\"Observed Instances\"])\nhorse_kicks\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Number of Deaths Per Year</th>\n      <th>Predicted Instances (Poisson)</th>\n      <th>Observed Instances</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>108.67</td>\n      <td>109.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>66.29</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>20.22</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>4.11</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>0.63</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.0</td>\n      <td>0.08</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.0</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#1a5143c6 .cell execution_count=15}\n``` {.python .cell-code}\nhorse_kicks[[\"Predicted Instances (Poisson)\",\"Observed Instances\"]].plot.bar()\nplt.xlabel(\"Number of Deaths Per Year\", size=14)\nplt.ylabel(\"Count\", size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-15-output-1.png){width=598 height=430 fig-align='center'}\n:::\n:::\n\n\nThe message here is that when events occur at random, we actually tend to perceive them as clustered.   \n\n::: {.content-visible when-profile=\"web\"}\nHere is another example:\n\n![](figs/pinker-glow-worms-and-stars-plot.jpg)\n\nWhich of these was generated by a random process ocurring equally likely everywhere?\n\nThese images are from Steven Pinker’s book, _The Better Angels of our Nature._\n\nIn the left figure, the number of dots falling into regions of a given size follows the Poisson distribution.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Exponential Distribution\n\nThe exponential distribution is an example of a continuous distribution. It concerns a Poisson process and has one parameter, $\\lambda$, the rate of success.\n\n__Definition.__ A continuous random variable whose PDF is given, for some $\\lambda > 0$, by\n\n$$f(x) = \\;\\left\\{\\begin{array}{cr} \\lambda e^{-\\lambda x} & \\text\n{for  } x \\geq 0, \\\\ 0 & \\text{otherwise}\\end{array}\\right.$$\n\nis said to be _exponentially distributed_ with parameter $\\lambda$.\n\nThe CDF, $F(a)$, of an exponential random variable is given by\n\n$$F(a) = P(X \\leq a) = 1 - e^{-\\lambda a} \\mbox{ for } a \\geq 0.$$ \n\nIts mean is $1/\\lambda$, and the variance is $1/\\lambda^2$.\n\nThe exponential distribution answers the question: \"What is the probability it takes time $x$ to obtain the first success?\"\n\n::: {#df3d32f9 .cell execution_count=16}\n``` {.python .cell-code}\nfrom scipy.stats import expon\np = 0.3\nx = np.linspace(expon.ppf(0.01, scale=1/p), expon.ppf(0.995, scale=1/p), 100)\nplt.plot(x, expon.pdf(x, scale=1/p),'b-', lw = 5, alpha = 0.6, label='expon pdf')\nplt.title(f'Exponential PDF, $\\lambda$ = {p}', size=14)\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$p(x)$', size=14)\nplt.ylim([0, 0.4])\nplt.xlim([0, 14])\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:5: SyntaxWarning: invalid escape sequence '\\l'\n<>:5: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/hy/hnd1x3g539dfkglbs96k3myh00d1lj/T/ipykernel_29865/3643123630.py:5: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f'Exponential PDF, $\\lambda$ = {p}', size=14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-16-output-2.png){width=613 height=458 fig-align='center'}\n:::\n:::\n\n\nThe exponential is the __continuous analog__ of the geometric distribution.\n\n::: {#35dbf44c .cell execution_count=17}\n``` {.python .cell-code}\nfrom math import exp \nlam = 0.3\np = 1 - exp(- lam)\nx = np.linspace(expon.ppf(0, scale=1/lam), expon.ppf(0.995, scale=1/lam), 100)\nplt.plot(x, expon.cdf(x, scale=1/lam),'b-', lw = 5, alpha = 0.6, label='Exponential')\nxg = np.arange(geom.ppf(0, p), geom.ppf(0.995, p))\nplt.ylim([0, 1])\nplt.xlim([-0.5, max(xg)])\nplt.plot(xg, geom.cdf(xg, p), 'ro', ms = 8, label = 'Geometric')\nplt.suptitle(f'Geometric and Exponential CDFs', size = 14)\nplt.title(r'$\\lambda = 0.3; \\;\\;\\; p = 1-e^{-\\lambda} =$' + f' {p:0.3f}', size=12)\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$P(X \\leq x)$', size = 14)\nplt.legend(loc = 'best')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-17-output-1.png){width=594 height=483 fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Uniform Distribution\n\nThe uniform distribution models the case in which all outcomes are equally probable.  \n\nIt can be a discrete or continuous distribution.\n\nWe have already seen the uniform distribution in the case of rolls of a fair die.\n\n::: {#0318d1f2 .cell execution_count=18}\n``` {.python .cell-code}\nplt.figure()\nx = np.arange(1, 7)\nplt.plot(x, 6*[1/6.], 'bo', ms=8)\nplt.vlines(x, 0, 1/6., colors='b', lw=5, alpha=0.5)\nplt.xlim([0.5, 6.5])\nplt.ylim([0, 1.1])\nplt.xlabel(r'x (Number of points showing)', size=14)\nplt.ylabel(r'$P(X = x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-18-output-1.png){width=594 height=435 fig-align='center'}\n:::\n:::\n\n\nThere is an important relationship between the uniform and Poisson distributions.\n\n__When the time an event occurs is uniformly distributed, the number of events in a time interval is Poisson distributed.__\n\nYou can replace \"time\" with \"location\", and so on.\n\nAlso, the reverse statment is true as well.\n\nSo a simple way to generate a picture like the scattered points above is to select the $x$ and $y$ coordinates of each point uniformly distributed over the picture size.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### The Gaussian Distribution\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Gaussian Distribution\n:::\n\nThe Gaussian Distribution is also called the Normal Distribution.\n\nWe will make __extensive__ use of Gaussian distribution, for a number of reasons.\n\n![](/figs/Gauss-on-Deutschemark.png){fig-align=\"center\" width=\"600px\"}\n\nOne of reasons we will use it so much is that it is a good guess for how errors are distributed in data.\n\n::: {.content-visible when-profile=\"web\"}\nThis comes from the celebrated __Central Limit Theorem__.  Informally, \n\n>_The sum of a large number of independent observations from any\n>distribution with finite variance tends to have a Gaussian\n> distribution._\n\n![](figs/Galton-Bean-Machine.png){fig-align=\"center\" width=\"350px\"}\"\n\nAs a special case, the sum of $n$ independent Gaussian variates is Gaussian.\n\nThus Gaussian processes remain Gaussian after passing through linear\nsystems.   \n\nIf $X_1$ and $X_2$ are Gaussian, then $X_3 = aX_1 + bX_2$ is Gaussian.\n\nOne way of thinking of the Gaussian is that it is the limit of the Binomial when $N$ is large, that is, the limit of the sum of many Bernoulli trials.    \n\nHowever, because of the central limit theorem, many other sums of random variables (not just Bernoulli trials) converge to the Gaussian.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## The Gaussian Distribution PDF\n:::\n\nThe _standard Gaussian_ distribution has mean zero and a variance (and standard deviation) of 1. The pdf of the standard Gaussian is:\n\n$$ p(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2}. $$\n\nFor an arbitrary Gaussian distribution with mean $\\mu$ and variance\n$\\sigma^2$, the pdf is simply the standard Gaussian that is relocated to\nhave its center at $\\mu$ and its width scaled by $\\sigma$\n\n$$ p_{\\mu,\\sigma}(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.$$\n\n::: {.content-visible when-profile=\"slides\"}\n## The Gaussian PDF Example\n:::\n\n::: {#6b9bb7fd .cell execution_count=19}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.pdf(x),'b-', lw = 5, alpha = 0.6)\nplt.title(r'Standard Gaussian PDF.  $\\mu = 0, \\sigma = 1$', size=14)\nplt.xlabel('x', size=14)\nplt.ylabel(r'$p(x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-19-output-1.png){width=605 height=458 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## The Gaussian CDF Example\n:::\n\n::: {#e9e6a4bc .cell execution_count=20}\n``` {.python .cell-code}\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.cdf(x),'b-', lw = 5, alpha = 0.6)\nplt.title(r'Standard Gaussian CDF.  $\\mu = 0, \\sigma = 1$', size=14)\nplt.xlabel('x', size=14)\nplt.ylabel(r'$P(X\\leq x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-20-output-1.png){width=594 height=458 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n### Heavy Tails\n\nEarlier we discussed high- and low-variability.   \n\nAll of the distributions we have discussed so far have \"light tails\", meaning that they show low variability.\n\nIn other words, extremely large observations are essentially impossible.\n\nHowever in other cases, extremely large observations can occur.   Distributions that capture this property are called \"heavy tailed\".\n\nSome examples of data that can be often modeled using heavy-tailed distributions:\n\n* The sizes of files in a file system.\n* The sizes of objects transferred over the Internet.\n* The execution time of jobs on a computer system.\n* The degree of nodes in a network (e.g., social network).\n\nIn practice, random variables that follow heavy tailed distributions\nare characterized as exhibiting many small observations mixed in with\na few large observations.  \n\nIn such datasets, most of the observations\nare small, but most of the contribution to the sample mean or variance\ncomes from the rare, large observations.\n\n#### The Pareto Distribution\n\nThe Pareto distribution is the simplest continuous heavy-tailed\ndistribution.    \n\nPareto was an Italian economist who studied income distributions.\n(In fact, income distributions typically show heavy tails.)\n\nIts pdf is\n\n$$ p(x) = \\alpha k^{\\alpha} x^{-\\alpha-1}\\;\\;\\; k \\geq x,\\; \\;0 < \\alpha\n\\leq 2. $$\n\nIt takes on values in the range $[k, \\infty]$. \n\n::: {#5afdb455 .cell execution_count=21}\n``` {.python .cell-code}\nfrom scipy.stats import pareto\nalpha = 1.3\nx = np.linspace(pareto.ppf(0.005, alpha), pareto.ppf(0.995, alpha), 100)\nplt.plot(x, pareto.pdf(x, alpha),'b-', lw = 5, alpha = 0.6, label='pareto pdf')\nplt.title(r'Pareto PDF.  $\\alpha$ = {}'.format(alpha), size=14)\nplt.xlabel(r'x', size=14)\nplt.ylabel(r'p(x)', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-21-output-1.png){width=594 height=457 fig-align='center'}\n:::\n:::\n\n\nThe variance of the Pareto distribution is infinite.  (The corresponding\nintegral diverges.)\n\nIn practice, this means that a new observation that significantly\nchanges the sample variance is _always_ possible, no matter how many samples\nof the random variable have already been taken.\n\nThe mean of the Pareto is $\\frac{k\\alpha}{\\alpha-1}$, for $\\alpha > 1$.  \n\nBut note that as $\\alpha$ decreases, the variability of the Pareto increases.  \n\nIn fact, for $\\alpha \\leq 1$, the Pareto distribution has _infinite mean._  Again, in practice this means that a swamping observation for the mean is always possible.   \n\nHence the running average of a series of Pareto observations with $\\alpha \\leq 1$ will never converge to a fixed value, and the mean itself is not a useful statistic in this case.\n\n### The Multivariate Gaussian\n\nThe most common multivariate distribution we will work with is the multivariate Gaussian.\n\nThe multivariate normal distribution of a random vector $\\mathbf{X} = (X_1, \\dots, X_k)^T$ is denoted:\n\n$$\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\Sigma)$$\n\nwhere $\\mathbf{\\mu} = E[\\mathbf{X}] = (E[X_1], \\dots, E[X_k])^T$\n\nand $\\Sigma$ is the $k \\times k$ covariance matrix where $\\Sigma_{i,j} = \\text{Cov}(X_i, X_j)$.\n\nHere are some examples.\n\nWe'll consider two-component random vectors:\n\n$$\\mathbf{X} = \\begin{bmatrix}X_1\\\\X_2\\end{bmatrix}.$$\n\nAnd our first example will be a simple one\n\n$$ \\mathbf{\\mu} = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\;\\;\\;\\Sigma = \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix}.$$\n\nWe see that the variance (and standard deviation) of each component is 1.\n\nHowever the covariances are zero -- the components are uncorrelated.\n\nWe will take 600 samples from this distribution.\n\n::: {#6d1b29c3 .cell execution_count=22}\n``` {.python .cell-code}\nfrom scipy.stats import multivariate_normal\nnp.random.seed(4)\ndf1 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]),\n                                           cov = np.eye(2), \n                                           size = 600), \n                                           columns = ['X1', 'X2'])\n```\n:::\n\n\n::: {#a2eb4ea9 .cell execution_count=23}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-23-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#b8eb5e1c .cell execution_count=24}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-24-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#34b9c47c .cell execution_count=25}\n``` {.python .cell-code}\nnp.random.seed(4)\ndf1 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), \n                                           cov = np.array([[1, 0.8],[0.8, 1]]), \n                                           size = 600),\n                                           columns = ['X1', 'X2'])\n```\n:::\n\n\nNext, we look at the case\n\n$$\\mathbf{\\mu} = \n\\begin{bmatrix}\n1 \\\\ \n1\n\\end{bmatrix}\n\\qquad\n\\Sigma = \n\\begin{bmatrix}\n1 & 0.8 \\\\\n0.8 & 1\n\\end{bmatrix}.$$\n\nNotice that $\\text{Cov}(X_1, X_2) = 0.8$.   \n\nWe say that the components are __positively correlated.__\n\nNonetheless, __the marginals are still Gaussian.__\n\n::: {#88caf9b4 .cell execution_count=26}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-26-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#ea103b02 .cell execution_count=27}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-27-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#f4305a48 .cell execution_count=28}\n``` {.python .cell-code}\nnp.random.seed(4)\ndf1 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), \n                                           cov = np.array([[1, -0.8],[-0.8, 1]]), \n                                           size = 600), \n                                           columns = ['X1', 'X2'])\n```\n:::\n\n\nNext, we look at the case:\n\n$$\\mathbf{\\mu} = \n\\begin{bmatrix} \n1 \\\\\n1\n\\end{bmatrix}\n\\qquad\n\\Sigma = \n\\begin{bmatrix} \n1 & -0.8 \\\\\n-0.8 & 1\n\\end{bmatrix}.$$\n\nNotice that $\\text{Cov}(X_1, X_2) = -0.8$.   We say that the components are __negatively correlated__ or __anticorrelated.__\n\n::: {#df897049 .cell execution_count=29}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-29-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#cc5387a4 .cell execution_count=30}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-30-output-1.png){width=469 height=470 fig-align='center'}\n:::\n:::\n\n\nFinally, let's look at our stock data:\n\n::: {#bb518fa9 .cell execution_count=31}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df, x = 'TSLA', y = 'NVDA', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(df.mean()['TSLA'], df.mean()['NVDA'], 'ro', markersize = 6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-31-output-1.png){width=466 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {#669c6d4c .cell execution_count=32}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df, x = 'TSLA', y = 'NVDA', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(df.mean()['TSLA'], df.mean()['NVDA'], 'ro', markersize = 6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-32-output-1.png){width=466 height=470 fig-align='center'}\n:::\n:::\n\n\nRecall that the correlation between these two stocks was about 0.86.\n\nThat is, the stocks are positively correlated.\n\n## Confidence Intervals\n\nSay you are concerned with some data that we take as coming from a random process.\n\nYou want to characterize it as accurately as possible. You measure it, yielding a\nsingle value.  \n\nHow much does that value tell you? Can you rely on it as a description of the random process?\n\nLet's say you have a dataset and you compute its average value.   \n\nHow certain are you that the average would be the same if you took another dataset from the same source (i.e., the same random process)?\n\nWe think of the hypothetical data source as a random variable with a true mean $\\mu$.\n\n(Note that we are using frequentist style thinking here.)\n\nWe would like to find a range within which we are 90% sure that the\ntrue mean $\\mu$ lies.  \n\nIn other words, we want the probability that the true mean lies in\nthe interval to be 0.9.  \n\nThis interval is then called the 90% confidence interval.\n\nTo be more precise:  A confidence interval at level $\\gamma$ for a fixed\nbut unknown parameter $m$ is an interval $(a,b)$ such that\n\n$$ P(A < m < B) \\geq \\gamma. $$\n\nNote that $m$ is fixed --- it is not random.  \n\nWhat is random is the\ninterval $(A, B)$.  \n\nThis interval is constructed based on the\ndata, which (by assumption) are random.\n\n![](figs/confidence-intervals.png){fig-align=\"center\"}\n\n## Confidence Intervals for the Mean\n\nImagine we have a set of $n$ samples of a random variable,\n$x_1, x_2, ..., x_n$ Let's assume that the random variable has mean\n$\\mu$ and variance $\\sigma^2$.\n\nAn estimate of $\\mu$ is the empirical average of the samples, $\\bar{x}$.   \n\nNow, the Central Limit Theorem tells us that the sum of a\nlarge number $n$ of random variables, each with mean $\\mu$ and variance\n$\\sigma^2$, yields a Gaussian random variable with mean\n$n\\mu$ and variance $n \\sigma^2$.   \n\nSo the distribution of the average of $n$ samples\nis normal with mean $\\mu$ and variance $\\sigma^2 / n$.  That\nis, \n\n$$ \\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma/\\sqrt{n}) $$\n\nWe usually assume that the number of samples should be 30 or more for\nthe CLT to hold.   \n\nWhile the specific value 30 is a bit arbitrary, we will usually be\nusing very large samples (datasets) in this course for which this assumption is \nvalid.\n\nThe standard deviation of the sample mean is called the standard error.\n\nNotice that the standard error decreases as we increase the sample size, according to $1/\\sqrt{n}.$\n\nSo it will turn out that using $\\bar{x}$, we can get\nincreasingly \"tight\" estimates of $\\mu$ as we increase the number of\nsamples $n$.\n\nNow, remember that the true mean $\\mu$ is a constant, while the\nempirical mean $\\bar{x}$ is a random variable.   \n\nLet us assume for a moment that we know the true $\\mu$ and $\\sigma$, and that we accept that\n$\\bar{x}$ has a $N(\\mu, \\sigma/\\sqrt{n})$ distribution.   \n\nThen it is true that \n\n$$ P(\\mu-k\\sigma/\\sqrt{n} < \\bar{x} < \\mu+k\\sigma/\\sqrt{n}) = P(-k < S <\nk)$$\n\nwhere $S$ is the standard Gaussian random variable (having distribution $N(0,1)$).\n\nWe write $z_{1-\\alpha/2}$ to be the $1-\\alpha/2$ quantile of the\nunit normal.  That is, \n\n$$ P(-z_{1-\\alpha/2} < S < z_{1-\\alpha/2}) = 1-\\alpha.$$\n\nSo to form a 90% probability interval for $S$ (centered on\nzero) we choose $k = z_{0.95}$.  \n\n::: {#d2464583 .cell execution_count=33}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nx90 = np.linspace(norm.ppf(0.05), norm.ppf(0.95), 100)\nplt.plot(x, norm.pdf(x),'b-')\nplt.fill_between(x90, 0, norm.pdf(x90))\nplt.title(r'90% region for Standard Gaussian', size = 14)\nplt.xlabel('x', size = 14)\nplt.ylabel(r'$p(x)$', size = 14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-33-output-1.png){width=605 height=457 fig-align='center'}\n:::\n:::\n\n\nTurning back to $\\bar{x}$, the 90% probability interval on $\\bar{x}$ would be:\n\n$$ \\mu-z_{0.95}\\sigma/\\sqrt{n} < \\bar{x} < \\mu+z_{0.95}\\sigma/\\sqrt{n}. $$\n\nThe last step: by a simple argument, we can show that the sample mean is \nin some fixed-size interval centered on the true mean, if and only if the true\nmean is also in a fixed-size interval (of the same size) centered on the sample mean.\n\nThis means that:\n\n\\begin{align*}\n1-\\alpha & = P(\\mu-z_{1-\\alpha/2}\\sigma/\\sqrt{N} < \\bar{x} <\n\\mu+z_{1-\\alpha/2}\\sigma/\\sqrt{N}) \\\\\n& = P(\\bar{x}-k\\sigma\\sqrt{N} < \\mu < \\bar{x}+k\\sigma/\\sqrt{N}).\n\\end{align*}\n\nThis latter expression defines the __$1-\\alpha$ confidence interval\n  for the mean.__\n\nWe are done, except for estimating $\\sigma$.  We do this\ndirectly from the data: $\\hat{\\sigma} = s$\n\nwhere $s$ is the sample standard deviation, that is,\n$s = \\sqrt{1/(n-1) \\sum (x_i - \\bar{x})^2}$.\n\nTo summarize: by the argument presented here, a 100(1-$\\alpha$)%\nconfidence interval for the population mean is given by\n\n$$\\bar{x} \\pm  z_{1-\\alpha/2} \\, \\frac{s}{\\sqrt{n}}. $$\n\nAs an example, a 95% confidence interval for the mean is the sample average plus or minus two standard errors.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Recap\nToday we covered the following topics in probability:\n\n- Frequentist vs Bayesian views\n- Probability rules\n- Conditional probability, Bayes' theorem, and independent events\n- Random variables\n- Expected value and variance\n- Distributions\n  - Bernoulli\n  - Poisson\n  - Gaussian (normal)\n:::\n\n",
    "supporting": [
      "03-Probability-and-Statistics-Refresher_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}