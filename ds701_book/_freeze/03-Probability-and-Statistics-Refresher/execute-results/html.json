{
  "hash": "d5ad9e17cfa7f6f843f94e84781307fd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Probability and Statistics Refresher\njupyter: python3\n---\n\n\nToday we'll review the essentials of probability and statistics that we'll need for this course.\n\n## Probability\n\nWhat is probability?  \n\nWe all have a general sense of what is meant by probability.\n\nHowever, when we look closely, we see that there are at least two different ways to interpret the notion of \"the probability of an event.\"\n\nThe first view is summarized by this quote from the first pages of a probability textbook:\n\n>Suppose an experiment under consideration can be repeated any number of times, so that, in principle at least, we can produce a whole series of “independent trials under identical conditions” in each of which, depending on chance, a particular event $A$ of interest either occurs or does not occur. \n\n>Let $n$ be the total number of experiments in the whole series of trials, and let $n(A)$ be the number of experiement in which $A$ occurs. Then the ratio $n(A)/n$ is called the relative frequency of the event $A.$ \n\n> It turns out that the relative frequencies observed in different series of trials are virtually the same for large $n,$ clustering about some constant $P[A],$ which is called the **probability of the event $A.$**\n\nY. A. Rozanov. Probability Theory: A Concise Course. Dover Publications, Inc., 1969.\n\nThis is called the **frequentist** intepretation of probability.\n\nYou can think of it as treating each event as a sort of idealized coin-flip.\n\nThe key idea in the above definition is to be able to:\n\n> produce a whole series of “independent trials under identical conditions” \n\nHowever, we often use probability in another setting.\n\nOn a visit to the doctor, we may ask, \"What is the probability that I have disease X?\"\n\nOr, before digging a well, we may ask, \"What is the probability that I will strike water?\"\n\nThese questions are __totally incompatible__ with the notion of \"independent trials under identical conditions\"!\n\nRather, it seems that we are really asking:\n>\"How certain should I be that I have disease X?\" \n\n> \"How certain should I be that I will strike water?\"\n\nIn this setting, we are using probability to encode \"degree of belief\" or a \"state of knowledge.\"\n\nThis is called the __Bayesian__ interpretation of probability.\n\n> ... the probability emerging in the time of Pascal is essentially dual.\n> It has to do both with stable frequencies and with degrees of belief.\n> It is, as I shall put it, both aleatory and epistemological.\n> \n> The Emergence of Probability, Ian Hacking, 2006\n\nSomewhat amazingly, it turns out that whichever way we think of probability (frequentist or Bayesian),\n\n... the __rules__ that we use for computing probabilities should be __exactly the same.__\n\nIn most cases in this course we will use frequentist language and examples when talking about probability.\n\nBut in point of fact, it's often really a \"state of knowledge\" that we are really talking about when we use probability models in this course.\n\n> A thing appears random only through the incompleteness of our knowledge \n\n-- Spinoza\n\nIn other words, we use probability as an abstraction that __hides details__ we don't want to deal with.  \n\n(This is a time-honored use of probability.)\n\nSo it's important to recognize that both frequentist and Bayesian views are __valid__ and __useful__ views of probability.\n\n<center>\n\n<img src=\"./figs/Bayes-Frequentism.png\" width=\"500px\">\n\n</center>\n\n\nSo, now, let's talk about rules for computing probabilities.\n\n>Any simple idea is approximate; as an illustration, consider an object ... what is an object? Philosophers are always \n>saying, “Well, just take a chair for example.” The moment they say that, you know that they do not know what they are \n>talking about any more. What is a chair? ... every object is a mixture of a lot of things, so we can deal with it \n> only as a series of approximations and idealizations.\n\n>The trick is the idealizations.\n\nRichard Feynman, _The Feynman Lectures on Physics, 12-2_\n\nHere is an illustration of this principle applied to probability:\n\n>In a serious work ... an expression such as “this phenomenon is due to chance” constitutes simply, \n>an elliptic form of speech. ... It really means “everything occurs as if this phenomenon were due to chance,” \n>or, to be more precise: “To describe, or interpret or formalize this phenomenon, \n>only probabilistic models have so far given good results.”\n\nGeorges Matheron, _Estimating and Choosing: An Essay on Probability in Practice_\n\n### Probability and Conditioning\n\n__Definition.__  Consider a set $\\Omega$, referred to as the\n_sample space._  A _probability\n  measure_ on $\\Omega$ is a function $P[\\cdot]$ defined on all the subsets of $\\Omega$ (the\n  _events_) such that:\n  \n1. $P[\\Omega] = 1$\n2. For any event $A \\subset \\Omega$, $P[A] \\geq 0.$\n3. For any events $A, B \\subset \\Omega$ where $A \\cap B =\n    \\emptyset$, $P[A \\cup B] = P[A] + P[B]$.\n\n\nOften we want to ask how a probability measure changes if we restrict the sample space to be some subset of $\\Omega$.  \n\nThis is called __conditioning.__\n\n__Here and throughout we need examples.__\n\n__Definition.__ The _conditional probability_ of an event $A$ given that\nevent $B$ (having positive probability) is known to occur, is \n\n$$ P[A|B] = \\frac{P[A \\cap B]}{P[B]}  \\mbox{ where } P[B] > 0 $$\n\nThe function $P[\\cdot|B]$ is a probability measure over the sample space\n$B$.  \n\nNote that in the expression $P[A|B]$, $A$ is random but $B$ is fixed. \n\nNow if $B$ is a proper subset of $\\Omega,$ then $P[B] < 1$.   So $P[\\cdot|B]$ is a rescaling of the quantity $P[A\\cap B]$ so that $P[B|B] = 1.$ \n\nThe sample space $\\Omega$ may be continuous or discrete, and bounded or unbounded.\n\n__Independent Events.__\n\n__Definition.__ Two events $A$ and $B$ are __independent__ if $P[A\\cap B] = P[A] \\cdot P[B].$\n\nThis is exactly the same as saying that $P[A|B] = P[A].$  \n\nSo we can see that the intuitive notion of independence is that \"if one event occurs, that does not change the probability of the other event.\"\n\n## Random Variables\n\nWe are usually interested in numeric values associated with events.  \n\nWhen a random event has a numeric value we refer to it as a random variable.\n\nNotationally, we use CAPITAL LETTERS for random variables and lowercase for non-random quantities.\n\nTo collect information about what values of the random variable are more probable than others, we have some more definitions.\n\n__Definition.__ The cumulative distribution function (CDF) F for a random\n variable $X$ is equal to the probability measure for the event that\n consists of all possible outcomes with a value of the random variable $X$\n less than or equal to $x$, that is, $F(x) = P[X \\leq x].$\n\n__Example.__  Consider the roll of a single die.  The random variable here is the number of points showing.  What is the CDF?\n\n<img src=\"./figs/single-die.png\" alt=\"Die\" width=\"100px\">\n\n::: {#5b8866f2 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=2}\n``` {.python .cell-code}\nplt.figure()\nfor i in range(7):\n    plt.plot([i, i+1-.08], [i/6, i/6],'-b')\nfor i in range(1, 7):\n    plt.plot(i, i/6, 'ob')\n    plt.plot(i, (i-1)/6, 'ob', fillstyle = 'none')\nplt.xlim([0, 7])\nplt.ylim([-0.05, 1.1])\nplt.title('Cumulative Distribution Function (CDF)')\nplt.xlabel(r'$x$ (Number of points showing)', size=14)\nplt.ylabel(r'$P[X\\leq x]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-3-output-1.png){width=599 height=457}\n:::\n:::\n\n\nNow, consider this CDF of some random variable:\n\n::: {#80e26b3e .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.cdf(x),'b-', lw=5, alpha = 0.6)\nplt.title('Cumulative Distribution Function (CDF)')\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$P[X\\leq x]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-4-output-1.png){width=595 height=455}\n:::\n:::\n\n\nWhat does it mean when the slope is steeper for some $x$ values?\n\nThe slope tells us how likely values are in a particular range.\n\nThis is important enough that we define a function to capture it.  \n\n__Definition.__ The _probability density function_ (pdf) is the derivative of the CDF, when that is defined.\n\n$$ f(x) = \\frac{dF(x)}{dx}.$$\n\nOften we will go the other way as well:\n\n$$ F(x) = \\int_{-\\infty}^{x} f(t)\\; dt $$\n\nYou should be able to see that:\n\n$$ \\int_{-\\infty}^{+\\infty} f(x)\\; dx = 1 $$\n\nand\n\n$$      f(x) \\geq 0 $$\n\nNow, for a discrete random variable, the CDF is not differentiable (because the CDF is a step function).  \n\nFor the PDF of discrete RVs, we simply plot the probability function of\neach value.  That is, we plot $P[X = x]$ for the various values of $x$.\n\nAnother way to think of the PDF is that it consists of impulses at the\npoints of discontinuity of the CDF. \n\nFor our example of the single die:\n\n::: {#6153ba8c .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nplt.figure()\nx = np.arange(1, 7)\nplt.plot(x, 6*[1/6.], 'bo', ms=8)\nplt.vlines(x, 0, 1/6., colors='b', lw=5, alpha=0.5)\nplt.xlim([0.5, 6.5])\nplt.ylim([0, 1.1])\nplt.xlabel(r'x (Number of points showing)', size=14)\nplt.ylabel(r'$P[X = x]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-5-output-1.png){width=595 height=435}\n:::\n:::\n\n\n## Characterizing Random Variables\n\n__Definition.__ The _expected value_ $E[X]$ of a random\nvariable $X$ is the probability-weighted sum or integral of all possible values of\nthe R.V.  \n\nFor a discrete random variable, this is:\n\n$$ E[X] \\equiv \\sum_{-\\infty}^{+\\infty} x\\; P[X=x] $$\n\nand for a continuous random variable with pdf $p():$\n\n$$ E[X] \\equiv \\int_{-\\infty}^{+\\infty} x\\; p(x)\\; dx $$\n\nThe expected value is also called the average or the mean, although we\nprefer to reserve those terms for empirical statistics (actual\nmeasurements, not idealizations like these formulas).\n\nThe expected value is in some sense the \"center of mass\" of the random\nvariable.   It is often denoted $\\mu$.\n\nThe mean is usually a quite useful characterization of the random variable.\n\nHowever, be careful: in some cases, the mean may not be very informative, or important.  \n\nIn some cases a\nrandom variable may not ever take on the mean as a possible value.  (Consider again the single die, whose mean is 3.5)\n\nIn other cases the notion of average isn't useful, as for the person\nwith their head in the oven and feet in the freezer who claims \"on\naverage I feel fine.\" \n\nIn other words, the mean may not be very informative when\nobservations are highly variable.   \n\nIn fact, the variability of random quantities is crucially important to characterize.\n\nFor this we use __variance,__ the mean squared difference of the random variable from its mean.\n\n__Definition.__ The variance of a random variable $X$ is\n\n$$ \\mbox{Var} (X) \\equiv E[(X - E[X])^2]. $$\n\nFor example, given a discrete R.V. with $E[X] = \\mu$ this would be:\n\n$$ \\mbox{Var} (X) = \\sum_{x=-\\infty}^{+\\infty} (x-\\mu)^2\\; P[X=x]. $$\n\nWe use the symbol $\\sigma^2$ to denote variance.\n\nThe units of variance are the square of the units of the mean. \n\nSo to compare variance and mean in the same units, we take the square root of the variance.\n\nThis is called the __standard deviation__ and is denoted $\\sigma$.\n\nNext, let's recall the case of the Tesla and Yelp returns from the last lecture:\n\n::: {#12af114a .cell hide_input='false' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport yfinance as yf\n\nstocks = ['TSLA', 'YELP']\ndf = pd.DataFrame()\nfor s in stocks:\n    df[s] = pd.DataFrame(yf.download(s,start='2014-01-01',end='2014-12-31', progress = False))['Close']\n\nrets = df.pct_change(30)\nrets[['TSLA', 'YELP']].plot(lw=2)\nplt.legend(loc='best');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-6-output-1.png){width=590 height=412}\n:::\n:::\n\n\nTreating these two timeseries as random variables, we are interested in how they vary __together__.\n\nThis is captured by the concept of __covariance.__\n\n__Definition.__ For two random variables $X$ and $Y$, their _covariance_ is defined as:\n\n$$\\text{Cov}(X,Y) = E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]$$\n\nIf covariance is positive, this tells us that $X$ and $Y$ tend to be both above their means together, and both below their means together.\n\nWe will often denote $\\text{Cov}(X,Y)$ as $\\sigma_{XY}$\n\nIf we are interested in asking \"how similar\" are two random variables, we want to normalize covariance by the amount of variance shown by the random variables.\n\nThe tool for this purpose is __correlation__, ie, normalized covariance:\n\n$$\\rho(X,Y) = \\frac{E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]}{\\sigma_X \\sigma_Y}$$\n\n$\\rho(X, Y)$ takes on values between -1 and 1.   \n\nIf $\\rho(X, Y) = 0$ then $X$ and $Y$ are __uncorrelated__.   Note that this is not the same thing as being independent!\n\n$\\rho$ is sometimes called \"Pearson $r$\" after Karl Pearson who popularized it.\n\nLet's look at our example again:\n\n::: {#1c4aefc3 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nrets = df.pct_change(30)\nrets[['TSLA', 'YELP']].plot(lw=2)\nplt.legend(loc='best');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-7-output-1.png){width=590 height=412}\n:::\n:::\n\n\n::: {#dfba10ce .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=7}\n``` {.python .cell-code}\ndf.cov()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>YELP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>TSLA</th>\n      <td>3.830274</td>\n      <td>3.187969</td>\n    </tr>\n    <tr>\n      <th>YELP</th>\n      <td>3.187969</td>\n      <td>139.851859</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn the case of Tesla ($X$) and Yelp ($Y$) above, we find that\n\n$$\\text{Cov}(X,Y) = 9.53$$\n\nIs that a big number? small number?\n\n::: {#48b8297d .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=8}\n``` {.python .cell-code}\ndf.corr()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>YELP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>TSLA</th>\n      <td>1.000000</td>\n      <td>0.137742</td>\n    </tr>\n    <tr>\n      <th>YELP</th>\n      <td>0.137742</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSo:\n\n$$\\rho(X,Y) = 0.137$$\n\nApparently, not a big number!\n\n## Low and High Variability\n\nHistorically, most sources of random variation that have concerned\nstatisticians are instances of low variability.  \n\nThe original roots of\nprobability in the study of games of chance, and later in the study of\nbiology and medicine, have mainly studied objects with low variability.\n\n(Note that by \"low variability\" I don't mean that such variability is unimportant.)\n\nSome examples of random variation in this category are: \n\n* the heights of adult humans\n* the number of trees per unit area in a mature forest\n* the sum of 10 rolls of a die\n* the time between emission of subatomic particles from a radioactive material.\n\nIn each of these cases, there are a range of values that are\n \"typical,\" and there is a clear threshold above what is typical, that essentially never occurs.\n\nOn the other hand, there are some situations in which variability is\nquite different.  \n\nIn these cases, there is no real \"typical\" range of\nvalues, and arbitrarily large values can occur with non-negligible\nfrequency.   \n\nSome examples in this category are\n\n* the distribution of wealth among individuals in society\n* the sizes of human settlements\n* the areas burnt in forest fires\n* the runs of gains and losses in various financial markets over time\n* and the number of collaborators a scholar has over her lifetime.   \n\n> The banking system (betting against rare events) just lost [more than] 1\n> Trillion dollars (so far) on a single error, more than was ever earned\n> in the history of banking.\n\nNassim Nicholas Taleb, September 2008 \n\n\n\n\n![](figs/derivatives-portfolio-variation.png)\n\nAn example of a run of observations showing high variability.   This figure shows the daily\nvariations in a derivatives portfolio over the timeframe 1988-2008.\nAbout 99% of the variation over the 20 years occurs in a single day\n(the day the European Monetary System collapsed).\n\n## Important Random Variables\n\nNow we will review certain random variables that come up over and over again in typical situations.\n\n### Independent Binary Trials\n\nHere is our canonical experiment: flipping a weighted coin.  \n\nThe coin\ncomes up \"heads\" (aka \"success\") with probability $p$.  \n\nWe use the standard notation that the corresponding probability (\"tails\", \"failure\") is denoted $q$\n(i.e., $q = 1-p$).  \n\nThese are called _Bernoulli trials_.\n\nOne can think of each trial as a timestep so these are about _discrete\n  time_.\n  \nNotice that by definition, Bernoulli trials are __independent events.__\n\nNow we will extend this notion to continuous time.\n\nTo start with, imagine that you flip the coin once per second.\n\nSo we expect to observe $p$ successes per second.\n\nNow, imagine that you \"speed up\" the coin flipping so that instead of flipping a coin once per second,\nyou flip it $m > 1$ times per second, \n\n... and you simultaneously decrease the\nprobability of success to $p/m$.   \n\nThen you expect the same number of\nsuccesses per second (i.e., $p$) ... but events can happen at finer time\nintervals.   \n\nNow imagine the limit as $m \\rightarrow \\infty$.  \n\nThis is a\nmathematical abstraction in which \n* an event can happen at _any_ time instant\n* an event at any time instant is equally likely\n* and an event at any time instant is independent of any other time instant (it's still a coin with no memory).   \n\nIn this setting, we can think of events happening at some _rate_ $\\lambda$ that is\nequal to $p$ per second.  \n\nNote that $\\lambda$ has units of inverse time, e.g., sec$^{-1}$. \n\nThus we have defined two kinds of coin-flipping:\n* in discrete time (Bernoulli trials with success probability $p$)\n* and in continuous time (with success rate $\\lambda$)\n\nFor each of these two cases (discrete and continuous time) there are two questions we can ask:\n\n1. Given that an event has just occured, how long until the next\n  success?\n2. In a fixed number of trials or amount of time, how many events\n  occur?\n\nThese four cases define four commonly-used random variables.\n\n\n\n\n| | # Trials or Time Until Event | Number of Events in Fixed Time  |\n|-|:--:|:--:|\n|__Discrete Trials__| Geometric| Binomial |\n|__Continuous Rate__| Exponential | Poisson|\n\nEach one has an associated distribution.\n\n#### The Geometric Distribution\n\nThe geometric distribution concerns Bernoulli trials.\n\nIt answers the question: \"what is the probability it takes $k$ trials to obtain the first success?\"\n\nIts PDF is given by:\n\n$$ P[X = k] = p(1-p)^{k-1}. \\;\\; k \\geq 1$$ \n\nIts mean is $\\mu = \\frac{1}{p}$ and its variance is $\\sigma^2 = \\frac{1-p}{p^2}$.\n\nIts pdf looks like:\n\n::: {#73e6756b .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nfrom scipy.stats import geom\np = 0.3\nx = np.arange(geom.ppf(0.01, p), geom.ppf(0.995, p))\nplt.ylim([0, 0.4])\nplt.xlim([0.5, max(x)])\nplt.plot(x, geom.pmf(x, p), 'bo', ms=8, label = 'geom pmf')\nplt.vlines(x, 0, geom.pmf(x, p), colors='b', lw = 5, alpha = 0.6)\nplt.title(f'Geometric PDF, $p$ = {p}', size=14)\nplt.xlabel(r'$k$', size=14)\nplt.ylabel(r'$P[X = k]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-10-output-1.png){width=612 height=458}\n:::\n:::\n\n\n#### The Binomial Distribution\n\nThe Binomial also concerns Bernoulli trials.\n\nIn this experiment there are precisely $N$ coin flips, and $p$ is still\nthe probability of a success.  \n\nNow we ask: what is the probability there\nwill be $k$ successes?\n\nFor any __given__ sequence of $k$ successes and $N-k$ failures, the probability is $p^k \\;(1-p)^{N-k}$.\n\nBut there are many different such sequences: $\\binom{N}{k}$ of them in fact. \n\nSo this distribution is $P[X=k] = \\binom{N}{k}\\; p^k\\; (1-p)^{N-k}.$\n\nIts mean is $pN$, and its variance is $pqN$.  \n\n::: {#e6ebc662 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nfrom scipy.stats import binom\np = 0.3\nx = np.arange(binom.ppf(0.01, 10, p), binom.ppf(0.9995, 10, p))\nplt.ylim([0, 0.4])\nplt.xlim([-0.5, max(x)+0.5])\nplt.plot(x, binom.pmf(x, 10, p), 'bo', ms=8, label = 'binom pmf')\nplt.vlines(x, 0, binom.pmf(x, 10, p), colors='b', lw = 5, alpha=0.6)\nplt.title(f'Binomial PDF, $p$ = {p}, $N$ = 10', size=14)\nplt.xlabel(r'$k$', size=14)\nplt.ylabel(r'$P[X = k]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-11-output-1.png){width=603 height=458}\n:::\n:::\n\n\n#### The Exponential Distribution\n\nNow we shift to a situation where successes occur at a fixed __rate.__\n\nThe Exponential random variable is the analog of the geometric in the continuous case,\n_i.e.,_ the situation in which a success happens at some rate\n$\\lambda$.  \n\nThis RV can be thought of as measuring the __time__ until a success occurs. \n\nIts pdf is:\n\n$$ p(x) = \\lambda e^{- \\lambda x}$$\n\nThe mean is $1/\\lambda$, and the variance is $1/\\lambda^2$.\n\n::: {#d22045ad .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nfrom scipy.stats import expon\np = 0.3\nx = np.linspace(expon.ppf(0.01, scale=1/p), expon.ppf(0.995, scale=1/p), 100)\nplt.plot(x, expon.pdf(x, scale=1/p),'b-', lw = 5, alpha = 0.6, label='expon pdf')\nplt.title(f'Exponential PDF, $\\lambda$ = {p}', size=14)\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$p(x)$', size=14)\nplt.ylim([0, 0.4])\nplt.xlim([0, 14]);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:5: SyntaxWarning: invalid escape sequence '\\l'\n<>:5: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_17121/1767178432.py:5: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f'Exponential PDF, $\\lambda$ = {p}', size=14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-12-output-2.png){width=613 height=458}\n:::\n:::\n\n\nNotice how the Exponential is the __continuous analog__ of the Geometric:\n\n::: {#1f2bb0b8 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\nfrom math import exp \nlam = 0.3\np = 1 - exp(- lam)\nx = np.linspace(expon.ppf(0, scale=1/lam), expon.ppf(0.995, scale=1/lam), 100)\nplt.plot(x, expon.cdf(x, scale=1/lam),'b-', lw = 5, alpha = 0.6, label='Exponential')\nxg = np.arange(geom.ppf(0, p), geom.ppf(0.995, p))\nplt.ylim([0, 1])\nplt.xlim([-0.5, max(xg)])\nplt.plot(xg, geom.cdf(xg, p), 'ro', ms = 8, label = 'Geometric')\nplt.suptitle(f'Geometric and Exponential CDFs', size = 14)\nplt.title(r'$\\lambda = 0.3; \\;\\;\\; p = 1-e^{-\\lambda} =$' + f' {p:0.3f}', size=12)\nplt.xlabel(r'$x$', size=14)\nplt.ylabel(r'$P[X \\leq x]$', size = 14)\nplt.legend(loc = 'best');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-13-output-1.png){width=595 height=483}\n:::\n:::\n\n\n#### The Poisson Distribution\n\nWhen we ask the question \n\n\"How many successes occur in a fixed amount of time?\", \n\n...we get the Poisson distribution.\n\nThe Poisson Distribution is the limiting form of binomial, when the\nnumber of trials goes to infinity, happening at some rate $\\lambda$.\n\n$$ P[k \\mbox{ successes in time } T] = (\\lambda T)^k \\frac{e^{- \\lambda T}}{k!}$$\n\nIt answers the question: when events happen indepdently at some fixed rate, how many will occur in a given fixed interval?\n\nIts mean is $\\lambda T$ and its variance is $\\lambda T$ as well.\n\n::: {#c0d637e7 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats import poisson\nmu = 3\nx = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.9995, mu))\n# plt.ylim([0,1])\nplt.xlim([-0.5, max(x)+0.5])\nplt.ylim(ymax = 0.4)\nplt.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\nplt.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.6)\nplt.title(f'Poisson PDF.  $\\lambda T$ = {mu}', size=14)\nplt.xlabel(r'k', size=14)\nplt.ylabel(r'P[X = k]', size=14);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:9: SyntaxWarning: invalid escape sequence '\\l'\n<>:9: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_17121/3423589416.py:9: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title(f'Poisson PDF.  $\\lambda T$ = {mu}', size=14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-14-output-2.png){width=603 height=457}\n:::\n:::\n\n\nThe Poisson distribution has an interesting role in our perception of randomness (which you can read more about [here](http://www.empiricalzeal.com/2012/12/21/what-does-randomness-look-like/#more-2450)).    \n\nThe classic example comes from history.  From the above site:\n\n>In 1898 Ladislaus Bortkiewicz, a Russian statistician of Polish descent, was trying to understand why, in some years, \n>an unusually large number of soldiers in the Prussian army were dying due to horse-kicks. In a single army corp, \n>there were sometimes 4 such deaths in a single year. Was this just coincidence?\n\nTo assess whether horse-kicks were random (not following any pattern) Bortkiewicz simply compared the number per year to what would be predicted by the Poisson distribution.\n\n::: {#e6d4e8fa .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\n# note that this data is available in 'data/HorseKicks.txt'\nhorse_kicks = pd.DataFrame(\ndata = np.array([\n[0, 108.67, 109],\n[1, 66.29, 65],\n[2, 20.22, 22],\n[3, 4.11, 3],\n[4, 0.63, 1],\n[5, 0.08, 0],\n[6, 0.01, 0]]),\ncolumns = [\"Number of Deaths Per Year\",\"Predicted Instances (Poisson)\",\"Observed Instances\"])\nhorse_kicks\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Number of Deaths Per Year</th>\n      <th>Predicted Instances (Poisson)</th>\n      <th>Observed Instances</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>108.67</td>\n      <td>109.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>66.29</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>20.22</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>4.11</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>0.63</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.0</td>\n      <td>0.08</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.0</td>\n      <td>0.01</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#ae816a49 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nhorse_kicks[[\"Predicted Instances (Poisson)\",\"Observed Instances\"]].plot.bar()\nplt.xlabel(\"Number of Deaths Per Year\", size=14)\nplt.ylabel(\"Count\", size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-16-output-1.png){width=598 height=430}\n:::\n:::\n\n\nThe message here is that when events occur at random, we actually tend to perceive them as clustered.   \n\nHere is another example:\n\n![](figs/pinker-glow-worms-and-stars-plot.jpg)\n\nWhich of these was generated by a random process ocurring equally likely everywhere?\n\nThese images are from Steven Pinker’s book, _The Better Angels of our Nature._\n\nIn the left figure, the number of dots falling into regions of a given size follows the Poisson distribution.\n\n#### The Uniform Distribution\n\nThe uniform distribution models the case in which all outcomes are equally probable.  \n\nIt can be a discrete or continuous distribution.\n\nWe have already seen the uniform distribution in the case of rolls of a fair die:\n\n::: {#ffb61670 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nplt.figure()\nx = np.arange(1, 7)\nplt.plot(x, 6*[1/6.], 'bo', ms=8)\nplt.vlines(x, 0, 1/6., colors='b', lw=5, alpha=0.5)\nplt.xlim([0.5, 6.5])\nplt.ylim([0, 1.1])\nplt.xlabel(r'x (Number of points showing)', size=14)\nplt.ylabel(r'$P[X = x]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-17-output-1.png){width=595 height=435}\n:::\n:::\n\n\nThere is an important relationship between the uniform and Poisson distributions.\n\n__When the time an event occurs is uniformly distributed, the number of events in a time interval is Poisson distributed.__\n\nYou can replace \"time\" with \"location\", and so on.\n\nAlso, the reverse statment is true as well.\n\nSo a simple way to generate a picture like the scattered points above is to select the $x$ and $y$ coordinates of each point uniformly distributed over the picture size.\n\n### The Gaussian Distribution\n\nThe Gaussian Distribution is also called the Normal Distribution.\n\nWe will make __extensive__ use of Gaussian distribution, for a number of reasons.\n\n<center>\n\n<img src=\"./figs/Gauss-on-Deutschemark.png\" width=\"600px\">\n\n</center>\n\nOne of reasons we will use it so much is that it is a good guess for how errors are distributed in data.\n\nThis comes from the celebrated __Central Limit Theorem__.   Informally, \n\n>_The sum of a large number of independent observations from any\n>distribution with finite variance tends to have a Gaussian\n> distribution._\n\n<center>\n\n<img src=\"./figs/Galton-Bean-Machine.png\" width=\"350px\">\n\n     Francis Galton's \"Bean Machine\"\n    \n</center>\n\nAs a special case, the sum of $n$ independent Gaussian variates is Gaussian.\n\n\nThus Gaussian processes remain Gaussian after passing through linear\nsystems.   \n\nIf $X_1$ and $X_2$ are Gaussian, then $X_3 = aX_1 + bX_2$ is Gaussian.\n\nThus we can see that one way of thinking of the Gaussian is that it is the\nlimit of the Binomial when $n$ is large, that is, the limit of\nthe sum of many Bernoulli trials.    \n\nHowever many other\nsums of random variables (not just Bernoulli trials) converge to the\nGaussian as well.\n\nThe _standard Gaussian_ distribution has mean zero and a variance (and standard deviation)\nof 1.   The pdf of the standard Gaussian is:\n\n$$ p(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2}. $$\n\n::: {#bb29d599 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=17}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.pdf(x),'b-', lw = 5, alpha = 0.6)\nplt.title(r'Standard Gaussian PDF.  $\\mu = 0, \\sigma = 1$', size=14)\nplt.xlabel('x', size=14)\nplt.ylabel(r'$p(x)$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-18-output-1.png){width=605 height=458}\n:::\n:::\n\n\n::: {#55b14c81 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nplt.plot(x, norm.cdf(x),'b-', lw = 5, alpha = 0.6)\nplt.title(r'Standard Gaussian CDF.  $\\mu = 0, \\sigma = 1$', size=14)\nplt.xlabel('x', size=14)\nplt.ylabel(r'$P[X\\leq x]$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-19-output-1.png){width=595 height=458}\n:::\n:::\n\n\nFor an arbitrary Gaussian distribution with mean $\\mu$ and variance\n$\\sigma^2$, the pdf is simply the standard Gaussian that is relocated to\nhave its center at $\\mu$ and its width scaled by $\\sigma$:\n\n$$ p_{\\mu,\\sigma}(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}. $$\n\n### Heavy Tails\n\nEarlier we discussed high- and low-variability.   \n\nAll of the distributions we have discussed so far have \"light tails\", meaning that they show low variability.\n\nIn other words, extremely large observations are essentially impossible.\n\nHowever in other cases, extremely large observations can occur.   Distributions that capture this property are called \"heavy tailed\".\n\nSome examples of data that can be often modeled using heavy-tailed distributions:\n\n* The sizes of files in a file system\n* The sizes of objects transferred over the Internet\n* The execution time of jobs on a computer system\n* The degree of nodes in a network (eg, social network).\n\nIn practice, random variables that follow heavy tailed distributions\nare characterized as exhibiting many small observations mixed in with\na few large observations.  \n\nIn such datasets, most of the observations\nare small, but most of the contribution to the sample mean or variance\ncomes from the rare, large observations.\n\n#### The Pareto Distribution\n\nThe Pareto distribution is the simplest continuous heavy-tailed\ndistribution.    \n\nPareto was an Italian economist who studied income distributions.\n(In fact, income distributions typically show heavy tails.)\n\nIts pdf is:\n\n$$ p(x) = \\alpha k^{\\alpha} x^{-\\alpha-1}\\;\\;\\; k \\leq x,\\; \\;0 < \\alpha\n\\leq 2. $$\n\nIt takes on values in the range $[k, \\infty]$. \n\n::: {#3c3f1f7d .cell hide_input='false' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=19}\n``` {.python .cell-code}\nfrom scipy.stats import pareto\nalpha = 1.3\nx = np.linspace(pareto.ppf(0.005,alpha), pareto.ppf(0.995,alpha), 100)\nplt.plot(x, pareto.pdf(x,alpha),'b-', lw = 5, alpha = 0.6, label='pareto pdf')\nplt.title(r'Pareto PDF.  $\\alpha$ = {}'.format(alpha), size=14)\nplt.xlabel(r'x', size=14)\nplt.ylabel(r'p(x)', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-20-output-1.png){width=594 height=457}\n:::\n:::\n\n\nThe variance of the Pareto distribution is infinite.  (The corresponding\nintegral diverges.)\n\nIn practice, this means that a new observation that significantly\nchanges the sample variance is _always_ possible, no matter how many samples\nof the random variable have already been taken.\n\nThe mean of the Pareto is $\\frac{k\\alpha}{\\alpha-1}$, for $\\alpha > 1$.  \n\nBut\nnote that as $\\alpha$ decreases, the variability of the Pareto\nincreases.  \n\nIn fact, for $\\alpha \\leq 1$, the Pareto distribution has\n_infinite mean._  Again, in practice this means that a swamping\nobservation for the mean is always possible.   \n\nHence the running average\nof a series of Pareto observations with $\\alpha \\leq 1$ will never\nconverge to a fixed value, and the mean itself is not a useful\nstatistic in this case.\n\n### Multivariate Random Variables\n\nA __multivariate random variable__ is a vector of random variables.\n\nWe often simply say a \"random vector\".\n\nThat is, \n\n$$ \\mathbf{X} = \\left[\\begin{array}{c}X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n\\end{array}\\right] $$\n\nThe expected value of a random vector is obtained by taking the expected value of each component random variable:\n    \n$$ E[\\mathbf{X}] = \\mathbf{\\mu_X} = \\left[\\begin{array}{c}E[X_1] \\\\ E[X_2] \\\\ \\vdots \\\\ E[X_n]\\end{array}\\right] $$\n\nTo properly characterize the variability of a random vector, we need to specify all the covariances of pairs of components.\n\nWe organize these values into a __covariance matrix:__\n\n$$ \\text{Cov}[X] = \\left[\\begin{array}{cccc}\n\\text{Var}[X_1] & \\text{Cov}[X_1, X_2] & \\dots & \\text{Cov}[X_1, X_n]\\\\\n\\text{Cov}[X_2, X_1] & \\text{Var}[X_2] & \\dots & \\text{Cov}[X_2, X_n]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\text{Cov}[X_n,X_1] & \\text{Cov}[X_n, X_2] & \\dots & \\text{Var}[X_n]\\\\\n\\end{array}\\right] $$\n\nA couple things to note about a covariance matrix.\n\n1. The covariance matrix is symmetric (because $\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)$)\n2. The covariance matrix is __positive semidefinite__.\n\n### Random Variables as Vectors\n\nWhen working with data, we will often treat multiple observations of some feature as samples of a random variable.\n\nWe will also typically organize the observations into a vector.\n\nFor example, consider our stock data:\n\n::: {#dc79b985 .cell execution_count=20}\n``` {.python .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TSLA</th>\n      <th>YELP</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-01-02</th>\n      <td>10.006667</td>\n      <td>67.919998</td>\n    </tr>\n    <tr>\n      <th>2014-01-03</th>\n      <td>9.970667</td>\n      <td>67.660004</td>\n    </tr>\n    <tr>\n      <th>2014-01-06</th>\n      <td>9.800000</td>\n      <td>71.720001</td>\n    </tr>\n    <tr>\n      <th>2014-01-07</th>\n      <td>9.957333</td>\n      <td>72.660004</td>\n    </tr>\n    <tr>\n      <th>2014-01-08</th>\n      <td>10.085333</td>\n      <td>78.419998</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014-12-23</th>\n      <td>14.731333</td>\n      <td>53.360001</td>\n    </tr>\n    <tr>\n      <th>2014-12-24</th>\n      <td>14.817333</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>2014-12-26</th>\n      <td>15.188000</td>\n      <td>52.939999</td>\n    </tr>\n    <tr>\n      <th>2014-12-29</th>\n      <td>15.047333</td>\n      <td>53.009998</td>\n    </tr>\n    <tr>\n      <th>2014-12-30</th>\n      <td>14.815333</td>\n      <td>54.240002</td>\n    </tr>\n  </tbody>\n</table>\n<p>251 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nEach column can be treated as a vector.\n\nSo:\n* Let's say that our data frame `df` is represented as a matrix $D$, \n* and that `df['TSLA']` are observations of some random variable $X$, \n* and `df['YELP']` are observations of some random variable $Y$.  \n\nLet $D$ have $n$ rows (ie, $n$ observations).\n\nNow, let us subtract from each column its mean, to form a new matrix $\\tilde{D}$.\n\nIn the new matrix $\\tilde{D}$, every column has zero mean.\n\nThen notice the following: the Covariance matrix of $(X, Y)$ is simply $\\frac{1}{n}\\; \\tilde{D}^T\\tilde{D}$.\n\nFor example, \n\n$$ \\text{Cov}(X,Y) = E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right] $$ \n\n$$ = \\frac{1}{n} \\sum_i (\\tilde{D}_{i1} \\cdot \\tilde{D}_{i2}) $$\n\n$$ = \\frac{1}{n}\\;\\tilde{d}_1^T\\tilde{d}_2$$\n\nwhere $\\tilde{d}_1$ and $\\tilde{d}_2$ are the columns of $\\tilde{D}$.\n\nThis shows that __covariance is actually an inner product__ between normalized observation vectors.\n\n### The Multivariate Gaussian\n\nThe most common multivariate distribution we will work with is the multivariate Gaussian.\n\n\n\nThe multivariate normal distribution of a random vector $\\mathbf{X} = (X_1, \\dots, X_k)^T$ is denoted:\n\n$$\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\Sigma)$$\n\nwhere $\\mathbf{\\mu} = E[\\mathbf{X}] = (E[X_1], \\dots, E[X_k])^T$\n\nand $\\Sigma$ is the $k \\times k$ covariance matrix where $\\Sigma_{i,j} = \\text{Cov}(X_i, X_j)$.\n\nHere are some examples.\n\nWe'll consider two-component random vectors:\n\n$$ \\mathbf{X} = \\begin{bmatrix}X_1\\\\X_2\\end{bmatrix} $$\n\nAnd our first example will be a simple one:\n\n$$ \\mathbf{\\mu} = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\;\\;\\;\\Sigma = \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix} $$\n\nWe see that the variance (and standard deviation) of each component is 1.\n\nHowever the covariances are zero -- the components are uncorrelated.\n\nWe will take 600 samples from this distribution.\n\n\n::: {#60c99f12 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=22}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-23-output-1.png){width=469 height=470}\n:::\n:::\n\n\n::: {#c6d1ceba .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=23}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-24-output-1.png){width=469 height=470}\n:::\n:::\n\n\n\nNext, we look at the case:\n\n$$ \\mathbf{\\mu} = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\;\\;\\;\\Sigma = \\begin{bmatrix}1 & 0.8\\\\0.8 & 1\\end{bmatrix} $$\n\nNotice that $\\text{Cov}(X_1, X_2) = 0.8$.   \n\nWe say that the components are __positively correlated.__\n\nNonetheless, __the marginals are still Gaussian.__\n\n::: {#6a77b08f .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=25}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-26-output-1.png){width=469 height=470}\n:::\n:::\n\n\n::: {#60c72c95 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=26}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-27-output-1.png){width=469 height=470}\n:::\n:::\n\n\n\nNext, we look at the case:\n\n$$ \\mathbf{\\mu} = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\;\\;\\;\\;\\Sigma = \\begin{bmatrix}1 & -0.8\\\\-0.8 & 1\\end{bmatrix} $$\n\nNotice that $\\text{Cov}(X_1, X_2) = -0.8$.   We say that the components are __negatively correlated__ or __anticorrelated.__\n\n::: {#7e927554 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=28}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-29-output-1.png){width=469 height=470}\n:::\n:::\n\n\n::: {#408f22ca .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=29}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(1, 1, 'ro', markersize = 6)\ng.ax_marg_x.plot(1, 0, 'ro')\ng.ax_marg_y.plot(0, 1, 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-30-output-1.png){width=469 height=470}\n:::\n:::\n\n\nFinally, let's look at our stock data:\n\n::: {#9d997ae4 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=30}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df, x = 'TSLA', y = 'YELP', height = 5)\ng.plot(sns.scatterplot, sns.kdeplot)\ng.ax_joint.plot(df.mean()['TSLA'], df.mean()['YELP'], 'ro', markersize = 6);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-31-output-1.png){width=474 height=470}\n:::\n:::\n\n\n::: {#e9ea1056 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=31}\n``` {.python .cell-code}\ng = sns.JointGrid(data = df, x = 'TSLA', y = 'YELP', height = 5)\ng.plot(sns.kdeplot, sns.kdeplot)\ng.ax_joint.plot(df.mean()['TSLA'], df.mean()['YELP'], 'ro', markersize = 6);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-32-output-1.png){width=474 height=470}\n:::\n:::\n\n\nRecall that the correlation between these two stocks was about 0.137.\n\nThat is, the stocks are positively correlated, but not highly so.\n\n### Bayes' Rule\n\nBayes' Rule is a simple way of manipulating conditional probabilities in a way that can be very useful.\n\nStart with a situation in which we are interested in two events, $A_1$ and $A_2$.  \n\nThese are exhaustive, meaning that in any experiment either $A_1$ or $A_2$ must occur.   (That is, they form a partition of $\\Omega$.)\n\nNow, according to the rules of probability:\n\n$$ P[A_1|B] = \\frac{P[A_1 \\cap B]}{P[B]}$$\n\n$$ = \\frac{P[A_1 \\cap B]}{P[B|A_1] + P[B|A_2]}$$\n\n$$ = \\frac{P[B|A_1]\\,P[A_1]}{P[B|A_1] + P[B|A_2]}$$\n\n(This is easy to extend to the case where $A_1, A_2, ..., A_n$ form a partition of $\\Omega$.)\n\nThis formula is useful because often the probabilities $P[B|A_i]$ can be\nestimated, while the probabilties $P[A_i|B]$ may be hard to estimate.\n\nOr perhaps $P[B]$ itself (which is really the denominator) is easy to estimate.\n\nWe interpret this transformation as updating our estimate of the probability of each $A_i$\nbased on new information, namely, that $B$ is true.   \n\nThis update\ntransforms the __prior__ probabilities $P[A_i]$ into the \n__posterior__ probabilities $P[A_i|B]$.\n\n__Example.__\n\nEmpirical evidence suggests that amongs sets of twins,\nabout 1/3 are identical.  \n\nAssume therefore that probability of a pair of\ntwins being identical to be 1/3.  \n\nNow, consider how a couple might\nupdate this probability after they get an ultrasound that shows that the\ntwins are of the same gender.   \n\nWhat is their new estimate of the\nprobability that their twins are identical?\n\nLet $I$ be the event that the twins are identical.  Let $G$ be the event\nthat gender is the same via ultrasound.  \n\nThe prior probabilities here\nis $P[I]$.\n\nWhat we want to calculate is the\nposterior probability $P[I\\,|\\,G]$.\n\nFirst, we note:\n\n$$P[G\\,|\\,I] = 1 $$\n\n(Surprisingly, people are sometimes confused about that fact!)   \n\nNote that this conditional probability is easy to estimate.\n\nAlso,\nwe assume that if the twins are not identical, they are like any two\nsiblings, _i.e.,_ their probability of being same gender is 1/2: \n\n$$P[G\\,|\\,\\bar{I}] = 1/2 $$\n\nAgain, easy to estimate.\n\nAnd we know from observing the population at large that among all sets\nof twins, about 1/3 are identical:\n\n$$ P[I] = 1/3 $$\n\nNote that this statistic is easy to obtain from data.\n\nThen:\n\n$$ P[I\\,|\\,G] = \\frac{P[G\\,|\\,I]\\; P[I]}{P[G\\,|\\,I]\\,P[I] + P[G\\,|\\,\\bar{I}]\\,P[\\bar{I}]} $$\n\n$$ = \\frac{1 \\cdot 1/3}{(1 \\cdot 1/3) + (1/2 \\cdot 2/3)} = \\frac{1}{2}$$\n\nSo we have updated our estimate of the twins being identical from 1/3\n(prior probability) to 1/2 (posterior probability).\n\nWe did this in a way that used quantities that were relatively easy to obtain or measure.\n\n### Confidence Intervals\n\nSay you are concerned with some data that we take as coming from a random process.\n\nYou want to characterize it as accurately as possible.   You measure it, yielding a\nsingle value.  \n\nHow much does that value tell you?   Can you rely on it as a description of the random process?\n\nLet's say you have a dataset and you compute its average value.   \n\nHow certain are you that the average would be the same if you took another dataset from the same source (i.e., the same random process)?\n\nWe think of the hypothetical data source as a random variable with a true mean $\\mu$.\n\n(Note that we are using frequentist style thinking here.)\n\nWe would like to find a range within which we are 90% sure that the\ntrue mean $\\mu$ lies.  \n\nIn other words, we want the probability that the true mean lies in\nthe interval to be 0.9.  \n\nThis interval is then called the 90% confidence interval.\n\nTo be more precise:  A confidence interval at level $\\gamma$ for a fixed\nbut unknown parameter $m$ is an interval $(a,b)$ such that\n\n$$ P[A < m < B] \\geq \\gamma. $$\n\nNote that $m$ is fixed --- it is not random.  \n\nWhat is random is the\ninterval $(A, B)$.  \n\nThis interval is constructed based on the\ndata, which (by assumption) are random.\n\n![](figs/confidence-intervals.png)\n\n### Confidence Intervals for the Mean\n\nImagine we have a set of $n$ samples of a random variable,\n$x_1, x_2, ..., x_n$ Let's assume that the random variable has mean\n$\\mu$ and variance $\\sigma^2$.\n\nAn estimate of $\\mu$ is the empirical average of the samples, $\\bar{x}$.   \n\nNow, the Central Limit Theorem tells us that the sum of a\nlarge number $n$ of random variables, each with mean $\\mu$ and variance\n$\\sigma^2$, yields a Gaussian random variable with mean\n$n\\mu$ and variance $n \\sigma^2$.   \n\nSo the distribution of the average of $n$ samples\nis normal with mean $\\mu$ and variance $\\sigma^2 / n$.  That\nis, \n\n$$ \\bar{x} \\sim \\mathcal{N}(\\mu, \\sigma/\\sqrt{n}) $$\n\nWe usually assume that the number of samples should be 30 or more for\nthe CLT to hold.   \n\nWhile the specific value 30 is a bit arbitrary, we will usually be\nusing very large samples (datasets) in this course for which this assumption is \nvalid.\n\nThe standard deviation of the sample mean is called the standard error.\n\nNotice that the standard error decreases as we increase the sample size, according to $1/\\sqrt{n}.$\n\nSo it will turn out that using $\\bar{x}$, we can get\nincreasingly \"tight\" estimates of $\\mu$ as we increase the number of\nsamples $n$.\n\nNow, remember that the true mean $\\mu$ is a constant, while the\nempirical mean $\\bar{x}$ is a random variable.   \n\nLet us assume for a\nmoment that we know the true $\\mu$ and $\\sigma$, and that we accept that\n$\\bar{x}$ has a $N(\\mu, \\sigma/\\sqrt{n})$ distribution.   \n\nThen it is\ntrue that \n\n$$ P[\\mu-k\\sigma/\\sqrt{n} < \\bar{x} < \\mu+k\\sigma/\\sqrt{n}] = P[-k < S <\nk]$$\n\nwhere $S$ is the standard Gaussian random variable (having distribution $N(0,1)$).\n\nWe write $z_{1-\\alpha/2}$ to be the $1-\\alpha/2$ quantile of the\nunit normal.  That is, \n\n$$ P[-z_{1-\\alpha/2} < S < z_{1-\\alpha/2}] = 1-\\alpha.$$\n\nSo to form a 90% probability interval for $S$ (centered on\nzero) we choose $k = z_{0.95}$.  \n\n::: {#c2d24ec8 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=32}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)\nx90 = np.linspace(norm.ppf(0.05), norm.ppf(0.95), 100)\nplt.plot(x, norm.pdf(x),'b-')\nplt.fill_between(x90, 0, norm.pdf(x90))\nplt.title(r'90% region for Standard Gaussian', size = 14)\nplt.xlabel('x', size = 14)\nplt.ylabel(r'$p(x)$', size = 14);\n```\n\n::: {.cell-output .cell-output-display}\n![](03-Probability-and-Statistics-Refresher_files/figure-html/cell-33-output-1.png){width=605 height=457}\n:::\n:::\n\n\nTurning back to $\\bar{x}$, the\n90% probability interval on $\\bar{x}$ would be:\n\n$$ \\mu-z_{0.95}\\sigma/\\sqrt{n} < \\bar{x} <\n\\mu+z_{0.95}\\sigma/\\sqrt{n}. $$\n\nThe last step: by a simple argument, we can show that the sample mean is \nin some fixed-size interval centered on the true mean, if and only if the true\nmean is also in\na fixed-size interval (of the same size) centered on the sample mean.\n\nThis means that:\n\n\\begin{eqnarray*}\n  1-\\alpha & = & P[\\mu-z_{1-\\alpha/2}\\sigma/\\sqrt{N} < \\bar{x} <\n\\mu+z_{1-\\alpha/2}\\sigma/\\sqrt{N}]\\\\\n& = & P[\\bar{x}-k\\sigma\\sqrt{N} < \\mu < \\bar{x}+k\\sigma/\\sqrt{N}].\n\\end{eqnarray*}\n\nThis latter expression defines the __$1-\\alpha$ confidence interval\n  for the mean.__\n\nWe are done, except for estimating $\\sigma$.  We do this\ndirectly from the data: $\\hat{\\sigma} = s$\n\nwhere $s$ is the sample standard deviation, that is,\n$s = \\sqrt{1/(n-1) \\sum (x_i - \\bar{x})^2}$.\n\nTo summarize: by the argument presented here, a 100(1-$\\alpha$)%\nconfidence interval for the population mean is given by\n\n$$\\bar{x} \\pm  z_{1-\\alpha/2} \\, \\frac{s}{\\sqrt{n}}. $$\n\nAs an example, a 95% confidence interval for the mean is the sample average plus or minus two standard errors.\n\n",
    "supporting": [
      "03-Probability-and-Statistics-Refresher_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}