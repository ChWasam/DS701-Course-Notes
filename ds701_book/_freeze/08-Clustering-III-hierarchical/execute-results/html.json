{
  "hash": "dc1931f0c74f1c347b7464e0cddc631d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hierarchical Clustering\njupyter: python3\nfig-align: center\n---\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/08-Clustering-III-hierarchical.ipynb)\n\n::: {#a8a0308d .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.datasets as sk_data\nfrom sklearn.cluster import KMeans\n\n#import matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\nToday we will look at a fairly different approach to clustering.\n\nSo far, we have been thinking of clustering as finding a __partition__ of our dataset.\n\nThat is, a set of nonoverlapping clusters, in which each data item is in one cluster.\n\nHowever, in many cases, the notion of a strict partition is not as useful.\n\n# Example\n\n## How Many Clusters?\n\nHow many clusters would you say there are here?\n\n::: {#34cb09b2 .cell execution_count=3}\n``` {.python .cell-code}\nX_rand, y_rand = sk_data.make_blobs(\n    n_samples=[100, 100, 250, 70, 75, 80], \n    centers = [[1, 2], [1.5, 1], [3, 2], [1.75, 3.25], [2, 4], [2.25, 3.25]], \n    n_features = 2,\n    center_box = (-10.0, 10.0), \n    cluster_std = [.2, .2, .3, .1, .15, .15], \n    random_state = 0\n)\ndf_rand = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\ndf_rand = df_rand.astype({'label': 'int'})\ndf_rand['label2'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3}[x] for x in df_rand['label']]\ndf_rand['label3'] = [{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 2}[x] for x in df_rand['label']]\n# kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\n# df_rand['label'] = kmeans.fit_predict(df_rand[['X', 'Y']])\n\ndf_rand.plot('X', 'Y', kind = 'scatter',  \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-3-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## __Three clusters?__\n\n::: {#0414b848 .cell execution_count=4}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label3', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-4-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## __Four clusters?__\n\n::: {#f4f7f897 .cell execution_count=5}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label2', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-5-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## __Six clusters?__\n\n::: {#dd396367 .cell execution_count=6}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-6-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n---\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#31f2a43f .cell fig-height='400px' fig-width='400px' execution_count=7}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n                   colorbar = False, figsize = (4, 4))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-7-output-1.png){width=315 height=315 fig-align='left'}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\nThis dataset shows clustering on __multiple scales.__\n\n:::: {.fragment}\nTo fully capture the structure in this dataset, two things are needed:\n::::\n\n::: {.incremental}\n1. Capturing the differing clusters depending on the scale\n2. Capturing the containment relations -- which clusters lie within other clusters\n:::\n\n:::: {.fragment}\nThese observations motivate the notion of __hierarchical__ clustering.\n\nIn hierarchical clustering, we move away from the __partition__ notion of $k$-means, \n\nand instead capture a more complex arrangement that includes containment of one cluster within another.\n::::\n\n:::\n::::\n\n\n# Hierarchical Clustering\n\n## Hierarchical Clustering\n\nA hierarchical clustering produces a set of __nested__ clusters organized into a tree.\n\nA hierarchical clustering is visualized using a...\n\n __dendrogram__ \n\n* A tree-like diagram that records the containment relations among clusters.\n\n![](./figs/L08-dendrogram.png){width=\"600px\"}\n\n\n## Strengths of Hierarchical Clustering\n\nHierarchical clustering has a number of advantages:\n\n:::: {.incremental}\n* Encodes many __different__ clusterings.\n    * It does not itself decide on the correct number of clusters.\n* A clustering is obtained by \"cutting\" the dendrogram at some level.\n* You can make this crucial decision yourself, by inspecting the dendrogram.  \n* You can obtain a (somewhat) arbitrary number of clusters.\n::::\n\n![](./figs/L08-dendrogram-cut.png){width=\"600px\"}\n\n## Another advantage\n\nAnother advantage is that the dendrogram may itself correspond to a meaningful\nstructure, for example, a taxonomy.\n\n![](./figs/L08-animal-taxonomy.jpg){width=\"100%\"}\n\n## Yet another advantage\n\n* Many hierarchical clustering methods can be performed\nusing either similarity (proximity) or dissimilarity (distance) metrics.\n* This can be very helpful! \n* (Note that techniques like $k$-means cannot be used with unmodified similarity metrics.)\n\n## Compared to $k$-means\n\nAnother aspect of hierachical clustering is that it can handle certain cases\nbetter than $k$-means.\n\nBecause of the nature of the $k$-means algorithm, $k$-means tends to produce:\n\n* Roughly spherical clusters\n* Clusters of approximately equal size\n* Non-overlapping clusters\n\nIn many real-world situations, clusters may not be round, they may be of\nunequal size, and they may overlap.\n\nHence we would like clustering algorithms that can work in those cases also.\n\n\n# Hierarchical Clustering Algorithms\n\n## Hierarchical Clustering Algorithms\n\nThere are two main approaches to hierarchical clustering: \"bottom-up\" and \"top-down.\"\n\n::: {.fragment}\n__Agglomerative__ Clustering (\"bottom-up\"):\n\n* Start by defining each point as its own cluster\n* At each successive step, merge the two clusters that are closest to each other\n* Repeat until only one cluster is left.\n:::\n\n::: {.fragment}\n__Divisive__ Clustering (\"top-down\"):\n    \n* Start with one, all-inclusive cluster\n* At each step, find the cluster split that creates the largest distance between resulting clusters\n* Repeat until each point is in its own cluster.\n:::\n\n## Some key points\n \n* Agglomerative techniques are by far the more common.\n* The key to both of these methods is defining __the distance between two clusters.__\n* Different definitions for the inter-cluster distance yield different clusterings.\n\n::: {.fragment}\nTo illustrate the impact of the choice of cluster distances, we'll focus on agglomerative clustering.\n:::\n\n## Hierarchical Clustering Algorithm Inputs\n\n1. __Input data__ as either\n    * __2-D sample/feature matrix__\n    * __1D condensed distance matrix__ -- upper triangular of pairwise distance matrix flattened\n2. The cluster __linkage__ method (__single__, complete, average, ward, ...)\n3. The __distance metric__ (_euclidean_, manhattan, jaccard, ...)\n\n## Hierarchical Clustering Algorithm Output\n\nAn $(n-1,4)$ shaped __linkage matrix__.\n\nWhere __each row__ is `[idx1, idx2, dist, sample_count]` is a single step in the clustering\nprocess and\n\n`idx1` and `idx2` are indices of the cluster being merged, where \n\n* indices $\\{ 0...n-1 \\}$ refer to the original data points and \n* indices $\\{n, n+1, ...\\}$ refer to each new cluster created\n\nAnd `sample_count` is the number of original data samples in the new cluster.\n\n\n## Hierarchical Clustering Algorithm Explained\n\n1. __Initialization__\n    * Start with each data point as its own cluster.\n    * Calculate the distance matrix if not provided\n2. __Iterative Clustering__\n    * At each step, the two closest clusters (accourding to linkage method and distance metric)\n      are merged into a new cluster.\n    * The distance between new cluster and the remaining clusters is added\n    * Repeat previous two steps until only one cluster remains.\n\n\n## Defining Cluster Proximity\n\nGiven two clusters, how do we define the _distance_ between them?\n\nHere are three natural ways to do it.\n\n## Cluster Proximity -- Single-Linkage\n\n![](./figs/L08-hierarchical-criteria-a.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Single-Linkage:__ the distance between two clusters is the distance between the\nclosest two points that are in different clusters.\n   \n$$\nD_\\text{single}(i,j) = \\min_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n$$\n\n## Cluster Proximity -- Complete-Linkage\n\n![](./figs/L08-hierarchical-criteria-b.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Complete-Linkage:__ the distance between two clusters is the distance between\nthe farthest two points that are in different clusters.\n\n$$\nD_\\text{complete}(i,j) = \\max_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n$$\n\n## Cluster Proximity -- Average-Linkage\n\n![](./figs/L08-hierarchical-criteria-c.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Average-Linkage:__ the distance between two clusters is the average distance between all pairs of points from different clusters.\n\n$$\nD_\\text{average}(i,j) = \\frac{1}{|C_i|\\cdot|C_j|}\\sum_{x \\in C_i,\\, y \\in C_j}d(x, y)\n$$\n\n## Cluster Proximity Example\n\nNotice that it is easy to express the definitions above in terms of similarity instead of distance.\n\nHere is a set of 6 points that we will cluster to show differences between distance metrics.\n\n::: {#fb7b446c .cell fig-width='600px' execution_count=8}\n``` {.python .cell-code}\npt_x = [0.4, 0.22, 0.35, 0.26, 0.08, 0.45]\npt_y = [0.53, 0.38, 0.32, 0.19, 0.41, 0.30]\nplt.plot(pt_x, pt_y, 'o', markersize = 10, color = 'k')\nplt.ylim([.15, .60])\nplt.xlim([0.05, 0.70])\nfor i in range(6):\n    plt.annotate(f'{i}', (pt_x[i]+0.02, pt_y[i]-0.01), fontsize = 12)\nplt.axis('on')\nplt.xticks([])\nplt.yticks([])\n#plt.savefig('figs/L08-basic-pointset.png');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-8-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Cluster Proximity Example, cont.\n:::\n\nWe can calculate the distance matrix\n\n::: {#8c0cefe8 .cell execution_count=9}\n``` {.python .cell-code}\nX = np.array([pt_x, pt_y]).T\nfrom scipy.spatial import distance_matrix\nlabels = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5']\nD = pd.DataFrame(distance_matrix(X, X), index = labels, columns = labels)\nD.style.format('{:.2f}')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_ba172\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_ba172_level0_col0\" class=\"col_heading level0 col0\" >p0</th>\n      <th id=\"T_ba172_level0_col1\" class=\"col_heading level0 col1\" >p1</th>\n      <th id=\"T_ba172_level0_col2\" class=\"col_heading level0 col2\" >p2</th>\n      <th id=\"T_ba172_level0_col3\" class=\"col_heading level0 col3\" >p3</th>\n      <th id=\"T_ba172_level0_col4\" class=\"col_heading level0 col4\" >p4</th>\n      <th id=\"T_ba172_level0_col5\" class=\"col_heading level0 col5\" >p5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_ba172_level0_row0\" class=\"row_heading level0 row0\" >p0</th>\n      <td id=\"T_ba172_row0_col0\" class=\"data row0 col0\" >0.00</td>\n      <td id=\"T_ba172_row0_col1\" class=\"data row0 col1\" >0.23</td>\n      <td id=\"T_ba172_row0_col2\" class=\"data row0 col2\" >0.22</td>\n      <td id=\"T_ba172_row0_col3\" class=\"data row0 col3\" >0.37</td>\n      <td id=\"T_ba172_row0_col4\" class=\"data row0 col4\" >0.34</td>\n      <td id=\"T_ba172_row0_col5\" class=\"data row0 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_ba172_level0_row1\" class=\"row_heading level0 row1\" >p1</th>\n      <td id=\"T_ba172_row1_col0\" class=\"data row1 col0\" >0.23</td>\n      <td id=\"T_ba172_row1_col1\" class=\"data row1 col1\" >0.00</td>\n      <td id=\"T_ba172_row1_col2\" class=\"data row1 col2\" >0.14</td>\n      <td id=\"T_ba172_row1_col3\" class=\"data row1 col3\" >0.19</td>\n      <td id=\"T_ba172_row1_col4\" class=\"data row1 col4\" >0.14</td>\n      <td id=\"T_ba172_row1_col5\" class=\"data row1 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_ba172_level0_row2\" class=\"row_heading level0 row2\" >p2</th>\n      <td id=\"T_ba172_row2_col0\" class=\"data row2 col0\" >0.22</td>\n      <td id=\"T_ba172_row2_col1\" class=\"data row2 col1\" >0.14</td>\n      <td id=\"T_ba172_row2_col2\" class=\"data row2 col2\" >0.00</td>\n      <td id=\"T_ba172_row2_col3\" class=\"data row2 col3\" >0.16</td>\n      <td id=\"T_ba172_row2_col4\" class=\"data row2 col4\" >0.28</td>\n      <td id=\"T_ba172_row2_col5\" class=\"data row2 col5\" >0.10</td>\n    </tr>\n    <tr>\n      <th id=\"T_ba172_level0_row3\" class=\"row_heading level0 row3\" >p3</th>\n      <td id=\"T_ba172_row3_col0\" class=\"data row3 col0\" >0.37</td>\n      <td id=\"T_ba172_row3_col1\" class=\"data row3 col1\" >0.19</td>\n      <td id=\"T_ba172_row3_col2\" class=\"data row3 col2\" >0.16</td>\n      <td id=\"T_ba172_row3_col3\" class=\"data row3 col3\" >0.00</td>\n      <td id=\"T_ba172_row3_col4\" class=\"data row3 col4\" >0.28</td>\n      <td id=\"T_ba172_row3_col5\" class=\"data row3 col5\" >0.22</td>\n    </tr>\n    <tr>\n      <th id=\"T_ba172_level0_row4\" class=\"row_heading level0 row4\" >p4</th>\n      <td id=\"T_ba172_row4_col0\" class=\"data row4 col0\" >0.34</td>\n      <td id=\"T_ba172_row4_col1\" class=\"data row4 col1\" >0.14</td>\n      <td id=\"T_ba172_row4_col2\" class=\"data row4 col2\" >0.28</td>\n      <td id=\"T_ba172_row4_col3\" class=\"data row4 col3\" >0.28</td>\n      <td id=\"T_ba172_row4_col4\" class=\"data row4 col4\" >0.00</td>\n      <td id=\"T_ba172_row4_col5\" class=\"data row4 col5\" >0.39</td>\n    </tr>\n    <tr>\n      <th id=\"T_ba172_level0_row5\" class=\"row_heading level0 row5\" >p5</th>\n      <td id=\"T_ba172_row5_col0\" class=\"data row5 col0\" >0.24</td>\n      <td id=\"T_ba172_row5_col1\" class=\"data row5 col1\" >0.24</td>\n      <td id=\"T_ba172_row5_col2\" class=\"data row5 col2\" >0.10</td>\n      <td id=\"T_ba172_row5_col3\" class=\"data row5 col3\" >0.22</td>\n      <td id=\"T_ba172_row5_col4\" class=\"data row5 col4\" >0.39</td>\n      <td id=\"T_ba172_row5_col5\" class=\"data row5 col5\" >0.00</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Single-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-singlelink-pointset.png){width=\"600px\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#18548a44 .cell execution_count=10}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nZ = hierarchy.linkage(X, method='single', metric='euclidean')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-10-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Single Linkage Clustering Advantages\n\nSingle-linkage clustering can handle non-elliptical shapes.\n\nHere we use SciPy's [fcluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html)\nto form flat custers from hierarchical clustering.\n\n::: {#43d5c564 .cell execution_count=11}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n\nZ = hierarchy.linkage(X_moon_05, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Single-Linkage Can Find Irregularly Shaped Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-11-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides \"}\n## Single Linkage Advantages, cont.\n:::\n\nSingle-Linkage can find different sized clusters:\n\n::: {#4071cc84 .cell execution_count=12}\n``` {.python .cell-code}\nX_rand_lo, y_rand_lo = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.1, .5], random_state = 0)\n\nZ = hierarchy.linkage(X_rand_lo, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_rand_lo[:,0], X_rand_lo[:,1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Single-Linkage Can Find Different-Sized Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-12-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Single Linkage Disadvantages\n\nSingle-linkage clustering can be sensitive to noise and outliers.\n\nThe results can change drastically on even slightly more noisy data.\n\n::: {#ff3a3012 .cell execution_count=13}\n``` {.python .cell-code}\nX_moon_10, y_moon_10 = sk_data.make_moons(random_state = 0, noise = 0.1)\n\nZ = hierarchy.linkage(X_moon_10, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-13-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Single Linkage Disadvantages, cont.\n:::\n\nAnd here's another example where we bump the standard deviation on the clusters slightly.\n\n::: {#88a461c9 .cell execution_count=14}\n``` {.python .cell-code}\nX_rand_hi, y_rand_hi = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.15, .6], random_state = 0)\n\nZ = hierarchy.linkage(X_rand_hi, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\n# plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-14-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Complete-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-completelink-pointset.png){width=\"100%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#ff0f1d3a .cell execution_count=15}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='complete')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-15-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Complete-Linkage Clustering Advantages\n\nProduces more-balanced clusters -- more-equal diameters\n\n::: {#2b3e0c24 .cell execution_count=16}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n\nZ = hierarchy.linkage(X_moon_05, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\n#plt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-16-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {#51d1b233 .cell execution_count=17}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\n#plt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-17-output-1.png){width=763 height=389}\n:::\n:::\n\n\nLess susceptible to noise:\n\n::: {#9ed1d0b7 .cell execution_count=18}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n#plt.title('Complete-Linkage Clustering of Noisy Data similar to Less Noisy')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-18-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Average-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#5aec47ca .cell execution_count=19}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='average')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-19-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Average-Linkage Clustering Strengths and Limitations\n\nAverage-Linkage clustering is in some sense a compromise between Single-linkage and Complete-linkage clustering.\n\n__Strengths:__\n    \n* Less susceptible to noise and outliers\n\n__Limitations:__\n    \n* Biased toward elliptical clusters\n\nProduces more isotropic clusters.\n\n::: {#18ff02c2 .cell execution_count=20}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Average-Linkage Similar to Complete - Globular Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-20-output-1.png){width=763 height=389}\n:::\n:::\n\n\nMore resistant to noise than Single-Linkage.\n\n::: {#9a5c031f .cell execution_count=21}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Average-Linkage More resistant to noise than Single-Linkage')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-21-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## All Three Compared\n\n::: {layout-ncol=\"3\"}\n\n![Single-Linkage](./figs/L08-singlelink-pointset.png){width=\"100%\"}\n\n![Complete-Linkage](./figs/L08-completelink-pointset.png){width=\"100%\"}\n\n![Average-Linkage](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n\n:::\n\n## Ward's Distance\n\nFinally, we consider one more cluster distance.\n\nWard's distance asks \"What if we combined these two clusters -- how would clustering improve?\"\n\nTo define \"how would clustering improve?\" we appeal to the $k$-means criterion.\n\nSo:\n\n__Ward's Distance__ between clusters $C_i$ and $C_j$ is the difference between\nthe total within cluster sum of squares for the two clusters separately, \n__compared to__ the _within cluster sum of squares_ resulting from merging the two\nclusters into a new cluster $C_{i+j}$:\n\n$$\nD_\\text{Ward}(i, j) = \\sum_{x \\in C_i} (x - c_i)^2 + \\sum_{x \\in C_j} (x - c_j)^2  - \\sum_{x \\in C_{i+j}} (x - c_{i+j})^2 \n$$\n\nwhere $c_i, c_j, c_{i+j}$ are the corresponding cluster centroids.\n\n::: {.content-visible when-profile=\"slides\"}\n## Ward's Distance continued\n:::\n\nIn a sense, this cluster distance results in a hierarchical analog of $k$-means.\n\nAs a result, it has properties similar to $k$-means:\n    \n* Less susceptible to noise and outliers\n* Biased toward elliptical clusters\n\nHence it tends to behave more like group-average hierarchical clustering.\n\n\n# Hierarchical Clustering in Practice\n\n## Hierarchical Clustering In Practice\n\nNow we'll look at doing hierarchical clustering in practice.\n\nWe'll use the same synthetic data as we did in the k-means case -- ie.,\nthree \"blobs\" living in 30 dimensions.\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\n::: {#797a48e7 .cell execution_count=22}\n``` {.python .cell-code}\nX, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n                          center_box=(-10.0, 10.0),random_state=0)\n```\n:::\n\n\nAs a reminder of the raw data here is the visualization: first the raw data, \n\n::: {#4561099c .cell execution_count=23}\n``` {.python .cell-code}\nsns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-23-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\n\nthen an embedding into 2-D (using MDS).\n\n::: {#25995b14 .cell execution_count=24}\n``` {.python .cell-code}\nimport sklearn.manifold\nimport sklearn.metrics as metrics\neuclidean_dists = metrics.euclidean_distances(X)\nmds = sklearn.manifold.MDS(n_components = 2, max_iter = 3000, eps = 1e-9, random_state = 0,\n                   dissimilarity = \"precomputed\", n_jobs = 1)\nfit = mds.fit(euclidean_dists)\npos = fit.embedding_\nplt.axis('equal')\nplt.scatter(pos[:, 0], pos[:, 1], s = 8);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-24-output-1.png){width=801 height=411}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\nHierarchical clustering is available in __`sklearn`__, but there is a much more\nfully developed set of \n[tools](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) \nin the [scipy](https://docs.scipy.org/doc/scipy/index.html) package and that is the one to use.\n\nLet's run hierarchical clustering on our synthetic dataset.\n\n::: {.callout-tip}\nTry the other linkage methods and see how the clustering and dendogram changes.\n:::\n\n::: {#6cc18c57 .cell execution_count=25}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nimport scipy.spatial.distance\n\n# linkages = ['single','complete','average','weighted','ward']\nZ = hierarchy.linkage(X, method = 'single')\n```\n:::\n\n\nAnd draw the dendogram.\n\n::: {#4e4d0d09 .cell execution_count=26}\n``` {.python .cell-code}\nR = hierarchy.dendrogram(Z)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-26-output-1.png){width=790 height=407}\n:::\n:::\n\n\n## Hierarchical Clustering Real Data\n\nOnce again we'll use the\n[\"20 Newsgroup\"](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\ndata provided as example data in sklearn.\n\nLoad three of the newsgroups.\n\n::: {#c400a181 .cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset = 'train', categories = categories)\n```\n:::\n\n\nVectorize the data.\n\n::: {#a191f290 .cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english', min_df = 4, max_df = 0.8)\ndata = vectorizer.fit_transform(news_data.data).todense()\ndata.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(1781, 9409)\n```\n:::\n:::\n\n\nCluster hierarchically and display dendogram. Feel free to experiment with different metrics.\n\n::: {#159aea37 .cell execution_count=29}\n``` {.python .cell-code}\n# linkages are one of 'single','complete','average','weighted','ward'\n#\n# metrics can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, \n# ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, \n# ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, \n# ‘sqeuclidean’, ‘yule’.\n\nZ_20ng = hierarchy.linkage(data, method = 'ward', metric = 'euclidean')\nplt.figure(figsize=(14,4))\nR_20ng = hierarchy.dendrogram(Z_20ng, p=4, truncate_mode = 'level', show_leaf_counts=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-29-output-1.png){width=1079 height=361}\n:::\n:::\n\n\n## Selecting the Number of Clusters\n\nLet's flatten the hierarchy to different numbers clusters and calculate the \n_Silhouette Score_.\n\n::: {#d026a201 .cell execution_count=30}\n``` {.python .cell-code}\nmax_clusters = 20\ns = np.zeros(max_clusters+1)\n\nfor k in range(2, max_clusters+1):\n    clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n    s[k] = metrics.silhouette_score(np.asarray(data), clusters, metric = 'euclidean')\n\nplt.plot(range(2, len(s)), s[2:], '.-')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-30-output-1.png){width=829 height=429}\n:::\n:::\n\n\nWe see a first peak at 5.\n\nTop terms per cluster when we flatten to a depth of 5.\n\n::: {#848096ac .cell execution_count=31}\n``` {.python .cell-code}\nk = 5\nclusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\nfor i in range(1,k+1):\n    items = np.array([item for item,clust in zip(data, clusters) if clust == i])\n    centroids = np.squeeze(items).mean(axis = 0)\n    asc_order_centroids = centroids.argsort()#[:, ::-1]\n    order_centroids = asc_order_centroids[::-1]\n    terms = vectorizer.get_feature_names_out()\n    print(f'Cluster {i}:')\n    for ind in order_centroids[:10]:\n        print(f' {terms[ind]}')\n    print('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 1:\n space\n nasa\n edu\n henry\n gov\n alaska\n access\n com\n moon\n digex\n\nCluster 2:\n ax\n max\n b8f\n g9v\n a86\n 145\n 1d9\n pl\n 2di\n 0t\n\nCluster 3:\n edu\n com\n year\n baseball\n article\n writes\n cs\n team\n game\n university\n\nCluster 4:\n risc\n instruction\n ghhwang\n csie\n set\n nctu\n cisc\n tw\n reduced\n mq\n\nCluster 5:\n windows\n edu\n file\n dos\n com\n files\n card\n drivers\n driver\n use\n\n```\n:::\n:::\n\n\n## Comparison of Linkages\n\nScikit-Learn has a very nice [notebook](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html)\nand plot, copied here, that shows the different clusters resulting from different\nlinkage methods.\n\n![](./figs/L08-sphx_glr_plot_linkage_comparison_001.png){width=\"100%\" fig-align=\"center\"}\n\n\n\n\n# Recap\n\n## Clustering Recap\n\nThis wraps up our _partitional_ Cluster topics. We covered:\n\n::: {.incremental}\n* What the clusering problem is\n* An overview of the $k$-means clustering algorithm including initialization with $k$-means++\n* Visualization techniques such as Multi-Dimensional Scaling\n* Cluster evaluation with (Adjusted) Rand Index and Silhouette Coefficient\n* Using evaluation to determine number of clusters\n* Hierarchical Clustering with different methods and metrics\n* Looked at applications of clustering on various types of synthetic data, image\n  color quantization, newsgroup clustering\n:::\n\n",
    "supporting": [
      "08-Clustering-III-hierarchical_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}