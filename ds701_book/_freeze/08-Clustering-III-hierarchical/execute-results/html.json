{
  "hash": "162856959dfb7673f3634a0bede1f80b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hierarchical Clustering\njupyter: python3\n---\n\n::: {#25b4377b .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.datasets as sk_data\nfrom sklearn.cluster import KMeans\n\n#import matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\nToday we will look at a fairly different approach to clustering.\n\nSo far, we have been thinking of clustering as finding a __partition__ of our dataset.\n\nThat is, a set of nonoverlapping clusters, in which each data item is in one cluster.\n\nHowever, in many cases, the notion of a strict partition is not as useful.\n\n## How Many Clusters?\n\nHow many clusters would you say there are here?\n\n::: {#dd1564df .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=2}\n``` {.python .cell-code}\nX_rand, y_rand = sk_data.make_blobs(n_samples=[100, 100, 250, 70, 75, 80], centers = [[1, 2], [1.5, 1], [3, 2], [1.75, 3.25], [2, 4], [2.25, 3.25]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.2, .2, .3, .1, .15, .15], random_state = 0)\ndf_rand = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\ndf_rand = df_rand.astype({'label': 'int'})\ndf_rand['label2'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3}[x] for x in df_rand['label']]\ndf_rand['label3'] = [{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 2}[x] for x in df_rand['label']]\n# kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\n# df_rand['label'] = kmeans.fit_predict(df_rand[['X', 'Y']])\n```\n:::\n\n\n::: {#38fbf670 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter',  \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-4-output-1.png){width=463 height=463}\n:::\n:::\n\n\n__Three clusters?__\n\n::: {#52ab972e .cell hide_input='true' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label3', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-5-output-1.png){width=463 height=463}\n:::\n:::\n\n\n__Four clusters?__\n\n::: {#5a7ffaaa .cell hide_input='true' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label2', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-6-output-1.png){width=463 height=463}\n:::\n:::\n\n\n__Six clusters?__\n\n::: {#0d63b6b8 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-7-output-1.png){width=463 height=463}\n:::\n:::\n\n\nThis dataset shows clustering on __multiple scales.__\n\nTo fully capture the structure in this dataset, two things are needed:\n1. Capturing the differing clusters depending on the scale\n2. Capturing the containment relations -- which clusters lie within other clusters\n\nThese observations motivate the notion of __hierarchical__ clustering.\n\nIn hierarchical clustering, we move away from the __partition__ notion of $k$-means, \n\nand instead capture a more complex arrangement that includes containment of one cluster within another.\n\n## Hierarchical Clustering\n\nA hierarchical clustering produces a set of __nested__ clusters organized into a tree.\n\nA hierarchical clustering is visualized using a __dendrogram__ \n\n* A tree-like diagram that records the containment relations among clusters.\n\n<center>\n\n<img src=\"./figs/L08-dendrogram.png\" width=\"600px\">\n\n</center>\n\n### Strengths of Hierarchical Clustering\n\nHierarchical clustering has a number of advantages:\n\nFirst, a hierarchical clustering encodes many __different__ clusterings.  That is, it does not itself decide on the correct number of clusters.  \n\nA clustering is obtained by \"cutting\" the dendrogram at some level.\n\nThis means that you can make this crucial decision yourself, by inspecting the dendrogram.  \n\nPut another way, you can obtain any desired number of clusters.\n\n<center>\n\n<img src=\"./figs/L08-dendrogram-cut.png\" width=\"600px\">\n\n</center>\n\nThe second advantage is that the dendrogram may itself correspond to a meaningful structure, for example, a taxonomy.\n\n<center>\n\n<img src=\"figs/L08-animal-taxonomy.jpg\" width=\"600px\">\n\n</center>\n\nThe third advantage is that many hierarchical clustering methods can be performed using either similarity (proximity) or dissimilarity (distance) metrics.\n\nThis can be very helpful! \n\n(Note that techniques like $k$-means cannot be used with unmodified similarity metrics.)\n\n### Compared to $k$-means\n\nAnother aspect of hierachical clustering is that it can handle certain cases better than $k$-means.\n\nBecause of the nature of the $k$-means algorithm, $k$-means tends to produce:\n* Roughly spherical clusters\n* Clusters of approximately equal size\n* Non-overlapping clusters\n\nIn many real-world situations, clusters may not be round, they may be of unequal size, and they may overlap.\n\nHence we would like clustering algorithms that can work in those cases also.\n\n## Hierarchical Clustering Algorithms\n\nThere are two main approaches to hierarchical clustering: \"bottom-up\" and \"top-down.\"\n\n__Agglomerative__ Clustering (\"bottom-up\"):\n\n* Start by defining each point as its own cluster\n* At each successive step, merge the two clusters that are closest to each other\n* Repeat until only one cluster is left.\n\n__Divisive__ Clustering (\"top-down\"):\n    \n* Start with one, all-inclusive cluster\n* At each step, find the cluster split that creates the largest distance between resulting clusters\n* Repeat until each point is in its own cluster.\n\nAgglomerative techniques are by far the more common.\n\nThe key to both of these methods is defining __the distance between two clusters.__\n\nDifferent definitions for the inter-cluster distance yield different clusterings.\n\nTo illustrate the impact of the choice of cluster distances, we'll focus on agglomerative clustering.\n\n### Defining Cluster Proximity\n\nGiven two clusters, how do we define the _distance_ between them?\n\nHere are three natural ways to do it:\n   * __Single-Linkage:__ the distance between two clusters is the distance between the closest two points that are in different clusters.\n   \n$$ D_\\text{single}(i,j) = \\min_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}$$\n\n* __Complete-Linkage:__ the distance between two clusters is the distance between the farthest two points that are in different clusters.\n\n$$ D_\\text{complete}(i,j) = \\max_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}$$\n\n* __Average-Linkage:__ the distance between two clusters is the average distance between all pairs of points from different clusters.\n\n$$ D_\\text{average}(i,j) = \\frac{1}{|C_i|\\cdot|C_j|}\\sum_{x \\in C_i,\\, y \\in C_j}d(x, y)$$\n\n<center>\n\n<img src=\"./figs/L08-hierarchical-criteria.png\" width=\"600px\">\n\n</center>\n\n<div style = \"float: left; width: 41%; text-align: center;\">\n    Single-Linkage\n</div>\n<div style = \"float: left; width: 18%; text-align: center;\">\n    Complete-Linkage\n</div>\n<div style = \"float: left; width: 41%; text-align: center;\">\n    Average-Linkage\n</div>\n\nNotice that it is easy to express the definitions above in terms of similarity instead of distance.\n\nHere is a set of 6 points that we will cluster to show differences between distance metrics.\n\n::: {#d26c42e7 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\npt_x = [0.4, 0.22, 0.35, 0.26, 0.08, 0.45]\npt_y = [0.53, 0.38, 0.32, 0.19, 0.41, 0.30]\nplt.plot(pt_x, pt_y, 'o', markersize = 10, color = 'k')\nplt.ylim([.15, .60])\nplt.xlim([0.05, 0.70])\nfor i in range(6):\n    plt.annotate(f'{i}', (pt_x[i]+0.02, pt_y[i]-0.01), fontsize = 12)\nplt.axis('off')\nplt.savefig('figs/L08-basic-pointset.png');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-8-output-1.png){width=540 height=389}\n:::\n:::\n\n\n::: {#2e28e2b3 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nX = np.array([pt_x, pt_y]).T\nfrom scipy.spatial import distance_matrix\nlabels = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5']\nD = pd.DataFrame(distance_matrix(X, X), index = labels, columns = labels)\nD.style.format('{:.2f}')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_47aaf\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_47aaf_level0_col0\" class=\"col_heading level0 col0\" >p0</th>\n      <th id=\"T_47aaf_level0_col1\" class=\"col_heading level0 col1\" >p1</th>\n      <th id=\"T_47aaf_level0_col2\" class=\"col_heading level0 col2\" >p2</th>\n      <th id=\"T_47aaf_level0_col3\" class=\"col_heading level0 col3\" >p3</th>\n      <th id=\"T_47aaf_level0_col4\" class=\"col_heading level0 col4\" >p4</th>\n      <th id=\"T_47aaf_level0_col5\" class=\"col_heading level0 col5\" >p5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_47aaf_level0_row0\" class=\"row_heading level0 row0\" >p0</th>\n      <td id=\"T_47aaf_row0_col0\" class=\"data row0 col0\" >0.00</td>\n      <td id=\"T_47aaf_row0_col1\" class=\"data row0 col1\" >0.23</td>\n      <td id=\"T_47aaf_row0_col2\" class=\"data row0 col2\" >0.22</td>\n      <td id=\"T_47aaf_row0_col3\" class=\"data row0 col3\" >0.37</td>\n      <td id=\"T_47aaf_row0_col4\" class=\"data row0 col4\" >0.34</td>\n      <td id=\"T_47aaf_row0_col5\" class=\"data row0 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_47aaf_level0_row1\" class=\"row_heading level0 row1\" >p1</th>\n      <td id=\"T_47aaf_row1_col0\" class=\"data row1 col0\" >0.23</td>\n      <td id=\"T_47aaf_row1_col1\" class=\"data row1 col1\" >0.00</td>\n      <td id=\"T_47aaf_row1_col2\" class=\"data row1 col2\" >0.14</td>\n      <td id=\"T_47aaf_row1_col3\" class=\"data row1 col3\" >0.19</td>\n      <td id=\"T_47aaf_row1_col4\" class=\"data row1 col4\" >0.14</td>\n      <td id=\"T_47aaf_row1_col5\" class=\"data row1 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_47aaf_level0_row2\" class=\"row_heading level0 row2\" >p2</th>\n      <td id=\"T_47aaf_row2_col0\" class=\"data row2 col0\" >0.22</td>\n      <td id=\"T_47aaf_row2_col1\" class=\"data row2 col1\" >0.14</td>\n      <td id=\"T_47aaf_row2_col2\" class=\"data row2 col2\" >0.00</td>\n      <td id=\"T_47aaf_row2_col3\" class=\"data row2 col3\" >0.16</td>\n      <td id=\"T_47aaf_row2_col4\" class=\"data row2 col4\" >0.28</td>\n      <td id=\"T_47aaf_row2_col5\" class=\"data row2 col5\" >0.10</td>\n    </tr>\n    <tr>\n      <th id=\"T_47aaf_level0_row3\" class=\"row_heading level0 row3\" >p3</th>\n      <td id=\"T_47aaf_row3_col0\" class=\"data row3 col0\" >0.37</td>\n      <td id=\"T_47aaf_row3_col1\" class=\"data row3 col1\" >0.19</td>\n      <td id=\"T_47aaf_row3_col2\" class=\"data row3 col2\" >0.16</td>\n      <td id=\"T_47aaf_row3_col3\" class=\"data row3 col3\" >0.00</td>\n      <td id=\"T_47aaf_row3_col4\" class=\"data row3 col4\" >0.28</td>\n      <td id=\"T_47aaf_row3_col5\" class=\"data row3 col5\" >0.22</td>\n    </tr>\n    <tr>\n      <th id=\"T_47aaf_level0_row4\" class=\"row_heading level0 row4\" >p4</th>\n      <td id=\"T_47aaf_row4_col0\" class=\"data row4 col0\" >0.34</td>\n      <td id=\"T_47aaf_row4_col1\" class=\"data row4 col1\" >0.14</td>\n      <td id=\"T_47aaf_row4_col2\" class=\"data row4 col2\" >0.28</td>\n      <td id=\"T_47aaf_row4_col3\" class=\"data row4 col3\" >0.28</td>\n      <td id=\"T_47aaf_row4_col4\" class=\"data row4 col4\" >0.00</td>\n      <td id=\"T_47aaf_row4_col5\" class=\"data row4 col5\" >0.39</td>\n    </tr>\n    <tr>\n      <th id=\"T_47aaf_level0_row5\" class=\"row_heading level0 row5\" >p5</th>\n      <td id=\"T_47aaf_row5_col0\" class=\"data row5 col0\" >0.24</td>\n      <td id=\"T_47aaf_row5_col1\" class=\"data row5 col1\" >0.24</td>\n      <td id=\"T_47aaf_row5_col2\" class=\"data row5 col2\" >0.10</td>\n      <td id=\"T_47aaf_row5_col3\" class=\"data row5 col3\" >0.22</td>\n      <td id=\"T_47aaf_row5_col4\" class=\"data row5 col4\" >0.39</td>\n      <td id=\"T_47aaf_row5_col5\" class=\"data row5 col5\" >0.00</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Single-Linkage Clustering\n\n<img src=\"figs/L08-singlelink-pointset.png\" width=\"100%\">\n\n::: {#0ba0047f .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nZ = hierarchy.linkage(X, method='single')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-10-output-1.png){width=579 height=414}\n:::\n:::\n\n\n__Advantages__:\n\n* Single-linkage clustering can handle non-elliptical shapes.\n\nIn fact it can produce long, elongated clusters:\n\n::: {#e8bd745c .cell hide_input='true' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\nZ = hierarchy.linkage(X_moon_05, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Single-Linkage Can Find Irregularly Shaped Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-11-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#ec35ecee .cell hide_input='true' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nX_rand_lo, y_rand_lo = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.1, .5], random_state = 0)\nZ = hierarchy.linkage(X_rand_lo, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_rand_lo[:,0], X_rand_lo[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Single-Linkage Can Find Different-Sized Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-12-output-1.png){width=540 height=409}\n:::\n:::\n\n\n__Disadvantages:__ \n\n* Single-linkage clustering can be sensitive to noise and outliers.\n\n::: {#2c271d3d .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\nX_moon_10, y_moon_10 = sk_data.make_moons(random_state = 0, noise = 0.1)\nZ = hierarchy.linkage(X_moon_10, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-13-output-1.png){width=613 height=409}\n:::\n:::\n\n\n::: {#f4370272 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nX_rand_hi, y_rand_hi = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.15, .6], random_state = 0)\nZ = hierarchy.linkage(X_rand_hi, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-14-output-1.png){width=613 height=409}\n:::\n:::\n\n\n### Complete-Linkage Clustering\n\n<img src=\"figs/L08-completelink-pointset.png\" width=\"100%\">\n\n::: {#c25962a6 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='complete')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-15-output-1.png){width=579 height=414}\n:::\n:::\n\n\n__Advantages__:\n\n* Produces more-balanced clusters -- more-equal diameters\n\n::: {#55ab79d2 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\nZ = hierarchy.linkage(X_moon_05, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_05[:,0], X_moon_05[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-16-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#39bc6249 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Complete-Linkage Seeks Globular Clusters of Similar Size')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-17-output-1.png){width=540 height=409}\n:::\n:::\n\n\nLess susceptible to noise:\n\n::: {#51499987 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=17}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Complete-Linkage Clustering of Noisy Data similar to Less Noisy')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-18-output-1.png){width=540 height=409}\n:::\n:::\n\n\n### Average-Linkage Clustering\n\n<img src=\"figs/L08-averagelink-pointset.png\" width=\"100%\">\n\n::: {#4ae8cbc4 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='average')\nhierarchy.dendrogram(Z);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-19-output-1.png){width=579 height=414}\n:::\n:::\n\n\nAverage-Linkage clustering is in some sense a compromise between Single-linkage and Complete-linkage clustering.\n\n__Strengths:__\n    \n* Less susceptible to noise and outliers\n\n__Limitations:__\n    \n* Biased toward elliptical clusters\n\n::: {#18af82fb .cell cell_style='center' hide_input='true' tags='[\"hide-input\"]' execution_count=19}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:,0], X_moon_10[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Average-Linkage Similar to Complete - Globular Clusters')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-20-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#3ec8befa .cell hide_input='true' tags='[\"hide-input\"]' execution_count=20}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_rand_hi[:,0], X_rand_hi[:,1], c = [['b','g'][i-1] for i in labels])\nplt.title('Average-Linkage More resistant to noise than Single-Linkage')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-21-output-1.png){width=540 height=409}\n:::\n:::\n\n\n### All Three Compared\n\n<div style = \"float: left; width: 33%; text-align: center;\">\n    <img src=\"figs/L08-singlelink-pointset.png\" style=\"width:100%\">\n    Single-Linkage\n</div>\n\n<div style = \"float: left; width: 33%; text-align: center;\">\n    <img src=\"figs/L08-completelink-pointset.png\"  style=\"width:100%\">\n    Complete-Linkage\n</div>\n\n<div style = \"float: left; width: 33%; text-align: center;\">\n    <img src=\"figs/L08-averagelink-pointset.png\" style=\"width:100%\">\n    Average-Linkage\n</div>\n\n## Ward's Distance\n\nFinally, we consider one more cluster distance.\n\nWard's distance asks \"what if\".\n\nThat is, \"What if we combined these two clusters -- how would clustering improve?\"\n\nTo define \"how would clustering improve?\" we appeal to the $k$-means criterion.\n\nSo:\n\n__Ward's Distance__ between clusters $C_i$ and $C_j$ is the difference between the total within cluster sum of squares for the two clusters separately, __compared to__ the within cluster sum of squares resulting from merging the two clusters into a new cluster $C_{i+j}$:\n\n$$D_\\text{Ward}(i, j) = \\sum_{x \\in C_i} (x - r_i)^2 + \\sum_{x \\in C_j} (x - r_j)^2  - \\sum_{x \\in C_{i+j}} (x - r_{i+j})^2 $$\n\nwhere $r_i, r_j, r_{i+j}$ are the corresponding cluster centroids.\n\nIn a sense, this cluster distance results in a hierarchical analog of $k$-means.\n\nAs a result, it has properties similar to $k$-means:\n    \n* Less susceptible to noise and outliers\n* Biased toward elliptical clusters\n\nHence it tends to behave more like group-average hierarchical clustering.\n\n## Hierarchical Clustering In Practice\n\nNow we'll look at doing hierarchical clustering in practice, using python.\n\nWe'll use the same synthetic data as we did in the k-means case -- ie.,\nthree \"blobs\" living in 30 dimensions.\n\n::: {#68dd0e36 .cell execution_count=21}\n``` {.python .cell-code}\nX, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n                          center_box=(-10.0, 10.0),random_state=0)\n```\n:::\n\n\nAs a reminder of the raw data here is the visualization: first the raw data, then an embedding into 2-D (using MDS).\n\n::: {#1b9992c8 .cell execution_count=22}\n``` {.python .cell-code}\nsns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-23-output-1.png){width=540 height=389}\n:::\n:::\n\n\n::: {#1abefd4b .cell hide_input='false' slideshow='{\"slide_type\":\"fragment\"}' execution_count=23}\n``` {.python .cell-code}\nimport sklearn.manifold\nimport sklearn.metrics as metrics\neuclidean_dists = metrics.euclidean_distances(X)\nmds = sklearn.manifold.MDS(n_components = 2, max_iter = 3000, eps = 1e-9, random_state = 0,\n                   dissimilarity = \"precomputed\", n_jobs = 1)\nfit = mds.fit(euclidean_dists)\npos = fit.embedding_\nplt.axis('equal')\nplt.scatter(pos[:, 0], pos[:, 1], s = 8);\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-24-output-1.png){width=577 height=411}\n:::\n:::\n\n\nHierarchical clustering is available in __`sklearn`__, but there is a much more fully developed set of tools in the __`scipy`__ package and that is the one to use.\n\n::: {#d47dc851 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=24}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nimport scipy.spatial.distance\n\n# linkages = ['single','complete','average','weighted','ward']\nZ = hierarchy.linkage(X, method = 'single')\n```\n:::\n\n\n::: {#690102de .cell execution_count=25}\n``` {.python .cell-code}\nR = hierarchy.dendrogram(Z)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-26-output-1.png){width=567 height=407}\n:::\n:::\n\n\n## Hierarchical Clustering Real Data\n\nOnce again we'll use the \"20 Newsgroup\" data provided as example data in sklearn.\n\n(http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n\n::: {#538511c7 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=26}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset = 'train', categories = categories)\n```\n:::\n\n\n::: {#db6b5cc0 .cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english', min_df = 4, max_df = 0.8)\ndata = vectorizer.fit_transform(news_data.data).todense()\ndata.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(1781, 9409)\n```\n:::\n:::\n\n\n::: {#c44176ac .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=28}\n``` {.python .cell-code}\n# metrics can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, \n# ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, \n# ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, \n# ‘sqeuclidean’, ‘yule’.\nZ_20ng = hierarchy.linkage(data, method = 'ward', metric = 'euclidean')\nplt.figure(figsize=(14,4))\nR_20ng = hierarchy.dendrogram(Z_20ng, p=4, truncate_mode = 'level', show_leaf_counts=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-29-output-1.png){width=1079 height=361}\n:::\n:::\n\n\n### Selecting the Number of Clusters\n\n::: {#2b8f64e9 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=29}\n``` {.python .cell-code}\nclusters = hierarchy.fcluster(Z_20ng, 3, criterion = 'maxclust')\nprint(clusters.shape)\nclusters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1781,)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray([3, 3, 3, ..., 1, 3, 1], dtype=int32)\n```\n:::\n:::\n\n\n::: {#4075de8d .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=30}\n``` {.python .cell-code}\nmax_clusters = 20\ns = np.zeros(max_clusters+1)\nfor k in range(2, max_clusters+1):\n    clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n    s[k] = metrics.silhouette_score(np.asarray(data), clusters, metric = 'euclidean')\nplt.plot(range(2, len(s)), s[2:], '.-')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-html/cell-31-output-1.png){width=606 height=429}\n:::\n:::\n\n\n::: {#27324191 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=31}\n``` {.python .cell-code}\nprint('Top Terms Per Cluster:')\nk = 5\nclusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\nfor i in range(1,k+1):\n    items = np.array([item for item,clust in zip(data, clusters) if clust == i])\n    centroids = np.squeeze(items).mean(axis = 0)\n    asc_order_centroids = centroids.argsort()#[:, ::-1]\n    order_centroids = asc_order_centroids[::-1]\n    terms = vectorizer.get_feature_names_out()\n    print(f'Cluster {i}:')\n    for ind in order_centroids[:10]:\n        print(f' {terms[ind]}')\n    print('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop Terms Per Cluster:\nCluster 1:\n space\n nasa\n edu\n henry\n gov\n alaska\n access\n com\n moon\n digex\n\nCluster 2:\n ax\n max\n b8f\n g9v\n a86\n 145\n 1d9\n pl\n 2di\n 0t\n\nCluster 3:\n edu\n com\n year\n baseball\n article\n writes\n cs\n team\n game\n university\n\nCluster 4:\n risc\n instruction\n ghhwang\n csie\n set\n nctu\n cisc\n tw\n reduced\n mq\n\nCluster 5:\n windows\n edu\n file\n dos\n com\n files\n card\n drivers\n driver\n use\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "08-Clustering-III-hierarchical_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}