{
  "hash": "0223377252cddaa020ce397dad01485d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Regularization\njupyter: python3\n---\n\n\n\n# Overfitting\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/19-Regression-III-More-Linear.ipynb)\n\nWe have referenced the concept of overfitting previously in our [decision tree](14-Classification-I-Decision-Trees.qmd) and [dimensionality reduction](11-Dimensionality-Reduction-SVD-II.qmd) lectures.\n\nWe know that a model that overfits does not generalize well. In other words, the model does not perform well on data it was not trained on.\n\nWe observe overfitting when the training accuracy of the model is high, while the accuracy on the validation set (or held out test set) stagnates.\n\nIn this lecture we will discuss **regularization**. A technique to help prevent overfitting. We will see how to apply regularization to regression problems. \n\nHowever, regularization is a general technique to ensure models learn broad patterns and is applicable in neural networks.\n\n\n## What is regularization?\n\nWe know regularization is a way to prevent overfitting, but how does this actually work?\n\nThe idea behind regularization is to penalize a model's loss function during training. The added penalty will discourage the model from becoming too complex (i.e., overfitting).\n\nIn regression, our training process was to compute coefficients $\\boldsymbol{\\beta}$ by minimizing\n\n$$\n\\min_{\\boldsymbol{\\beta}} \\Vert X \\boldsymbol{\\beta} - \\mathbf{y}\\Vert_{2}^{2},\n$$\n\n where $X\\in\\mathbb{R}^{m\\times n}$ is the design matrix and $\\mathbf{y}\\in\\mathbb{R}^{m}$ are the dependent variables.\n\n\n---\n\nRegularization adds a function $R(\\boldsymbol{\\beta})$ to the minimization problem. A regularized minimization problem is then: compute $\\boldsymbol{\\beta}$ such that\n\n$$\n\\min_{\\boldsymbol{\\beta}} \\Vert X \\boldsymbol{\\beta} - \\mathbf{y}\\Vert_{2}^{2} + cR(\\boldsymbol{\\beta}).\n$$\n\nThe regularization coefficient $c$ is a hyperparameter that controls the importance of the regularization term $R(\\boldsymbol{\\beta})$.\n\nWe will consider two common forms of $R(\\boldsymbol{\\beta})$:\n\n- $R(\\boldsymbol{\\beta}) = \\Vert \\boldsymbol{\\beta}\\Vert_{2}^{2}$, called **ridge regression**, and\n- $R(\\boldsymbol{\\beta}) = \\Vert \\boldsymbol{\\beta}\\Vert_1$, called **LASSO** regression.\n\n\n## Overview\n\nIn this lecture we will cover:\n\n- situations where regression is needed to help\n- when to use the different types of regression\n- importance of the hyperparameter $c$ \n\n\n# Multicollinearity\n\n>In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy.\n\nWe will see that multicollinearity can be problematic in our regression models.\n\nTo address these issues, we will consider\n\n- What are the potential sources for multicollinearity?\n- What does multicollinearity tell us about our data?\n\nUnderstanding these questions will inform us how regularization can be used to mitigate the issue of multicollinearity.\n\n\n## Sources of Multicollinearity\n\nWe will be working with our design matrix $X\\in\\mathbb{R}^{m\\times n}$, where each row corresponds to an instance of data and the $n$ columns are the $n$ features of the data points.\n\nWe will see that multicollinearity arises when the columns of $X$ are linearly dependent (or nearly linearly dependent).\n\nAs a consequence, the matrix $X^{T}X$ is no longer invertible. However, as we saw in our [Linear Regression](17-Regression-I-Linear.qmd) lecture, the least squares solution still exists. It is just not unique.\n\n---\n\nWhat are the situations where this can happen?\n\nOne clear case is when $m < n$, which means $X$ has more columns than rows. That is, there are  __more features than there are observations__ in the data.\n\nHowever, we can still observe multicollinearity when $m > n$. \n\nThis can happen when the columns of $X$ happen to be linearly dependent because of the nature of the data itself. In particular, this happens when one column is a linear function of the other columns. This means that one independent variable is a linear function of one or more of the others.\n\nUnfortunately, in practice we will run into trouble even if variables are *almost* linearly dependent. \n\n---\n\nTo illustrate the multcollinearity problem, we'll load a standard dataset.\n\nThe [Longley](https://www.statsmodels.org/stable/datasets/generated/longley.html) dataset contains various US macroeconomic variables from 1947–1962.\n\nWe have the following features:\n\n- GNP - GNP (Gross National Product)\n- GNPDEFL - GNP deflator\n- UNEMP - Number of unemployed\n- ARMED - Size of armed forces\n- POP - Population\n- YEAR - Year (1947 - 1962)\n\nWe want to predict:\n\n- TOTEMP - Total Employment\n\n\n--- \n\n::: {#96d7b7b4 .cell execution_count=3}\n``` {.python .cell-code}\nfrom statsmodels.datasets.longley import load_pandas\ny = load_pandas().endog\nX = load_pandas().exog\nX['const'] = 1.0\nX.index = X['YEAR']\ny.index = X['YEAR']\nX.drop('YEAR', axis = 1, inplace = True)\nprint(\"X.head()\")\nprint(X.head())\nprint(\"\\n\\ny.head()\")\nprint(y.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX.head()\n        GNPDEFL       GNP   UNEMP   ARMED       POP  const\nYEAR                                                      \n1947.0     83.0  234289.0  2356.0  1590.0  107608.0    1.0\n1948.0     88.5  259426.0  2325.0  1456.0  108632.0    1.0\n1949.0     88.2  258054.0  3682.0  1616.0  109773.0    1.0\n1950.0     89.5  284599.0  3351.0  1650.0  110929.0    1.0\n1951.0     96.2  328975.0  2099.0  3099.0  112075.0    1.0\n\n\ny.head()\nYEAR\n1947.0    60323.0\n1948.0    61122.0\n1949.0    60171.0\n1950.0    61187.0\n1951.0    63221.0\nName: TOTEMP, dtype: float64\n```\n:::\n:::\n\n\n---\n\n\nAn important warning is issued stating the condition number is large. What does this mean?\n\n::: {#70243fbd .cell execution_count=4}\n``` {.python .cell-code}\nols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    print(ols_results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 TOTEMP   R-squared:                       0.987\nModel:                            OLS   Adj. R-squared:                  0.981\nMethod:                 Least Squares   F-statistic:                     156.4\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):           3.70e-09\nTime:                        07:51:28   Log-Likelihood:                -117.83\nNo. Observations:                  16   AIC:                             247.7\nDf Residuals:                      10   BIC:                             252.3\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGNPDEFL      -48.4628    132.248     -0.366      0.722    -343.129     246.204\nGNP            0.0720      0.032      2.269      0.047       0.001       0.143\nUNEMP         -0.4039      0.439     -0.921      0.379      -1.381       0.573\nARMED         -0.5605      0.284     -1.975      0.077      -1.193       0.072\nPOP           -0.4035      0.330     -1.222      0.250      -1.139       0.332\nconst       9.246e+04   3.52e+04      2.629      0.025    1.41e+04    1.71e+05\n==============================================================================\nOmnibus:                        1.572   Durbin-Watson:                   1.248\nProb(Omnibus):                  0.456   Jarque-Bera (JB):                0.642\nSkew:                           0.489   Prob(JB):                        0.725\nKurtosis:                       3.079   Cond. No.                     1.21e+08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.21e+08. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Condition Number\n\nThe notion of conditioning pertains to the perturbation behavior of a mathematical problem. A well-conditioned problem is one where small changes to the inputs produce small changes to the output. An ill-conditioned problem is one where small changes in the input can produce very large changes in the output. \n\nThe condition number of a matrix provides an indication of how accurately you can compute with it. \n\nA large condition number tells us that our problem is ill conditioned, i.e., small changes to the input can produce very large changes in the output.\n\nA small condition number tells us that our problem is well-conditioned.\n\n---\n\nThe condition number is derived using matrix norms, which is beyond the scope of this course. However, we will use the following definition for the condition number of our matrix\n\n$$\n\\kappa(X) = \\frac{\\sigma_{\\text{max}}}{\\sigma_{\\text{min}}},\n$$\n\nwhere $\\sigma_{\\text{max}}$, $\\sigma_{\\text{min}}$ are the maximum and minimum singular values, respectively, of the design matrix $X$.\n\nThe SVD again provides us with important properties of a matrix.\n\n---\n\nAnother important fact is that\n\n$$\n\\kappa(X^TX) = \\frac{\\sigma_{\\text{max}}^2}{\\sigma_{\\text{min}}^2}.\n$$\n\nThis means that if we have a poorly conditioned data matrix $X$, then $X^TX$ is even more poorly conditioned.\n\nThis is why you should never work directly with $X^TX$ as it can be numerically unstable.\n\n## Normal Equations\n\nTo solve the least-squares problem we solve the normal equations\n\n$$\nX^TX\\boldsymbol{\\beta} = X^Ty.\n$$\n\nThese equations always have at least one solution. However, the *at least one* part is problematic.\n\nIf there are multiple solutions, they are in a sense all equivalent in that they yield the same value of $\\Vert X\\boldsymbol{\\beta} - y\\Vert_2$.\n\nHowever, the actual values of $\\boldsymbol{\\beta}$ can vary tremendously and so it is not clear how best to interpret which solution is actually the best.\n\nWhen does this problem occur? \n\n## Linear Dependence\n\nIt occurs when $X^TX$ is __not invertible.__\n\nThis happens when the columns of $X$ are linearly dependent -- that is, one column can be expressed as a linear combination of the other columns.\n\nIn that case, it is not possible to solve the normal equations by computing $\\hat{\\boldsymbol{\\beta}} \\neq (X^TX)^{-1}X^Ty.$\n\nThis is the simplest kind of __multicollinearity__.\n\n---\n\nWhat are the implications of a matrix not being invertible on the condition number?\n\nIf a matrix $Z = X^{T}X$ is not invertible, there is a zero singular value. This implies\n\n$$\n\\kappa(Z) = \\infty.\n$$\n\nIn other words, the problem of solving an equation with a non-invertible matrix is completely ill-conditioned. In fact, it's a problem that is impossible to solve.\n\n## Near Linear Dependence\n\nNear linear dependence causes problems as well. This can happen, for example, due to  measurement errors. Or when two or more columns are __strongly correlated__.\n\nIn such a situation, we have some column of our design matrix that is __close to__ being a linear combination of the other columns.\n\nWhen these situations occur we will have problems with linear regression.\n\n---\n\nAs a result of near linear dependence, the smallest singular value of the design matrix $X$ will be close to zero. This means that $\\kappa(X)$ will be very large.\n\nThe condition number tells us that a small change in the input to our problem can result in large changes to the output. \n\nThis means that for a design matrix $X$ with near linearly dependent columns, the values we compute for $\\boldsymbol{\\beta}$ in our linear regression can vary significantly.\n\nThis is why we see the addition of a regularization (penalty) term involving $\\boldsymbol{\\beta}$ in the least squares minimization problem. This process *regularizes* the solution $\\boldsymbol{\\beta}$.\n\n\n## Longley Dataset\n\nRecall that the condition number of our data is around $10^8$. \n\nA large condition number is evidence of a problem.\n\nAs a general rule of thumb:\n\n* If the condition number is less than 100, there is no serious problem\nwith multicollinearity.\n* Condition numbers between 100 and 1000 imply moderate to strong multicollinearity.\n* Condition numbers bigger than 1000 indicate severe multicollinearity.\n\n---\n\nLet's look at pairwise scatter plots of the Longley data.\n\n::: {#c8dd439d .cell execution_count=5}\n``` {.python .cell-code}\nsns.pairplot(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-5-output-1.png){width=1181 height=1182}\n:::\n:::\n\n\nWe can see __very__ strong linear relationships between, e.g., __GNP Deflator__, __GNP__, and __Population.__\n\n## Addressing Multicollinearity\n\nHere are two strategies we can employ to address multicollinearity:\n\n1. Ridge Regression\n2. Model Selection via LASSO\n\n::: {.aside}\nPCA also addresses multicollinearity by transforming the correlated features into uncorrelated features. However in this approach you lose the original features, which is less explainable.\n:::\n\n## Ridge Regression\n\nThe first thing to note is that when columns of $X$ are nearly dependent, the components of $\\hat{\\boldsymbol{\\beta}}$ tend to be __large in magnitude__.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#78cddb73 .cell execution_count=6}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(4,2))\nut.centerAxes(ax)\nu = np.array([1, 2])\nv = np.array([4, 1])\nalph = 1.6\nbeta = -1.25\nsum_uv = (alph * u) + (beta * v)\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.text(sum_uv[0]-.5, sum_uv[1]+0.25, r'$\\mathbf{y}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-6-output-1.png){width=331 height=180 fig-align='center'}\n:::\n:::\n\n\nConsider a regression in which we are predicting the point $\\mathbf{y}$ as a linear function of two $X$ columns, which we'll denote $\\mathbf{u}$ and $\\mathbf{v}$.\n:::\n::: {.column width=\"50%\"}\n\n::: {#d86f0b3a .cell execution_count=7}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(4, 2))\nut.centerAxes(ax)\nu = np.array([1, 2])\nv = np.array([4, 1])\nalph = 1.6\nbeta = -1.25\nsum_uv = (alph * u) + (beta * v)\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, alph * u[0], alph * u[1], head_width=0.2, \n         head_length=0.2, length_includes_head = True)\nax.arrow(alph * u[0], alph * u[1], sum_uv[0] - alph * u[0], sum_uv[1] - alph * u[1], \n         head_width=0.2, \n         head_length=0.2, length_includes_head = True, color = 'r')\nax.text(sum_uv[0]-2, sum_uv[1]+0.25, r'$\\beta_1{\\bf u}$+$\\beta_2{\\bf v}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(alph * u[0]+0.25, alph * u[1]-0.25, r'$\\beta_1{\\bf u}$', size=12)\nax.text(-2, 2.75, r'$\\beta_2{\\bf v}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-7-output-1.png){width=331 height=180 fig-align='center'}\n:::\n:::\n\n\nWe determine the coefficients $\\beta_1$ and $\\beta_2$.\n:::\n::::\n\n---\n\nNow consider if the columns of $X$ are __nearly dependent__.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#02d3340d .cell execution_count=8}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(4, 2))\nut.centerAxes(ax)\nu = np.array([2, 1])\nv = np.array([4, 1])\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.text(sum_uv[0]-.5, sum_uv[1]+0.25, r'$\\mathbf{y}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-8-output-1.png){width=331 height=180 fig-align='center'}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#c463e3f5 .cell execution_count=9}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(4, 2))\nut.centerAxes(ax)\nu = np.array([2, 1])\nv = np.array([4, 1])\nalph = 2.675\nbeta = -8.75\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, alph * u[0], alph * u[1], head_width=0.2, \n         head_length=0.2, length_includes_head = True)\nax.arrow(alph * u[0], alph * u[1], sum_uv[0] - alph * u[0], sum_uv[1] - alph * u[1], \n         head_width=0.2, \n         head_length=0.2, length_includes_head = True, color = 'r')\nax.text(sum_uv[0]-2, sum_uv[1]+0.25, r'$\\beta_1{\\bf u}$+$\\beta_2{\\bf v}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(alph * u[0]+0.25, alph * u[1]-0.25, r'$\\beta_1{\\bf u}$', size=12)\nax.text(-2, 2.75, r'$\\beta_2{\\bf v}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-9-output-1.png){width=346 height=180 fig-align='center'}\n:::\n:::\n\n\n:::\n::::\n\nIf you imagine the values of $\\beta_1$ and $\\beta_2$ necessary to create $\\mathbf{y} = \\beta_1{\\bf u}$+$\\beta_2{\\bf v}$, you can see that $\\beta_1$ and $\\beta_2$ will be __very large__ in magnitude.\n\nThis geometric argument illustrates why the regression coefficients will be very large under multicollinearity.\n\nAs a result, the value of $\\Vert\\boldsymbol{\\beta}\\Vert_2$ will be very large.\n\n## Ridge Regression\n\nRidge regression adjusts the least squares regression by shrinking the estimated coefficients towards zero.\n\nThe purpose is to fix the magnitude inflation of $\\Vert\\boldsymbol{\\beta}\\Vert_2$.\n\nTo do this, Ridge regression assumes that the model has no intercept term -- both the response and the predictors have been centered so that $\\beta_0 = 0$.\n\nRidge regression then consists of adding a penalty term to the regression:\n\n$$ \n\\hat{\\boldsymbol{\\beta}} = \\arg \\min_\\boldsymbol{\\beta} \\Vert X\\boldsymbol{\\beta} - y \\Vert_2^2 + c\\Vert\\boldsymbol{\\beta}\\Vert_2^2.\n$$\n\n---\n\nFor any given $c$ this has a closed-form solution in which $\\hat{\\boldsymbol{\\beta}} = (X^TX +cI)^{−1}X^T\\mathbf{y}.$\n\nThe solution to the Ridge regression problem always exists and is unique, even when the data contains multicollinearity.\n\nHere, $c \\geq 0$ is a tradeoff parameter and controls the strength of the penalty term:\n\n* When $c = 0$, we get the least squares estimator: $\\hat{\\boldsymbol{\\beta}} = (X^TX)^{−1}X^T\\mathbf{y}$\n* When $c \\rightarrow \\infty$, we get $\\hat{\\boldsymbol{\\beta}} \\rightarrow 0.$\n* Increasing the value of $c$ forces the norm of $\\hat{\\boldsymbol{\\beta}}$ to decrease, yielding smaller coefficient estimates in magnitude.\n\nFor a finite, positive value of $c$, we are balancing two tasks: fitting\na linear model and shrinking the coefficients.\n\nThe coefficient $c$ is a __hyperparameter__ that controls the model complexity. We typically set $c$ by holding out data, i.e., __cross-validation.__\n\n## Scaling\n\nNote that the penalty term $\\Vert\\boldsymbol{\\beta}\\Vert_2^2$ would be unfair to the different predictors if they are not on the same scale. \n\nTherefore, if we know that the variables are not measured in the same units, we typically first perform unit normal scaling on the columns of $X$ and on $\\mathbf{y}$ (to standardize the predictors), and then perform ridge regression.\n\nNote that by scaling $\\mathbf{y}$ to have zero-mean, we do not need (or include) an intercept in the model.\n\nAnother name for ridge regression is __Tikhanov regularization__. You may see this terminology used in textbooks on optimization.\n\n::: {.aside}\nNormalizing is needed for this specific method. This is in contrast to the previous lecture where we allowed the coefficients to correct for the scaling differences between different units of measure.\n:::\n\n---\n\nHere is the performance of Ridge regression on the Longley data.\n\nWe are training on half of the data and using the other half for testing.\n\n::: {#5798845b .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nnreps = 1000\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']])\ny_std = scaler.fit_transform(y.values.reshape(-1, 1))\n\nnp.random.seed(1)\n\nvals = []\nfor alpha in np.r_[np.array([0]), 10**np.linspace(-8.5, -0.5, 20)]:\n    res = []\n    for rep in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X_std, y_std,\n            test_size=0.5)\n        model = sm.OLS(y_train, X_train)\n        results = model.fit_regularized(alpha = alpha, L1_wt = 0)\n        y_oos_predict = results.predict(X_test)\n        r2_test = r2_score(y_test, y_oos_predict)\n        res.append(r2_test)\n    vals.append([alpha, np.mean(res), np.std(res)/np.sqrt(nreps)])\n\nresults = np.array(vals)\n```\n:::\n\n\n::: {#70eccd03 .cell execution_count=11}\n``` {.python .cell-code}\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.errorbar(np.log10(results[1:][:, 0]), results[1:][:, 1], \n            results[1:][:, 2],\n            label = 'Ridge Regression')\nax.hlines(results[0,1], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dashed',\n          label = 'Without Regularization')\nax.hlines(results[0,1]+results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.hlines(results[0,1]-results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.tick_params(labelsize=12)\nax.set_ylabel('$R^2$', fontsize = 14)\nplt.legend(loc = 'best')\nax.set_xlabel('$\\\\log_{10}(c)$', fontsize = 14)\nax.set_title('Ridge Regression Accuracy on Longley Data', fontsize = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-11-output-1.png){width=547 height=390 fig-align='center'}\n:::\n:::\n\n\n--- \n\nTo sum up the idea behind Ridge regression: \n\n1. There may be many $\\boldsymbol{\\beta}$ values that are consistent with the equations.   \n1. Over-fit $\\boldsymbol{\\beta}$ values tend to have large magnitudes.\n1. We add the regularization term $c \\Vert \\boldsymbol{\\beta}\\Vert_2^2$ to the least squares to avoid those solutions.\n1. We tune $c$ to an appropriate value via cross-validation.\n\n## Model Selection\n\nOf course, one might attack the problem of multicollinearity as follows:\n    \n- Multicollinearity occurs when variables (features) are close to linearly dependent.\n- These variables do not contribute anything *meaningful* to the quality of the model\n- As a result why not simply remove variables from the model that are nearly linearly dependent?\n\nWe create a new model when we remove these variables from our regression.\n\nThis strategy is called **model selection**.\n\n---\n\nOne of the advantages of model selection is __interpretability__: by eliminating variables, we get a clearer picture of the relationship between truly useful features and dependent variables.\n\nHowever, there is a big challenge inherent in model selection. In general, the possibilities to consider are exponential in the number of features.\n\nThat is, if we have $n$ features to consider, then there are $2^n-1$ possible models that incorporate one or more of those features. This space is usually too big to search directly.\n\nCan we use Ridge regression for this problem?\n\n:::: {.fragment}\nRidge regression does not set any coefficients exactly to zero unless $c\\rightarrow \\infty$, in which case they’re all zero. \n\nThis means Ridge regression cannot perform variable selection. Even though it performs well in terms of prediction accuracy, it does not offer a clear interpretation.\n::::\n\n\n## The LASSO\n\nLASSO differs from Ridge regression __only in terms of the norm__ used by the penalty term.\n\n$$ \n\\hat{\\beta} = \\arg \\min_\\beta \\Vert X\\beta - y \\Vert_2^2 + c \\Vert\\beta\\Vert_1.\n$$\n\nHowever, this small change in the norm makes a __big difference__ in practice.\n\nThe nature of the $\\ell_1$ penalty will cause some coefficients to be shrunken to zero exactly.\n\nThis means that LASSO can perform model selection by telling us which variables to keep and which to set aside.\n\nAs $c$ increases, more coefficients are set to zero, i.e., fewer variables are selected.\n\nIn terms of prediction error, LASSO performs comparably to Ridge regression but it has a __big advantage with respect to interpretation.__\n\n---\n\n::: {#f78ba33d .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nnreps = 200\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']])\nX_std = np.column_stack([X_std, np.ones(X_std.shape[0])])\ny_std = scaler.fit_transform(y.values.reshape(-1, 1))\n\nnp.random.seed(1)\n\nvals = []\nmean_params = []\nfor alpha in np.r_[np.array([0]), 10**np.linspace(-5, -0.75, 10)]:\n    res = []\n    params = []\n    for rep in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X_std, y_std,\n            test_size=0.5)\n        model = sm.OLS(y_train, X_train)\n        results = model.fit_regularized(alpha = alpha, L1_wt = 1.0)\n        y_oos_predict = results.predict(X_test)\n        r2_test = r2_score(y_test, y_oos_predict)\n        res.append(r2_test)\n        params.append(results.params)\n    vals.append([alpha, np.mean(res), np.std(res)/np.sqrt(nreps)])\n    mean_params.append(np.r_[alpha, np.mean(params, axis = 0)])\nresults = np.array(vals)\nmean_params = np.array(mean_params)\n```\n:::\n\n\n::: {#83955153 .cell execution_count=13}\n``` {.python .cell-code}\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.errorbar(np.log10(results[1:][:, 0]), results[1:][:, 1], \n            results[1:][:, 2],\n            label = 'LASSO Regression')\nax.hlines(results[0,1], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dashed',\n          label = 'Without Regularization')\nax.hlines(results[0,1]+results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.hlines(results[0,1]-results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.tick_params(labelsize=12)\nax.set_ylabel('$R^2$', fontsize = 14)\n#ax.set_xlim([-4, -1])\nplt.legend(loc = 'best')\nax.set_xlabel('$\\\\log_{10}(c)$', fontsize = 14)\nax.set_title('LASSO Accuracy on Longley Data', fontsize = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-13-output-1.png){width=536 height=390 fig-align='center'}\n:::\n:::\n\n\n---\n\n::: {#b179445e .cell execution_count=14}\n``` {.python .cell-code}\ndf = pd.DataFrame(mean_params, columns = ['$\\log_{10}(c)$', 'GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'const'])\nparam_df = df[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'const']].iloc[1:].copy()\nparam_df.index = np.log10(df.iloc[1:]['$\\log_{10}(c)$'])\n```\n:::\n\n\n::: {#87323b87 .cell execution_count=15}\n``` {.python .cell-code}\nparam_df.plot()\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nplt.title('LASSO Coefficients vs $c$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-revealjs/cell-15-output-1.png){width=991 height=451 fig-align='center'}\n:::\n:::\n\n\n--- \n\nWe can use the statsmodel `smf` sub-module to directly type formulas and expressions in the functions of the models. This allows us to, among other things,\n\n- specify the name of the columns to be used to predict another column\n- remove columns\n- infer the type of the variable (e.g., categorical, numerical)\n- apply functions to columns\n\nThe `smf` submodule makes use of the [patsy](https://patsy.readthedocs.io/en/latest/) package. patsy is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S.\n\nIn the following code cells we will see the syntax that is used to specify columns in the models and how to remove columns from our model\n\n---\n\nHere is an example where we specify the name of the columns to be used to predict another column.\n\n::: {#e43fac45 .cell execution_count=16}\n``` {.python .cell-code}\nX['TOTEMP'] = y\n```\n:::\n\n\n::: {#28fafc53 .cell execution_count=17}\n``` {.python .cell-code}\nmod = smf.ols(formula='TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP', data=X)\nres = mod.fit()   \nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    print(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 TOTEMP   R-squared:                       0.987\nModel:                            OLS   Adj. R-squared:                  0.981\nMethod:                 Least Squares   F-statistic:                     156.4\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):           3.70e-09\nTime:                        07:52:01   Log-Likelihood:                -117.83\nNo. Observations:                  16   AIC:                             247.7\nDf Residuals:                      10   BIC:                             252.3\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   9.246e+04   3.52e+04      2.629      0.025    1.41e+04    1.71e+05\nGNPDEFL      -48.4628    132.248     -0.366      0.722    -343.129     246.204\nGNP            0.0720      0.032      2.269      0.047       0.001       0.143\nUNEMP         -0.4039      0.439     -0.921      0.379      -1.381       0.573\nARMED         -0.5605      0.284     -1.975      0.077      -1.193       0.072\nPOP           -0.4035      0.330     -1.222      0.250      -1.139       0.332\n==============================================================================\nOmnibus:                        1.572   Durbin-Watson:                   1.248\nProb(Omnibus):                  0.456   Jarque-Bera (JB):                0.642\nSkew:                           0.489   Prob(JB):                        0.725\nKurtosis:                       3.079   Cond. No.                     1.21e+08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.21e+08. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n---\n\nThe formula\n\n`formula='TOTEMP ~ GNPDEFL + GNP + UNEMP + ARMED + POP'`\n\nis an R-style formula string that specifies the model.\n\nThe variable `TOTEMP` is the dependent variable, which is Total Employment in the Longley dataset.\n\nThe syntax `~` separates the dependent variable from the independent variables.\n\nThe sytnax `GNPDEFL + GNP + UNEMP + ARMED + POP` are the independent variables, which are GNP Deflator, Gross National Product, Number of Unemployed, Size of the Armed Forces, and Population.\n\n---\n\nThis is an example where we remove columns from the data and exclude the y-intercept.\n\n::: {#a3f2bdeb .cell execution_count=18}\n``` {.python .cell-code}\nmod = smf.ols(formula='TOTEMP ~ GNPDEFL + GNP + UNEMP - 1', data=X)\nres = mod.fit()\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    print(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                 TOTEMP   R-squared (uncentered):                   1.000\nModel:                            OLS   Adj. R-squared (uncentered):              1.000\nMethod:                 Least Squares   F-statistic:                          1.127e+04\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):                    1.92e-22\nTime:                        07:52:01   Log-Likelihood:                         -137.20\nNo. Observations:                  16   AIC:                                      280.4\nDf Residuals:                      13   BIC:                                      282.7\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGNPDEFL      871.0961     25.984     33.525      0.000     814.961     927.231\nGNP           -0.0532      0.007     -8.139      0.000      -0.067      -0.039\nUNEMP         -0.8333      0.496     -1.679      0.117      -1.905       0.239\n==============================================================================\nOmnibus:                        0.046   Durbin-Watson:                   1.422\nProb(Omnibus):                  0.977   Jarque-Bera (JB):                0.274\nSkew:                          -0.010   Prob(JB):                        0.872\nKurtosis:                       2.359   Cond. No.                     2.92e+04\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[3] The condition number is large, 2.92e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n---\n\nThe formula is \n\n`formula='TOTEMP ~ GNPDEFL + GNP + UNEMP - 1'`\n\nWe still have the same dependent variable `TOTEMP`. The independent variables are\n`GNPDEFL + GNP + UNEMP`\n\nThe syntax `-1` removes the intercept from the model. By default, an intercept is included in the model, but - 1 explicitly excludes it.\n\n---\n\n## The LASSO and Longley Data\n\nHere are some of the important observations from using LASSO regression on the Longley dataset:\n\n- We removed the near linearly dependent features from our model.\n- We improved the condition number of the data by 4 orders of magnitude.\n- There is only one variable whose condfidence interval contains 0.\n\n## Flexible Modeling\n\nTo look at model selection in practice, we will consider another famous dataset.\n\nThe Guerry dataset is a collection of historical data used in support of Andre-Michel Guerry’s 1833 \"Essay on the Moral Statistics of France.\"\n\n>Andre-Michel Guerry’s (1833) Essai sur la Statistique Morale\nde la France was one of the foundation studies of modern social science.\nGuerry assembled data on crimes, suicides, literacy and other “moral\nstatistics,” and used tables and maps to analyze a variety of social issues\nin perhaps the first comprehensive study relating such variables.\n\n---\n\n>Guerry’s results were startling for two reasons.\nFirst he showed that rates of crime and suicide remained\nremarkably stable over time, when broken\ndown by age, sex, region of France and even season\nof the year; yet these numbers varied systematically\nacross departements of France. This regularity\nof social numbers created the possibility to\nconceive, for the first time, that human actions in\nthe social world were governed by social laws, just\nas inanimate objects were governed by laws of the\nphysical world.\n\nSource: \"A.-M. Guerry’s Moral Statistics of France: Challenges for Multivariable\nSpatial Analysis\", Michael Friendly.  Statistical Science 2007, Vol. 22, No. 3, 368–399.\n\n--- \n\nHere is the dataset.\n\n::: {#ad4458a3 .cell execution_count=19}\n``` {.python .cell-code}\n# Lottery is per-capital wager on Royal Lottery\ndf = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\ndf = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lottery</th>\n      <th>Literacy</th>\n      <th>Wealth</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>37</td>\n      <td>73</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>51</td>\n      <td>22</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>13</td>\n      <td>61</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80</td>\n      <td>46</td>\n      <td>76</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>79</td>\n      <td>69</td>\n      <td>83</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nHere is a regression using the feature `Literacy`, `Wealth`, and `Region`.\n\n::: {#6670e170 .cell execution_count=20}\n``` {.python .cell-code}\nmod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)\nres = mod.fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):           1.07e-05\nTime:                        07:52:03   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      38.6517      9.456      4.087      0.000      19.826      57.478\nRegion[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nRegion[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nRegion[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nRegion[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy       -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth          0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n---\n\nIn the previous cell, using the patsy syntax determined that elements of `Region` were text strings, so it treated `Region` as a categorical variable. \n\nAlternatively, we could manually enforce this with the syntax on the following slide. Recall that the `-` sign is used to remove columns/variables. Here we remove the intercept from a model by.\n\n---\n\n::: {#d7cf80c0 .cell execution_count=21}\n``` {.python .cell-code}\nres = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):           1.07e-05\nTime:                        07:52:03   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nC(Region)[C]    38.6517      9.456      4.087      0.000      19.826      57.478\nC(Region)[E]    23.2239     14.931      1.555      0.124      -6.501      52.949\nC(Region)[N]    28.6347     13.127      2.181      0.032       2.501      54.769\nC(Region)[S]    34.1034     10.370      3.289      0.002      13.459      54.748\nC(Region)[W]    28.5604     10.018      2.851      0.006       8.616      48.505\nLiteracy        -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth           0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         653.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n---\n\nWe can also apply vectorized functions to the variables in our model. The following cell shows how to do this. In this case we apply the natural log function to the `Literacy` column and use this single column to predict the `Lottery` values.\n\n::: {#9c7349a0 .cell execution_count=22}\n``` {.python .cell-code}\nres = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.161\nModel:                            OLS   Adj. R-squared:                  0.151\nMethod:                 Least Squares   F-statistic:                     15.89\nDate:                Tue, 29 Oct 2024   Prob (F-statistic):           0.000144\nTime:                        07:52:03   Log-Likelihood:                -385.38\nNo. Observations:                  85   AIC:                             774.8\nDf Residuals:                      83   BIC:                             779.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          115.6091     18.374      6.292      0.000      79.064     152.155\nnp.log(Literacy)   -20.3940      5.116     -3.986      0.000     -30.570     -10.218\n==============================================================================\nOmnibus:                        8.907   Durbin-Watson:                   2.019\nProb(Omnibus):                  0.012   Jarque-Bera (JB):                3.299\nSkew:                           0.108   Prob(JB):                        0.192\nKurtosis:                       2.059   Cond. No.                         28.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n## Recap\n\nWe discussed how to perform regularization in linear regression to avoid issues of overfitting due to multicollinearity.\n\nWe discussed how the condition number of a matrix indicates whether we have issues with multicollinearity. \n\nWe saw that large condition numbers indicate multicollinearity.\n\nTo address this issue we considered both Ridge and LASSO regression.\n\nWe also learned about the patsy syntax in the statsmodel package.\n\n",
    "supporting": [
      "19-Regression-III-More-Linear_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}