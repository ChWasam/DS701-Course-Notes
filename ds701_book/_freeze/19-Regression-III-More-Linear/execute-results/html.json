{
  "hash": "63e4cbab4d218709c78bfd6f64a2dc7e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Regularization\njupyter: python3\n---\n\n::: {#202f15d2 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mp\nimport sklearn\nfrom IPython.display import Image, HTML\nimport statsmodels.api as sm\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport laUtilities as ut\n\n%matplotlib inline\n\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom statsmodels.regression.linear_model import OLS\nimport statsmodels.formula.api as smf\n\nimport warnings\n\nnp.random.seed(9876789)\n```\n:::\n\n\nToday, we'll look at some additional aspects of Linear Regression.\n\nOur first topic is multicollinearity.\n\n## Multicollinearity\n\nTo illustrate the multcollinearity problem, we'll load a standard dataset.\n\nThe Longley dataset contains various US macroeconomic variables from 1947–1962.\n\n\n\n\n```{note}\nA good reference for the following is \nhttps://www.sjsu.edu/faculty/guangliang.chen/Math261a/Ch9slides-multicollinearity.pdf\nand\nhttps://www.stat.cmu.edu/~ryantibs/datamining/lectures/17-modr2.pdf\n```\n\n::: {#f170b902 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=2}\n``` {.python .cell-code}\nfrom statsmodels.datasets.longley import load_pandas\ny = load_pandas().endog\nX = load_pandas().exog\nX['const'] = 1.0\nX.index = X['YEAR']\ny.index = X['YEAR']\nX.drop('YEAR', axis = 1, inplace = True)\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GNPDEFL</th>\n      <th>GNP</th>\n      <th>UNEMP</th>\n      <th>ARMED</th>\n      <th>POP</th>\n      <th>const</th>\n    </tr>\n    <tr>\n      <th>YEAR</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1947.0</th>\n      <td>83.0</td>\n      <td>234289.0</td>\n      <td>2356.0</td>\n      <td>1590.0</td>\n      <td>107608.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1948.0</th>\n      <td>88.5</td>\n      <td>259426.0</td>\n      <td>2325.0</td>\n      <td>1456.0</td>\n      <td>108632.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1949.0</th>\n      <td>88.2</td>\n      <td>258054.0</td>\n      <td>3682.0</td>\n      <td>1616.0</td>\n      <td>109773.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1950.0</th>\n      <td>89.5</td>\n      <td>284599.0</td>\n      <td>3351.0</td>\n      <td>1650.0</td>\n      <td>110929.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1951.0</th>\n      <td>96.2</td>\n      <td>328975.0</td>\n      <td>2099.0</td>\n      <td>3099.0</td>\n      <td>112075.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1952.0</th>\n      <td>98.1</td>\n      <td>346999.0</td>\n      <td>1932.0</td>\n      <td>3594.0</td>\n      <td>113270.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1953.0</th>\n      <td>99.0</td>\n      <td>365385.0</td>\n      <td>1870.0</td>\n      <td>3547.0</td>\n      <td>115094.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1954.0</th>\n      <td>100.0</td>\n      <td>363112.0</td>\n      <td>3578.0</td>\n      <td>3350.0</td>\n      <td>116219.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1955.0</th>\n      <td>101.2</td>\n      <td>397469.0</td>\n      <td>2904.0</td>\n      <td>3048.0</td>\n      <td>117388.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1956.0</th>\n      <td>104.6</td>\n      <td>419180.0</td>\n      <td>2822.0</td>\n      <td>2857.0</td>\n      <td>118734.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1957.0</th>\n      <td>108.4</td>\n      <td>442769.0</td>\n      <td>2936.0</td>\n      <td>2798.0</td>\n      <td>120445.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1958.0</th>\n      <td>110.8</td>\n      <td>444546.0</td>\n      <td>4681.0</td>\n      <td>2637.0</td>\n      <td>121950.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1959.0</th>\n      <td>112.6</td>\n      <td>482704.0</td>\n      <td>3813.0</td>\n      <td>2552.0</td>\n      <td>123366.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1960.0</th>\n      <td>114.2</td>\n      <td>502601.0</td>\n      <td>3931.0</td>\n      <td>2514.0</td>\n      <td>125368.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1961.0</th>\n      <td>115.7</td>\n      <td>518173.0</td>\n      <td>4806.0</td>\n      <td>2572.0</td>\n      <td>127852.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1962.0</th>\n      <td>116.9</td>\n      <td>554894.0</td>\n      <td>4007.0</td>\n      <td>2827.0</td>\n      <td>130081.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#83130346 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=3}\n``` {.python .cell-code}\nols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    print(ols_results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 TOTEMP   R-squared:                       0.987\nModel:                            OLS   Adj. R-squared:                  0.981\nMethod:                 Least Squares   F-statistic:                     156.4\nDate:                Sun, 18 Aug 2024   Prob (F-statistic):           3.70e-09\nTime:                        21:42:40   Log-Likelihood:                -117.83\nNo. Observations:                  16   AIC:                             247.7\nDf Residuals:                      10   BIC:                             252.3\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGNPDEFL      -48.4628    132.248     -0.366      0.722    -343.129     246.204\nGNP            0.0720      0.032      2.269      0.047       0.001       0.143\nUNEMP         -0.4039      0.439     -0.921      0.379      -1.381       0.573\nARMED         -0.5605      0.284     -1.975      0.077      -1.193       0.072\nPOP           -0.4035      0.330     -1.222      0.250      -1.139       0.332\nconst       9.246e+04   3.52e+04      2.629      0.025    1.41e+04    1.71e+05\n==============================================================================\nOmnibus:                        1.572   Durbin-Watson:                   1.248\nProb(Omnibus):                  0.456   Jarque-Bera (JB):                0.642\nSkew:                           0.489   Prob(JB):                        0.725\nKurtosis:                       3.079   Cond. No.                     1.21e+08\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.21e+08. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\nWhat does this mean?\n\n>In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy.\n\n(Wikipedia)\n\n### Condition Number\n\nThe condition number being referred to is the condition number of the design matrix.\n\nThat is the $X$ in $X\\beta = y$. \n\nRemember that to solve a least-squares problem $X\\beta = y$, we solve the normal equations\n\n$$X^TX\\beta = X^Ty.$$\n\nThese equations always have at least one solution.\n\nHowever, the \"at least one\" part is problematic!\n\nIf there are multiple solutions, they are in a sense all equivalent in that they yield the same value of $\\Vert X\\beta - y\\Vert$.\n\nHowever, the actual values of $\\beta$ can vary tremendously and so it is not clear how best to interpret the case when $X$ does not have full column rank.\n\nWhen does this problem occur?   Look at the normal equations:\n\n$$X^TX\\beta = X^Ty.$$\n\nIt occurs when $X^TX$ is __not invertible.__\n\nIn that case, we cannot simply solve the normal equations by computing $\\hat{\\beta} = (X^TX)^{-1}X^Ty.$\n\nWhen is $(X^TX)$ not invertible?\n\nThis happens when the columns of $X$ are linearly dependent --\n\nThat is, one column can be expressed as a linear combination of the other columns.\n\nThis is the simplest kind of __multicollinearity__.\n\n### Sources of Multicollinearity\n\nOne obvious case is if $X$ has more columns than rows.   That is, if data have __more features than there are observations__.\n\nThis case is easy to recognize. \n\nHowever, a more insidious case occurs when the columns of $X$ happen to be linearly dependent because of the nature of the data itself.\n\nThis happens when one column is a linear function of the other columns.   \n\nIn other words, one independent variable is a linear function of one or more of the others.\n\nUnfortunately, in practice we will run into trouble even if variables are __almost__ linearly dependent. \n\nNear-dependence causes problems because measurements are not exact, and small errors are magnified when computing $(X^TX)^{-1}$. \n\nSo, more simply, when two or more columns are __strongly correlated__, we will have problems with linear regression.\n\nConsider an experiment with the following predictors:\n    \n$$ x_1 = \\text{arm length} $$\n\n$$ x_2 = \\text{leg length} $$\n\n$$ x_3 = \\text{height} $$\n\n$$ \\dots $$\n\nCondition number is a measure of whether $X$ is __nearly__ lacking full column rank.\n\nIn other words, whether some column is __close to__ being a linear combination of the other columns.\n\nIn this case, the actual values of $\\beta$ can vary a lot due to noise in the measurements.\n\nOne way to say that $X^TX$ is not invertible is to say that it has at least one zero eigenvalue.   \n\nCondition number relaxes this -- it asks if $X^TX$ has a __very small__ eigenvalue (compared to its largest eigenvalue).\n\nAn easy way to assess this is using the SVD of $X$.\n\n(Thank you, \"swiss army knife\"!)\n\nThe eigenvalues of $X^TX$ are the squares of the singular values of $X$.   \n\nSo the condition number of $X$ is defined as:\n\n$$\\kappa(X) = \\frac{\\sigma_{\\mbox{max}}}{\\sigma_{\\mbox{min}}}$$\n\nwhere $\\sigma_{\\mbox{max}}$ and $\\sigma_{\\mbox{min}}$ are the largest and smallest singular values of $X$.\n\nA large condition number is evidence of a problem.   \n\n* If the condition number is less than 100, there is no serious problem\nwith multicollinearity.\n* Condition numbers between 100 and 1000 imply moderate to strong multicollinearity.\n* Condition numbers bigger than 1000 indicate severe multicollinearity.\n\nRecall that the condition number of our data is around $10^8$. \n\nLet's look at pairwise scatterplots of the Longley data:\n\n::: {#9aee0275 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nsns.pairplot(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']]);\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-5-output-1.png){width=1181 height=1182}\n:::\n:::\n\n\nWe can see __very__ strong linear relationships between, eg, __GNP Deflator__, __GNP__, and __Population.__\n\n## Addressing Multicollinearity\n\n\nThere are some things that can be done if it does happen.\n\nWe will review two strategies:\n\n1. Ridge Regression\n2. Model Selection via LASSO\n\n### Ridge Regression\n\nThe first thing to note is that when columns of $X$ are nearly dependent, the components of $\\hat{\\beta}$ tend to be __large in magnitude__.\n\nConsider a regression in which we are predicting the point $\\mathbf{y}$ as a linear function of two $X$ columns, which we'll denote $\\mathbf{u}$ and $\\mathbf{v}$.\n\n::: {#a3fd2b34 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(6,3))\nut.centerAxes(ax)\nu = np.array([1, 2])\nv = np.array([4, 1])\nalph = 1.6\nbeta = -1.25\nsum_uv = (alph * u) + (beta * v)\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.text(sum_uv[0]-.5, sum_uv[1]+0.25, r'$\\mathbf{y}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '');\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-6-output-1.png){width=480 height=254}\n:::\n:::\n\n\nVia least-squares, we determine the coefficients $\\beta_1$ and $\\beta_2$:\n\n::: {#aae649bc .cell hide_input='true' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(6,3))\nut.centerAxes(ax)\nu = np.array([1, 2])\nv = np.array([4, 1])\nalph = 1.6\nbeta = -1.25\nsum_uv = (alph * u) + (beta * v)\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, alph * u[0], alph * u[1], head_width=0.2, \n         head_length=0.2, length_includes_head = True)\nax.arrow(alph * u[0], alph * u[1], sum_uv[0] - alph * u[0], sum_uv[1] - alph * u[1], \n         head_width=0.2, \n         head_length=0.2, length_includes_head = True, color = 'r')\nax.text(sum_uv[0]-2, sum_uv[1]+0.25, r'$\\beta_1{\\bf u}$+$\\beta_2{\\bf v}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(alph * u[0]+0.25, alph * u[1]-0.25, r'$\\beta_1{\\bf u}$', size=12)\nax.text(-2, 2.75, r'$\\beta_2{\\bf v}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '');\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-7-output-1.png){width=480 height=254}\n:::\n:::\n\n\nNow consider if the columns of $X$ are __nearly dependent__: ie, they are almost in the same direction:\n\n::: {#23e9ccb3 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\nax = ut.plotSetup(size=(6,3))\nut.centerAxes(ax)\nu = np.array([2, 1])\nv = np.array([4, 1])\nax.arrow(0, 0, u[0], u[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.arrow(0, 0, v[0], v[1], head_width=0.2, head_length=0.2, length_includes_head = True)\nax.text(sum_uv[0]-.5, sum_uv[1]+0.25, r'$\\mathbf{y}$',size=12)\nax.text(u[0]+0.25, u[1]-0.25, r'${\\bf u}$', size=12)\nax.text(v[0]+0.25, v[1]+0.25, r'${\\bf v}$',size=12)\nut.plotPoint(ax, sum_uv[0], sum_uv[1])\nax.plot(0, 0, '');\n```\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-8-output-1.png){width=480 height=254}\n:::\n:::\n\n\nIf you imagine the values of $\\beta_1$ and $\\beta_2$ necessary to create $\\mathbf{y} = \\beta_1{\\bf u}$+$\\beta_2{\\bf v}$, you can see that $\\beta_1$ and $\\beta_2$ will be __very large__ in magnitude.\n\nThis geometric argument illustrates why the regression coefficients will be very large under multicollinearity.\n\nPut another way, the value of $\\Vert\\beta\\Vert$ will be very large.\n\n### Ridge Regression\n\nRidge regression adjusts least squares regression by shrinking the estimated coefficients towards zero.\n\nThe purpose is to fix the magnitude inflation of $\\Vert\\beta\\Vert$.\n\nTo do this, Ridge regression assumes that the model has no intercept term --\n\nboth the response and the predictors have been centered so that $\\beta_0 = 0$.\n\nRidge regression then consists of adding a penalty term to the regression:\n\n$$ \\hat{\\beta} = \\arg \\min_\\beta \\Vert X\\beta - y \\Vert^2 + \\lambda \\Vert\\beta\\Vert^2.$$\n\nFor any given $\\lambda$ this has a closed-form solution in which $\\hat{\\beta}$ is a linear function of $X$ (just as in ordinary least squares).\n\nThe solution to the Ridge regression problem always exists and is unique, even when the data contains multicollinearity.\n\n\nHere, $\\lambda \\geq 0$ is a tradeoff parameter (amount of shrinkage).\n\nIt controls the strength of the penalty term:\n* When $\\lambda = 0$, we get the least squares estimator: $\\hat{\\beta} = (X^TX)^{−1}X^T\\mathbf{y}$\n* When $\\lambda = \\infty$, we get $\\hat{\\beta} = 0.$\n* Increasing the value of $\\lambda$ forces the norm of $\\hat{\\beta}$ to decrease, yielding smaller coefficient estimates (in magnitude).\n\nFor a finite, positive value of $\\lambda$, we are balancing two tasks: fitting\na linear model and shrinking the coefficients.\n\nSo once again, we have a __hyperparameter__ that controls model complexity:\n* hence, we typically set $\\lambda$ by holding out data, ie, __cross-validation.__\n\nNote that the penalty term $\\Vert\\beta\\Vert^2$ would be unfair to the different predictors, if they are not on the same scale. \n\nTherefore, if we know that the variables are not measured in the same units, we typically first perform unit normal scaling on the columns of $X$ and on $\\mathbf{y}$ (to standardize the predictors), and then perform ridge regression.\n\nNote that by scaling $\\mathbf{y}$ to zero-mean, we do not need (or include) an intercept in the model.\n\nThe general strategy of including extra criteria to improve the behavior of a model is called \"regularization.\"\n\nAccordingly, Ridge regression is also known as __Tikhanov regularization__.\n\nHere is the performance of Ridge regression on the Longley data.\n\nWe are training on half of the data, and using the other half for testing.\n\n::: {#8c6261a0 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nnreps = 1000\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']])\ny_std = scaler.fit_transform(y.values.reshape(-1, 1))\n\nnp.random.seed(1)\n\nvals = []\nfor alpha in np.r_[np.array([0]), 10**np.linspace(-8.5, -0.5, 20)]:\n    res = []\n    for rep in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X_std, y_std,\n            test_size=0.5)\n        model = sm.OLS(y_train, X_train)\n        results = model.fit_regularized(alpha = alpha, L1_wt = 0)\n        y_oos_predict = results.predict(X_test)\n        r2_test = r2_score(y_test, y_oos_predict)\n        res.append(r2_test)\n    vals.append([alpha, np.mean(res), np.std(res)/np.sqrt(nreps)])\n\nresults = np.array(vals)\n```\n:::\n\n\n::: {#46784102 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.errorbar(np.log10(results[1:][:, 0]), results[1:][:, 1], \n            results[1:][:, 2],\n            label = 'Ridge Regression')\nax.hlines(results[0,1], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dashed',\n          label = 'Without Regularization')\nax.hlines(results[0,1]+results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.hlines(results[0,1]-results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.tick_params(labelsize=12)\nax.set_ylabel('$R^2$', fontsize = 14)\nplt.legend(loc = 'best')\nax.set_xlabel('$\\log_{10}(\\lambda)$', fontsize = 14)\nax.set_title('Ridge Regression Accuracy on Longley Data', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:15: SyntaxWarning: invalid escape sequence '\\l'\n<>:15: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_16926/1845775655.py:15: SyntaxWarning: invalid escape sequence '\\l'\n  ax.set_xlabel('$\\log_{10}(\\lambda)$', fontsize = 14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-10-output-2.png){width=547 height=390}\n:::\n:::\n\n\nTo sum up the idea behind Ridge regression: \n\n1. There may be many $\\beta$ values that are (approximately) consistent with the equations.   \n2. However over-fit $\\beta$ values tend to have large magnitudes \n3. We apply shrinkage to avoid those solutions\n4. We do so by tuning $\\lambda$ via cross-validation\n\n### Model Selection\n\nOf course, one might attack the problem of multicollinearity as follows:\n    \n1. Multicollinearity occurs because there are near-dependences among variables (features)\n2. The extra variables do not contribute anything \"meaningful\" to the quality of the model\n3. Hence, why not simply remove variables from the model that are causing dependences?\n\nIf we remove variables from our regression, we are creating a new model.\n\nHence this strategy is called \"model selection.\"\n\nOne of the advantages of model selection is __interpretability__: by eliminating variables, we get a clearer picture of the relationship between truly useful features and dependent variables.\n\nHowever, there is a big challenge inherent in model selection:\n    \nin general, the possibilities to consider are exponential in the number of features.\n\nThat is, if we have $n$ features to consider, then there are $2^n-1$ possible models that incorporate one or more of those features.\n\nThis space is usually too big to search directly.\n\nCan we use Ridge regression for this problem?\n\nRidge regression does not set any coefficients exactly to zero unless $\\lambda = \\infty$ (in which case they’re all zero). \n\nHence, Ridge regression cannot perform variable selection, and even though it performs well in terms of prediction accuracy, it does not offer a clear interpretation\n\n### The LASSO\n\nLASSO differs from Ridge regression __only in terms of the norm__ used by the penalty term.\n\n__Ridge regression:__\n\n$$ \\hat{\\beta} = \\arg \\min_\\beta \\Vert X\\beta - y \\Vert^2 + \\lambda \\Vert\\beta\\Vert_2^2.$$\n\n__LASSO:__\n\n$$ \\hat{\\beta} = \\arg \\min_\\beta \\Vert X\\beta - y \\Vert^2 + \\lambda \\Vert\\beta\\Vert_1.$$\n\nHowever, this small change in the norm makes a __big difference__ in practice.\n\nThe nature of the $\\ell_1$ penalty will cause some coefficients to be shrunken to zero exactly!\n\nThis means that LASSO can perform model selection: it can tell us which variables to keep and which to set aside.\n\nAs $\\lambda$ increases, more coefficients are set to zero (fewer variables are selected).\n\nIn terms of prediction error, LASSO performs comparably to Ridge regression, \n\n... but it has a __big advantage with respect to interpretation.__\n\n::: {#6d603359 .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\nnreps = 200\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP']])\nX_std = np.column_stack([X_std, np.ones(X_std.shape[0])])\ny_std = scaler.fit_transform(y.values.reshape(-1, 1))\n\nnp.random.seed(1)\n\nvals = []\nmean_params = []\nfor alpha in np.r_[np.array([0]), 10**np.linspace(-5, -0.75, 10)]:\n    res = []\n    params = []\n    for rep in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X_std, y_std,\n            test_size=0.5)\n        model = sm.OLS(y_train, X_train)\n        results = model.fit_regularized(alpha = alpha, L1_wt = 1.0)\n        y_oos_predict = results.predict(X_test)\n        r2_test = r2_score(y_test, y_oos_predict)\n        res.append(r2_test)\n        params.append(results.params)\n    vals.append([alpha, np.mean(res), np.std(res)/np.sqrt(nreps)])\n    mean_params.append(np.r_[alpha, np.mean(params, axis = 0)])\nresults = np.array(vals)\nmean_params = np.array(mean_params)\n```\n:::\n\n\n::: {#bedabb5e .cell hide_input='true' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.errorbar(np.log10(results[1:][:, 0]), results[1:][:, 1], \n            results[1:][:, 2],\n            label = 'LASSO Regression')\nax.hlines(results[0,1], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dashed',\n          label = 'Without Regularization')\nax.hlines(results[0,1]+results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.hlines(results[0,1]-results[0,2], np.log10(results[1, 0]), \n           np.log10(results[-1, 0]), linestyles = 'dotted')\nax.tick_params(labelsize=12)\nax.set_ylabel('$R^2$', fontsize = 14)\n#ax.set_xlim([-4, -1])\nplt.legend(loc = 'best')\nax.set_xlabel('$\\log_{10}(\\lambda)$', fontsize = 14)\nax.set_title('LASSO Accuracy on Longley Data', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:16: SyntaxWarning: invalid escape sequence '\\l'\n<>:16: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_16926/406416369.py:16: SyntaxWarning: invalid escape sequence '\\l'\n  ax.set_xlabel('$\\log_{10}(\\lambda)$', fontsize = 14)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-12-output-2.png){width=536 height=390}\n:::\n:::\n\n\n::: {#0cd51eba .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\ndf = pd.DataFrame(mean_params, columns = ['$\\log_{10}(\\lambda)$', 'GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'const'])\nparam_df = df[['GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'const']].iloc[1:].copy()\nparam_df.index = np.log10(df.iloc[1:]['$\\log_{10}(\\lambda)$'])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:1: SyntaxWarning: invalid escape sequence '\\l'\n<>:3: SyntaxWarning: invalid escape sequence '\\l'\n<>:1: SyntaxWarning: invalid escape sequence '\\l'\n<>:3: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_16926/1206425442.py:1: SyntaxWarning: invalid escape sequence '\\l'\n  df = pd.DataFrame(mean_params, columns = ['$\\log_{10}(\\lambda)$', 'GNPDEFL', 'GNP', 'UNEMP', 'ARMED', 'POP', 'const'])\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_16926/1206425442.py:3: SyntaxWarning: invalid escape sequence '\\l'\n  param_df.index = np.log10(df.iloc[1:]['$\\log_{10}(\\lambda)$'])\n```\n:::\n:::\n\n\n::: {#2117c4bd .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nparam_df.plot()\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nplt.title('LASSO Coefficients vs $\\lambda$');\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:3: SyntaxWarning: invalid escape sequence '\\l'\n<>:3: SyntaxWarning: invalid escape sequence '\\l'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_16926/612125094.py:3: SyntaxWarning: invalid escape sequence '\\l'\n  plt.title('LASSO Coefficients vs $\\lambda$');\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](19-Regression-III-More-Linear_files/figure-html/cell-14-output-2.png){width=768 height=451}\n:::\n:::\n\n\nWe can use another version of the module that can directly type formulas and expressions in the functions of the models.\n\n\nWe can specify the name of the columns to be used to predict another column, remove columns, etc.\n\n\n\n\n## Flexible Modeling\n\nTo look at model selection in practice, we will consider another famous dataset.\n\nThe Guerry dataset is a collection of historical data used in support of Andre-Michel Guerry’s 1833 \"Essay on the Moral Statistics of France.\"\n\n>Andre-Michel Guerry’s (1833) Essai sur la Statistique Morale\nde la France was one of the foundation studies of modern social science.\nGuerry assembled data on crimes, suicides, literacy and other “moral\nstatistics,” and used tables and maps to analyze a variety of social issues\nin perhaps the first comprehensive study relating such variables.\n\nWikipedia\n\n>Guerry’s results were startling for two reasons.\nFirst he showed that rates of crime and suicide remained\nremarkably stable over time, when broken\ndown by age, sex, region of France and even season\nof the year; yet these numbers varied systematically\nacross departements of France. This regularity\nof social numbers created the possibility to\nconceive, for the first time, that human actions in\nthe social world were governed by social laws, just\nas inanimate objects were governed by laws of the\nphysical world.\n\nSource: \"A.-M. Guerry’s Moral Statistics of France: Challenges for Multivariable\nSpatial Analysis\", Michael Friendly.  Statistical Science 2007, Vol. 22, No. 3, 368–399.\n\n::: {#fccbac81 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=17}\n``` {.python .cell-code}\n# Lottery is per-capital wager on Royal Lottery\ndf = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\ndf = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lottery</th>\n      <th>Literacy</th>\n      <th>Wealth</th>\n      <th>Region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>37</td>\n      <td>73</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>51</td>\n      <td>22</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>13</td>\n      <td>61</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80</td>\n      <td>46</td>\n      <td>76</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>79</td>\n      <td>69</td>\n      <td>83</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#40c7d5ab .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=18}\n``` {.python .cell-code}\nmod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)\nres = mod.fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Sun, 18 Aug 2024   Prob (F-statistic):           1.07e-05\nTime:                        21:43:04   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      38.6517      9.456      4.087      0.000      19.826      57.478\nRegion[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nRegion[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nRegion[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nRegion[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy       -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth          0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n__Categorical variables__\n\nPatsy is the name of the interpreter that parses the formulas.\n\nLooking at the summary printed above, notice that patsy determined that elements of Region were text strings, so it treated Region as a categorical variable. \n\nPatsy‘s default is also to include an intercept, so we automatically dropped one of the Region categories.\n\n__Removing variables__\n\nThe “-” sign can be used to remove columns/variables. For instance, we can remove the intercept from a model by:\n\n::: {#0463df06 .cell slideshow='{\"slide_type\":\"-\"}' execution_count=19}\n``` {.python .cell-code}\nres = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Sun, 18 Aug 2024   Prob (F-statistic):           1.07e-05\nTime:                        21:43:04   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nC(Region)[C]    38.6517      9.456      4.087      0.000      19.826      57.478\nC(Region)[E]    23.2239     14.931      1.555      0.124      -6.501      52.949\nC(Region)[N]    28.6347     13.127      2.181      0.032       2.501      54.769\nC(Region)[S]    34.1034     10.370      3.289      0.002      13.459      54.748\nC(Region)[W]    28.5604     10.018      2.851      0.006       8.616      48.505\nLiteracy        -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth           0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         653.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n__Functions__\n\nWe can also apply vectorized functions to the variables in our model:\n\n::: {#609d96c5 .cell slideshow='{\"slide_type\":\"-\"}' execution_count=20}\n``` {.python .cell-code}\nres = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()\nprint(res.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.161\nModel:                            OLS   Adj. R-squared:                  0.151\nMethod:                 Least Squares   F-statistic:                     15.89\nDate:                Sun, 18 Aug 2024   Prob (F-statistic):           0.000144\nTime:                        21:43:04   Log-Likelihood:                -385.38\nNo. Observations:                  85   AIC:                             774.8\nDf Residuals:                      83   BIC:                             779.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          115.6091     18.374      6.292      0.000      79.064     152.155\nnp.log(Literacy)   -20.3940      5.116     -3.986      0.000     -30.570     -10.218\n==============================================================================\nOmnibus:                        8.907   Durbin-Watson:                   2.019\nProb(Omnibus):                  0.012   Jarque-Bera (JB):                3.299\nSkew:                           0.108   Prob(JB):                        0.192\nKurtosis:                       2.059   Cond. No.                         28.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "19-Regression-III-More-Linear_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}