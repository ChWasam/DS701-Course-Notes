{
  "hash": "591c3fe1ec06d6f420f35568d47a81ba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Gaussian Mixture Models\njupyter: python3\n---\n\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/09-Clustering-IV-GMM-EM.ipynb)\n\n::: {#a17ce8ba .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\n<center>\n\n<img src=\"figs/L09-MultivariateNormal.png\" width=\"70%\">\n\n</center>\n\n## From Hard to Soft Clustering\n\n__So far,__ we have seen how to cluster objects using $k$-means: \n\n1. start with an initial set of cluster centers,\n2. assign each object to its closest cluster center, and \n3. recompute the centers of the new clusters. \n4. Repeat 2 $\\rightarrow$ 3 until convergence.\n\n__Note__ that in $k$-means, every object is assigned to a **single** cluster. \n\nThis is called __hard__ assignment.\n\nHowever, there may be cases were we either __cannot__ use hard assignments or we do not __want__ to do it! \n\nIn particular, we may believe that the best description of the data is a set of __overlapping__ clusters.\n\nFor example: \n\nImagine that we believe society consists of just __two__ kinds of individuals: poor, or rich.\n\nLet's think about how we might model society as a mixture of poor and rich, when viewed in terms of age.\n\nSay we sample 20,000 rich individuals, and 20,000 poor individuals, and get the following histograms:\n\n::: {#4d200267 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=2}\n``` {.python .cell-code}\n# original inspiration for this example from\n# https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf\nfrom scipy.stats import multivariate_normal\nnp.random.seed(4)\ndf = pd.DataFrame(multivariate_normal.rvs(mean = np.array([37, 45]), cov = np.diag([196, 121]), size = 20000),\n                   columns = ['poor', 'rich'])\ndf.hist(bins = range(80), sharex = True);\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-3-output-1.png){width=575 height=431}\n:::\n:::\n\n\nWe find that ages of the poor set have mean 37 with standard deviation 14,\n\nwhile the ages of the rich set have mean 45 with standard deviation 11.\n\n::: {#a9b012e0 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-4-output-1.png){width=622 height=455}\n:::\n:::\n\n\nClearly, viewed along the age dimension, there are two clusters that overlap.\n\nFurthermore, given some particular individual at a given age, say 25, we cannot say for sure which cluster they belong to.  \n\nRather, we will use _probability_ to quantify our uncertainty about the cluster that any single individual belongs to.\n\nThus, we could say that a given individual (\"John Smith\", age 25) belongs to the _rich_ cluster with some probability, and the _poor_ cluster with some different probability.\n\nNaturally we expect the probabilities for John Smith to sum up to 1.\n\nThis is called __soft assignment,__ and a clustering using this principle is called __soft clustering.__\n\nMore formally, we say that an object can belong to each particular cluster with some probability, such that the sum of the probabilities adds up to 1 for each object. \n\nFor example, assuming that we have two clusters $C_1$ and $C_2$, we can have that an object $x_1$ belongs to $C_1$ with probability $0.3$ and to $C_2$ with probability $0.7$.\n\nNote that the distribution over $C_1$ and $C_2$ only refers to object $x_1$.\n\nThus, it is a __conditional__ probability:\n\n$$P(C_1 \\,|\\, x_1) = 0.3$$\n$$P(C_2 \\,|\\, x_1) = 0.7$$\n\nAnd to return to our previous example:\n\n$$P(\\text{rich}\\,|\\,\\text{age 25}) + P(\\text{poor}\\,|\\,\\text{age 25}) = 1 $$\n\n## Mixtures of Gaussians\n\nWe're going to consider a particular model for each cluster: the Gaussian (or Normal) distribution.\n\n$${\\displaystyle f(x\\;|\\;\\mu ,\\sigma ^{2})={\\frac {1}{\\sqrt {2\\sigma ^{2}\\pi }}}\\;exp({-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}}})$$\n\n<center>\n\n<img src=\"figs/L09-Normal_Distribution_PDF.png\" width=\"60%\">\n\n</center>\n\nBy <a href=\"//commons.wikimedia.org/wiki/User:Inductiveload\" title=\"User:Inductiveload\">Inductiveload</a> - self-made, Mathematica, Inkscape, Public Domain, <a href=\"https://commons.wikimedia.org/w/index.php?curid=3817954\">https://commons.wikimedia.org/w/index.php?curid=3817954</a>\n\nYou can see that, for example, this is a reasonable model for the distribution of ages in each of the two population groups (rich and poor).\n\nIn the case of the population example, we have a single feature: age.   \n\nHow do we use a Gaussian when we have multiple features?\n\nThe answer is that we use a __multivariate Gaussian.__\n\n<center>\n\n<img src=\"figs/L09-MultivariateNormal.png\" width=\"70%\">\n\n</center>\n\nBy <a href=\"//commons.wikimedia.org/wiki/User:Bscan\" title=\"User:Bscan\">Bscan</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"http://creativecommons.org/publicdomain/zero/1.0/deed.en\" title=\"Creative Commons Zero, Public Domain Dedication\">CC0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=25235145\">https://commons.wikimedia.org/w/index.php?curid=25235145</a>\n\nNow each point is a vector in $n$ dimensions: $\\mathbf{x} = (x_1, \\dots, x_n)^T$.  \n\nAs a reminder, the multivariate Gaussian has a pdf (density) function of:\n\n$$\nf(x_{1},\\ldots ,x_{n})={\\frac  {1}{{\\sqrt  {(2\\pi )^{{n}}\n\\Vert{\\boldsymbol  \\Sigma }\\Vert}}}}\\exp \\left(-{\\frac  {1}{2}}({{\\mathbf  x}}-{{\\boldsymbol  \\mu }})^{{\\mathrm  {T}}}{{\\boldsymbol  \\Sigma }}^{{-1}}({{\\mathbf  x}}-{{\\boldsymbol  \\mu }})\\right)\n$$\n\nRecall also that the shape of a multivariate Gaussian -- the direction of its axes and the width along each axis -- is determined by the __covariance matrix $\\Sigma$__.\n\nThe covariance matrix is the multidimensional analog of the variance.   It determines the extent to which vector components are correlated.\n\nFor example, let's say we are looking to classify cars based on their model year and miles per gallon (mpg).  \n\nTo illustrate a particular model, let us consider the properties of cars produced in the US, Europe, and Asia.\n\n<!-- image credit: https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf -->\n<center>\n\n<img src=\"figs/L09-multivariate-example.png\" width=\"90%\">\n\n</center>\n\nIt seems that the data can be described (roughly) as a mixture of __three__ __multivariate__ Gaussian distributions.\n\nThe general situation is that we assume the data was generated according to a collection of arbitrary Gaussian distributions.\n\nThis is called a __Gaussian Mixture Model.__\n\nAka a \"GMM\".\n\n<!-- image credit: http://autonlab.org/_media/tutorials/gmm14.pdf  and\nhttps://web.iitd.ac.in/~sumeet/GMM_said_crv10_tutorial.pdf -->\n\n\n<div style = \"float: left; width: 45%; text-align: center;\">\n    \n<img src=\"figs/L09-general-GMM.png\" width=\"45%\">\nEllipsoid Representation\n</div>\n\n<div style = \"float: left; width: 55%; text-align: center;\">\n    \n<img src=\"figs/L09-GMM-density.png\"  style=\"width:55%\">\nDensity\n</div>\n\nA Gaussian Mixture Model is defined by:\n    \n$$ w_i, \\mu_i, \\Sigma_i,\\;\\; i = 1,\\dots,k$$\n\nWhere $w_i$ is the prior probability (weight) of the $i$th Gaussian, such that \n\n$$\\sum_i w_i = 1,\\;\\;\\; 0\\leq w_i\\leq 1.$$\n\nIntuitively, $w_i$ tells us \"what fraction of the data comes from Gaussian $i$.\"\n\nThen the probability density at any point $x$ is given by:\n    \n$$ p(x) = \\sum_i w_i \\cdot \\mathcal{N}(x\\,|\\, \\mu_i, \\Sigma_i) $$\n\n## Learning the Parameters of a GMM\n\nThis model is all very well, but how do we learn the parameters of such a model, given some data?\n\nThat is, assume we are told there are $k$ clusters.   \n\nFor each $i$ in $1, \\dots, k$, how do we estimate the \n* cluster probability $w_i$,\n* cluster mean $\\mu_i$, and \n* cluster covariance  $\\Sigma_i$?\n\nThere are a variety of ways of finding the best $(w_i, \\mu_i, \\Sigma_i)\\;\\;\\;i = 1,\\dots,k$.\n\nWe will consider the most popular method:  __Expectation Maximization (EM).__\n\nThis is another famous algorithm, in the same \"super-algorithm\" league as $k$-means.\n\nEM is formulated using a probabilistic model for data.   It can solve a problem like:\n\n> Given a set of data points and a parameter $k$, find the $(w_i, \\mu_i, \\Sigma_i)\\;\\;i = 1,\\dots,k$ that __maximizes the likelihood of the data__ assuming a GMM with those parameters.\n\n(It can also solve lots of other problems involving maximizing likelihood of data under a model.)\n\nHowever, note that problems of this type are often NP-hard.  \n\nEM only guarantees that it will find a __local__ optimum of the objective function.\n\n## Probabilities We Will Use\n\nAt a high level, EM for the GMM problem has strong similarities to $k$-means.\n\nHowever, there are two main differences:\n\n1. The $k$-means problem posits a **hard** assignment of objects to clusters, while GMM uses __soft__ assignment.\n2. The parameters of the soft assignment are chosed based on a __probability model__ for the data.\n\nLet's start by reviewing the situation probabilistically.\n\nAssume a set data points  $x_1, x_2,\\ldots,x_n$ in  a $d$ dimensional space. \n\nAlso assume a set of $k$ clusters $C_1, C_2, \\ldots, C_k$.   Each cluster is assumed to follow a Gaussian distribution:\n\n$$ C_i \\sim {\\mathcal N}({\\mathbf \\mu_i},{\\mathbf \\Sigma_i}) $$\n\nWe will be working with conditional probabilities.\n\nFirst, \n\n$P(x_i\\,|\\,C_j)$ is the probability of seeing data point $x_i$ when sampling from cluster $C_j$.  \n\nThat is, it is the value of a Gaussian pdf at the point $x_i$, for a Gaussian with parameters $({\\mathbf \\mu_j},{\\mathbf \\Sigma_j})$.\n\nClearly, if I give you the cluster parameters, it is a straightforward thing to compute this conditional probability.\n\nIn our example: $P(\\text{age 25}\\,|\\,\\text{rich})$\n\n::: {#91ba9608 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\nplt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro')\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-5-output-1.png){width=622 height=455}\n:::\n:::\n\n\nWe will also work with $P(C_j\\,|\\,x_i)$.\n\nThis is the probability that a data point at $x_i$ was drawn from cluster $C_j$.   \n\nThat is, the data point could have been drawn from any of the $k$ clusters -- what is the probability it was drawn from $C_j$ in particular?\n\nIn our example: $P(\\text{rich}\\,|\\,\\text{age 25})$\n\nHow can we compute $P(C_j\\,|\\,x_i)$?\n\nNote that the reverse conditional probability is easy to compute -- so this is a job for __Bayes' Rule!__\n\n$$ P(C_j\\,|\\,x_i)=\\frac{P(x_i\\,|\\,C_j)}{P(x_i)}P(C_j)$$\n\n::: {#ae4c7c73 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\nplt.plot(25, norm.pdf(25, loc = 37, scale = 14), 'ko', markersize = 8)\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\nplt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro', markersize = 8)\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14);\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-6-output-1.png){width=622 height=455}\n:::\n:::\n\n\n$$ P(\\text{rich}\\,|\\,\\text{age 25}) = \\frac{\\text{red}}{\\text{red} \\cdot P(\\text{rich}) + \\text{black} \\cdot P(\\text{poor})} \\cdot P(\\text{rich}) $$\n\nFinally, we will also need to estimate the parameters of each Gaussian, given some data.\n\nThis is also an easy problem.   \n\nFor example, the best estimate for the mean of a Gaussian, given some data, is the __average__ of the data points.\n\nLikewise, there is a simple formula for the covariance.\n\nThese are called __Maximum Likelihood Estimates__ (MLE) of the parameters.\n\nOften we use $\\mathbf \\theta$ to denote \"all the parameters of the model.\"   \n\nIn this case $\\mathbf \\theta_i = (w_i, {\\mathbf \\mu_i},{\\mathbf \\Sigma_i})$.\n\n## Expectation Maximization for GMM -- The Algorithm\n\nOK, now we have all the necessary pieces.   Here is the algorithm.\n\n__Initialization__: \n\nStart with an initial set of clusters  $C_1^1, C_2^1, \\ldots, C_k^1$ and the initial probabilities that a random point belongs to each cluster $P(C_1), P(C_2), \\ldots, P(C_k)$.\n\nThe result will be sensitive to this choice, so a good (and fast) initialization procedure is $k$-means.\n\n<!-- Source http://bengio.abracadoudou.com/lectures/gmm.pdf -->\n\n<center>\n\n<img src=\"figs/L09-EM-E-step.png\" width=\"60%\">\n\n</center>\n\n*Step 1* (__Expectation__): For each point $x_i$, compute the probability that it belongs to each cluster $C_j$:\n\n$$ P(C_j\\,|\\,x_i)=\\frac{P(x_i\\,|\\,C_j)}{P(x_i)}\\,P(C_j)$$\n    \n(Thank you, Rev. Bayes!)\n\nThis is called the _posterior_ probability of $C_j$ given $x_i$.\n\nWe know how to compute everything on the right.\n\nNote that: $P(x_i) = \\sum_{j=1}^k P(x_i\\,|\\,C_j)\\,P(C_j)$\n\n<!-- Source http://bengio.abracadoudou.com/lectures/gmm.pdf -->\n\n<center>\n\n<img src=\"figs/L09-EM-M-step.png\" width=\"60%\">\n\n</center>\n\n*Step 2* __(Maximization)__: Using the cluster membership probabilities computed in the previous step, compute new clusters (parameters) and cluster probabilities.\n\nThis is easy, using maximum likelihood estimates of the parameters $\\theta$.\n\n$$w_j = P(C_j) = \\frac{1}{n}\\sum_{i=1}^n P(C_j\\,|\\,x_i)$$\n\nLikewise, compute new parameters for the clusters $C_1, C_2, \\ldots, C_n$ using MLE.\n\n__Repeat__ Steps 1 and 2 until stabilization.\n\nLet's pause for a minute and compare GMM/EM with $k$-means.\n\nGMM/EM:\n\n1. Initialize randomly or using some rule\n2. Compute the probability that each point belongs in each cluster\n3. Update the clusters (weights, means and variances).\n4. Repeat 2-3 until convergence.\n\n$k$-means:\n\n1. Initialize randomly or using some rule\n2. Assign each point to a single cluster\n3. Update the clusters (means).\n4. Repeat 2-3 until convergence.\n\nFrom a practical standpoint, the main difference is that in GMM, data points do not belong to a __single__ cluster, but have some probability of belonging to __each__ cluster.\n\nIn other words, GMM uses soft assignment.\n\nFor that reason, GMM is also sometimes called __soft $k$-means.__\n\nHowever, there is also an important conceptual difference. \n\nThe GMM starts by making an __explicit assumption__ about how the data were generated.  \n\nIt says: \"the data came from a collection of multivariate Gaussians.\"\n\nNote that we made no such assumption when we came up with the $k$-means problem.   In that case, we simply defined an objective function and declared that it was a good one.\n\nNonetheless, it appears that we were making a sort of Gaussian assumption when we formulated the $k$-means objective function.   However, __we didn't explicitly state it.__\n\nThe point is that because the GMM makes its assumptions explicit, we can\n\n* examine them and think about whether they are valid\n* replace them with different assumptions if we wish\n\nFor example, it is perfectly possible to replace the Gaussian assumption with some other probility distribution.   As long as we can estimate the parameters of such distributions from data (eg, have MLEs), we can use EM in that case as well.\n\n## Instantiating EM with the Gaussian Model\n\nIf we model each cluster as a multi-dimensional Gaussian, then we can instatiate every part of\nthe algorithm. \n\nThis is the GMM (Gaussian Mixture Model) algorithm implemented in *sklearn.mixture* module.\n\nIn that case $C_i$ is represented by $(\\mu_i, \\Sigma_i)$ and in EM Step 1 we compute:\n\n$$ P(x_i|C_j) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma_j|}} exp (-\\frac{1}{2}(x_i-\\mu_j)^T\\Sigma_j^{-1}(x_i-\\mu_j))$$\n\nIn EM Step 2, we estimate the parameters of the Gaussian using the appropriate MLEs:\n\n$$\\mu_j'=\\frac{\\sum_{i=1}^n P(C_j|x_i) x_i}{\\sum_{i=1}^n P(C_j|x_i)}$$\n\nand\n\n$$\\Sigma_j = \\frac{\\sum_{i=1}^n P(C_j|x_i) (x_i-\\mu_j')(x_i-\\mu_j')^T}{\\sum_{i=1}^n P(C_j|x_i)}$$\n\nA final statement about EM generally.   EM is a versatile algorithm that can be used in many other settings.  What is the main idea behind it?\n\nNotice that the problem definition only required that we find the clusters, $C_i$, meaning that we were to find the $(\\mu_i, \\Sigma_i)$.\n\nHowever, the EM algorithm posited that we should find as well the $P(C_j|x_i)$, that is, the probability that each point is a member of each cluster.\n\nThis is the true heart of what EM does.   \n\nThe idea is called \"data augmentation.\"\n\nBy __adding parameters__ to the problem, it actually finds a way to make the problem solvable!\n\nThese parameters don't show up in the solution.  They are sometimes called \"hidden parameters.\"\n\nSo the basic strategy for using EM is: think up some __additional__ information which, if you had it, would make the problem solvable.\n\nFigure out how to estimate the additional information from a solved problem, and put the two steps into a loop.\n\nHere is an example using **GMM**.\n\nWe're going to create two clusters, one spherical, and one highly skewed.\n\n::: {#13e792d0 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=6}\n``` {.python .cell-code}\n# Number of samples of larger component\nn_samples = 1000\n\n# C is a transfomation that will make a heavily skewed 2-D Gaussian\nC = np.array([[0.1, -0.1], [1.7, .4]])\n\nprint(f'The covariance matrix of our skewed cluster will be:\\n {C.T@C}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe covariance matrix of our skewed cluster will be:\n [[2.9  0.67]\n [0.67 0.17]]\n```\n:::\n:::\n\n\n::: {#642ff700 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=7}\n``` {.python .cell-code}\n# make the sample deterministic\nnp.random.seed(0)\n\n# now we construct a data matrix that has n_samples from the skewed distribution,\n# and n_samples/2 from a symmetric distribution offset to position (-4, 2)\nX = np.r_[(np.random.randn(n_samples, 2) @ C),\n          .7 * np.random.randn(n_samples//2, 2) + np.array([-4, 2])]\n```\n:::\n\n\n::: {#ba4e7dcb .cell hide_input='true' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nplt.scatter(X[:, 0], X[:, 1], s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-9-output-1.png){width=540 height=389}\n:::\n:::\n\n\n::: {#1437b840 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=9}\n``` {.python .cell-code}\n# Fit a mixture of Gaussians with EM using two components\nimport sklearn.mixture\ngmm = sklearn.mixture.GaussianMixture(n_components=2, \n                                      covariance_type='full', \n                                      init_params = 'kmeans')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#7b78e96f .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\ncolors = ['bg'[p] for p in y_pred]\nplt.title('Clustering via GMM')\nplt.axis('off')\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8);\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-11-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#cfbd4e8a .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' weight: {gmm.weights_[clus]:0.3f}')\n    print(f' mean: {gmm.means_[clus]}')\n    print(f' cov: \\n{gmm.covariances_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n weight: 0.333\n mean: [-4.05403279  1.9822596 ]\n cov: \n[[0.464702   0.02385764]\n [0.02385764 0.42700883]]\n\nCluster 1:\n weight: 0.667\n mean: [-0.01895709 -0.00177815]\n cov: \n[[2.79024354 0.64760422]\n [0.64760422 0.16476598]]\n\n```\n:::\n:::\n\n\n::: {#aeaa6010 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\nimport sklearn.cluster\nkmeans = sklearn.cluster.KMeans(init = 'k-means++', n_clusters = 2, n_init = 100)\ny_pred_kmeans = kmeans.fit_predict(X)\ncolors = ['bg'[p] for p in y_pred_kmeans]\nplt.title('Clustering via $k$-means\\n$k$-means centers: red, GMM centers: black')\nplt.axis('off')\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'ro')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'ko');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-13-output-1.png){width=540 height=429}\n:::\n:::\n\n\n::: {#9c2b96f0 .cell cell_style='split' hide_input='true' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' center: {kmeans.cluster_centers_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n center: [0.28341737 0.06741478]\n\nCluster 1:\n center: [-3.88381453  1.56532945]\n\n```\n:::\n:::\n\n\nNow, let's construct __overlapping__ clusters.  What will happen?\n\n::: {#7b53c0e0 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=14}\n``` {.python .cell-code}\nX = np.r_[np.random.randn(n_samples, 2) @ C,\n          0.7 * np.random.randn(n_samples, 2) ]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='full')\ny_pred_over = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#66f898c4 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred_over]\nplt.title('GMM for overlapping clusters\\nNote they have nearly the same center')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'ro');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-16-output-1.png){width=540 height=427}\n:::\n:::\n\n\n::: {#447ba562 .cell cell_style='split' hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' weight: {gmm.weights_[clus]:0.3f}')\n    print(f' mean: {gmm.means_[clus]}\\n')\n    # print(f' cov: \\n{gmm.covariances_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n weight: 0.509\n mean: [0.00874631 0.00388373]\n\nCluster 1:\n weight: 0.491\n mean: [-0.0008464   0.01367245]\n\n```\n:::\n:::\n\n\n## How many parameters are estimated?\n\nMost of the parameters in the model are contained in the covariance matrices.\n\nIn the most general case, for $k$ clusters of points in $n$ dimensions, there are $k$ covariance matrices each of size $n \\times n$.   \n\nSo we need $kn^2$ parameters to specify this model.\n\nIt can happen that you may not have enough data to estimate so many parameters.\n\nAlso, it can happen that you believe that clusters should have some constraints on their shapes.\n\nHere is where the GMM assumptions become __really__ useful.\n\nLet's say you believe all the clusters should have the same shape, but the shape can be arbitrary. \n\nThen you only need to estimate __one__ covariance matrix - just $n^2$ parameters.\n\nThis is specified by the GMM parameter `covariance_type='tied'`.\n\n::: {#3e9d9a6c .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=17}\n``` {.python .cell-code}\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          0.7 * np.random.randn(n_samples, 2) ]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='tied')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#b21c7655 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color=colors, s=10, alpha=0.8)\nplt.title('Covariance type = tied')\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0],gmm.means_[:,1], 'ok');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-19-output-1.png){width=540 height=409}\n:::\n:::\n\n\nPerhaps you believe in even more restricted shapes: all clusters should have their axes aligned with the coordinate axes.\n\nThat is, clusters are not skewed.\n\n\nThen you only need to estimate the diagonals of the covariance matrices - just $kn$ parameters.\n\nThis is specified by the GMM parameter `covariance_type='diag'`.\n\n::: {#f6127bfa .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=19}\n``` {.python .cell-code}\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          0.7 * np.random.randn(n_samples, 2)]\ngmm = sklearn.mixture.GaussianMixture(n_components=4, covariance_type='diag')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#3efcc86e .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=20}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-21-output-1.png){width=540 height=389}\n:::\n:::\n\n\nFinally, if you believe that all clusters should be round, then you only need to estimate the $k$ variances.  \n\nThis is specified by the GMM parameter `covariance_type='spherical'`.\n\n::: {#bfe11749 .cell execution_count=21}\n``` {.python .cell-code}\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          0.7 * np.random.randn(n_samples, 2)]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='spherical')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#963e6d5e .cell hide_input='true' tags='[\"hide-input\"]' execution_count=22}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc');\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-23-output-1.png){width=540 height=389}\n:::\n:::\n\n\n",
    "supporting": [
      "09-Clustering-IV-GMM-EM_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}